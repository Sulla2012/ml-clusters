{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a4ad7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet18, ResNet18_Weights\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClusterDataset, get_transform, get_instance_segmentation_model, get_instance_frcnn_model\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_cat, reduce_cat, eval_model\n\u001b[1;32m     26\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import visionutils.transforms as T\n",
    "from visionutils.engine import train_one_epoch, evaluate\n",
    "import visionutils.utils\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.nn import ClusterDataset, get_transform, get_instance_segmentation_model, get_instance_frcnn_model\n",
    "from utils.evaluate import make_cat, reduce_cat, eval_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7aa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes the inverse-variance weighted average of a data vector\n",
    "def inv_var(data, variances):\n",
    "    ave = 0\n",
    "    var = 0\n",
    "    for i in range(len(data)):\n",
    "        ave += data[i]/variances[i]**2\n",
    "        var += 1/variances[i]**2\n",
    "    return ave/var, np.sqrt(1/var)\n",
    "\n",
    "#This function evaluates the performance of a model on a single image\n",
    "def eval_img(img, truth, model, threshold = 0.5):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    errs = []\n",
    "\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    for i in range(len(prediction[0]['boxes'])):\n",
    "        if prediction[0]['scores'][i]>=threshold:\n",
    "            xmin, ymin, xmax, ymax = prediction[0]['boxes'][i].cpu().numpy()\n",
    "            xs.append((xmax+xmin)/2)\n",
    "            ys.append((ymax+ymin)/2)\n",
    "            errs.append(prediction[0]['scores'][i].cpu().numpy())\n",
    "            #print(xs[-1], ys[-1])\n",
    "    \n",
    "    print(\"X loc: \", inv_var(xs, errs))\n",
    "    print(\"Y loc: \", inv_var(ys, errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c488d-f9ad-4562-ae46-a695c060c9cd",
   "metadata": {},
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44001e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_type = \"indv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#root = '/project/r/rbond/jorlo/datasets/ACT_tiles/'\n",
    "root = \"/mnt/welch/USERS/jorlo/ml-clusters/ACT_tiles\"\n",
    "len(list(sorted(os.listdir(os.path.join(root, \"{}_masks\".format(tile_type))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = ClusterDataset(root, get_transform(train=True),\n",
    "                        cluster_dir = '{}_freq_stamps'.format(tile_type),\n",
    "                        mask_dir = '{}_freq_masks'.format(tile_type))\n",
    "dataset_test = ClusterDataset(root, get_transform(train=False),\n",
    "                        cluster_dir = '{}_freq_stamps'.format(tile_type), \n",
    "                        mask_dir = '{}_freq_masks'.format(tile_type))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "test_num = 20\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-test_num])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-test_num:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=8, shuffle=True, num_workers=4,\n",
    "    collate_fn=visionutils.utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=visionutils.utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "backbone = 'mobilenet'\n",
    "backbone_path = \"/mnt/welch/USERS/jorlo/ml-clusters/models/torch-act/\"\n",
    "# get the model using our helper function\n",
    "model = get_instance_frcnn_model(num_classes, backbone_path = backbone_path + \"act-{}.pth\".format(backbone), backbone_type = backbone)\n",
    "#model = get_instance_frcnn_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd90ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 10\n",
    "\n",
    "model_path = \"/mnt/welch/USERS/jorlo/ml-clusters/models/torch-act/act-{}-frcnn-{}-tiles.pth\".format(backbone, tile_type)\n",
    "load_exiting_weights = True\n",
    "if load_exiting_weights and os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63853774-8c90-470a-999f-98a93241ef69",
   "metadata": {},
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0805c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"/project/r/rbond/jorlo/ml-clusters/models/torch-act/act-mobilenet-frcnn.pth\"))\n",
    "#model.load_state_dict(torch.load(model_path + \"act-{}-frcnn-indv.pth\".format(backbone)))\n",
    "#model_path = \"/mnt/welch/USERS/jorlo/ml-clusters/models/torch-act/\"\n",
    "\n",
    "#model.load_state_dict(torch.load(model_path+\"act-{}-frcnn-{}-tiles.pth\".format(backbone, tile_type)))\n",
    "#model.load_state_dict(torch.load(\"/mnt/welch/USERS/jorlo/ml-clusters/models/mist-torch-act/act-resnet-frcnn-tiles.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631fc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e7f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63bdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0364205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[1,...])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83b36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "\n",
    "    img, truth = dataset_test[j]\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    print(truth)\n",
    "    print(prediction)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839faad3-26e0-4165-a0a3-53e1427482f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420119b-970d-4a4e-9bd0-c2f1e6b4eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/welch/USERS/jorlo/ml-clusters/ACT_tiles/indv_freq_stamps/{:04}.fits\"\n",
    "\n",
    "cents = make_catalog(model, dataset_test, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3f7c8-5719-4fc9-b0e6-5f6ca7a1824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = reduce_cat(cents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665bc0e-7435-49b2-9761-af739693d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86e709-3a5f-4a04-aee4-03170a078223",
   "metadata": {},
   "outputs": [],
   "source": [
    "cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.imshow(img[0,...])\n",
    "\n",
    "colors = [\"red\", \"black\", \"purple\", \"green\", \"yellow\"]\n",
    "\n",
    "for i in range(len(prediction[0]['boxes'])):\n",
    "    if prediction[0]['scores'][i]>=0.5:\n",
    "        xmin, ymin, xmax, ymax = prediction[0]['boxes'][i].cpu().numpy()\n",
    "        rectangle = plt.Rectangle((xmin,ymin), xmax-xmin,ymax-ymin, fc='none',ec=colors[i], label = 'Guess')\n",
    "        ax.add_patch(rectangle)\n",
    "        \n",
    "for i in range(len(truth['boxes'])):\n",
    "    xmin, ymin, xmax, ymax = truth['boxes'][i].cpu().numpy()\n",
    "    rectangle = plt.Rectangle((xmin,ymin), xmax-xmin,ymax-ymin, fc='none',ec=colors[i], linestyle = '--', label = 'Truth')\n",
    "    ax.add_patch(rectangle)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('./figs/region_prop_{}.png'.format(j))\n",
    "plt.savefig('./figs/region_prop_{}.pdf'.format(j))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862244b0-4e2f-4ad1-a0c7-613324088fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ca776",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "    img, truth = dataset_test[j]\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(img[0,...])\n",
    "\n",
    "    colors = [\"red\", \"black\", \"purple\", \"green\", \"yellow\"]\n",
    "\n",
    "    for i in range(len(prediction[0]['boxes'])):\n",
    "        if prediction[0]['scores'][i]>=0.5:\n",
    "            xmin, ymin, xmax, ymax = prediction[0]['boxes'][i].cpu().numpy()\n",
    "            rectangle = plt.Rectangle((xmin,ymin), xmax-xmin,ymax-ymin, fc='none',ec=colors[i], label = 'Guess')\n",
    "            ax.add_patch(rectangle)\n",
    "\n",
    "    for i in range(len(truth['boxes'])):\n",
    "        xmin, ymin, xmax, ymax = truth['boxes'][i].cpu().numpy()\n",
    "        rectangle = plt.Rectangle((xmin,ymin), xmax-xmin,ymax-ymin, fc='none',ec=colors[i], linestyle = '--', label = 'Truth')\n",
    "        ax.add_patch(rectangle)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig('./figs/region_prop_{}.png'.format(j))\n",
    "    plt.savefig('./figs/region_prop_{}.pdf'.format(j))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832c6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddbd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9d8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e44dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e835fbed-efe9-4105-b99a-d39b10eff24c",
   "metadata": {},
   "source": [
    "# Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980572ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(455):\n",
    "    mask_list = list(sorted(os.listdir(os.path.join('/project/r/rbond/jorlo/datasets/ACT_tiles/', \"masks\"))))\n",
    "    mask_path = os.path.join('/project/r/rbond/jorlo/datasets/ACT_tiles/', \"masks\", mask_list[j])\n",
    "    with np.load(mask_path) as data:\n",
    "        mask = data['arr_0']\n",
    "    # convert the PIL Image into a numpy array\n",
    "    mask = np.array(mask)\n",
    "    # instances are encoded as different colors\n",
    "    obj_ids = np.unique(mask)\n",
    "    # first id is the background, so remove it\n",
    "    obj_ids = obj_ids[1:]\n",
    "    # split the color-encoded mask into a set\n",
    "    # of binary masks\n",
    "    masks = mask == obj_ids[:, None, None]\n",
    "    # get bounding box coordinates for each mask\n",
    "    num_objs = len(obj_ids)\n",
    "    boxes = []\n",
    "    for i in range(num_objs):\n",
    "        pos = np.where(masks[i])\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        if xmin==xmax and ymin == ymax:\n",
    "            print(mask_list[j])\n",
    "        \n",
    "        \n",
    "\n",
    "    # convert everything into a torch.Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d97d3-167b-4f09-aaa0-b1b2b51552dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root + \"/\"\n",
    "tile = '1_1_0'\n",
    "\n",
    "\n",
    "with np.load(path+'small_freq_tiles/'+tile+'.npz') as data:\n",
    "        img = data['arr_0']\n",
    "with np.load(path+'small_masks/'+tile+'_mask.npz') as data:\n",
    "        mask = data['arr_0']\n",
    "img = np.array(img)\n",
    "mask = np.array(mask)\n",
    "# instances are encoded as different colors\n",
    "obj_ids = np.unique(mask)\n",
    "# first id is the background, so remove it\n",
    "obj_ids = obj_ids[1:]\n",
    "# split the color-encoded mask into a set\n",
    "# of binary masks\n",
    "masks = mask == obj_ids[:, None, None]\n",
    "# get bounding box coordinates for each mask\n",
    "num_objs = len(obj_ids)\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(masks[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "img = np.ascontiguousarray(img.transpose(2,0,1))\n",
    "model.eval()\n",
    "\n",
    "img = torch.as_tensor(img, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5c155-22f9-47bb-bada-56761985f002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25838755-4ae5-4c31-9f90-5cd59764aa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea7b99-d248-4374-86d3-279b89eab0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

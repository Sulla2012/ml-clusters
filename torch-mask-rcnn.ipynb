{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5090c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import visionutils.transforms as T\n",
    "from visionutils.engine import train_one_epoch, evaluate\n",
    "import visionutils.utils\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db654c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"freq_tiles\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"freq_tiles\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
    "        #img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        with np.load(img_path) as data:\n",
    "            img = data['arr_0']\n",
    "        img = np.array(img, dtype='f')\n",
    "\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        with np.load(mask_path) as data:\n",
    "            mask = data['arr_0']\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7d27e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7fc71fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "920652f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_frcnn_model(num_classes, \n",
    "                             backbone_path = \"/project/r/rbond/jorlo/ml-clusters/models/torch-act/act-mobilenet.pth\"):\n",
    "    \n",
    "    backbone_model = torchvision.models.mobilenet_v2()\n",
    "    backbone_model.fc = nn.Linear(512, 2)\n",
    "\n",
    "    backbone_model.load_state_dict(torch.load(backbone_path))\n",
    "    backbone = backbone_model.features\n",
    "    \n",
    "    backbone.out_channels = 1280\n",
    "    # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "    # location, with 5 different sizes and 3 different aspect\n",
    "    # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "    # map could potentially have different sizes and\n",
    "    # aspect ratios\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "    # let's define what are the feature maps that we will\n",
    "    # use to perform the region of interest cropping, as well as\n",
    "    # the size of the crop after rescaling.\n",
    "    # if your backbone returns a Tensor, featmap_names is expected to\n",
    "    # be [0]. More generally, the backbone should return an\n",
    "    # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "    # feature maps to use.\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    \n",
    "\n",
    "    model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f464b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cc92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b8c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a532a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = ClusterDataset('/project/r/rbond/jorlo/datasets/ACT_tiles/', get_transform(train=True))\n",
    "dataset_test = ClusterDataset('/project/r/rbond/jorlo/datasets/ACT_tiles/', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=visionutils.utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=visionutils.utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ee1c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_frcnn_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57f9abe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/203]  eta: 1:50:43  lr: 0.000030  loss: 1.8438 (1.8438)  loss_classifier: 0.7085 (0.7085)  loss_box_reg: 0.0019 (0.0019)  loss_objectness: 0.6948 (0.6948)  loss_rpn_box_reg: 0.4386 (0.4386)  time: 32.7290  data: 0.6643  max mem: 5532\n",
      "Epoch: [0]  [ 10/203]  eta: 0:10:08  lr: 0.000277  loss: 1.8791 (1.9009)  loss_classifier: 0.6986 (0.6944)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.6948 (0.6953)  loss_rpn_box_reg: 0.4804 (0.5099)  time: 3.1526  data: 0.0648  max mem: 6294\n",
      "Epoch: [0]  [ 20/203]  eta: 0:05:18  lr: 0.000524  loss: 1.8701 (1.8386)  loss_classifier: 0.6436 (0.6355)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.6909 (0.6878)  loss_rpn_box_reg: 0.4804 (0.5141)  time: 0.1910  data: 0.0057  max mem: 6374\n",
      "Epoch: [0]  [ 30/203]  eta: 0:03:34  lr: 0.000772  loss: 1.5306 (1.6714)  loss_classifier: 0.4461 (0.5429)  loss_box_reg: 0.0013 (0.0014)  loss_objectness: 0.6570 (0.6710)  loss_rpn_box_reg: 0.3959 (0.4560)  time: 0.1843  data: 0.0075  max mem: 6724\n",
      "Epoch: [0]  [ 40/203]  eta: 0:02:40  lr: 0.001019  loss: 1.2643 (1.5598)  loss_classifier: 0.2560 (0.4656)  loss_box_reg: 0.0029 (0.0021)  loss_objectness: 0.6062 (0.6489)  loss_rpn_box_reg: 0.3354 (0.4432)  time: 0.1875  data: 0.0087  max mem: 6724\n",
      "Epoch: [0]  [ 50/203]  eta: 0:02:06  lr: 0.001266  loss: 1.0895 (1.4314)  loss_classifier: 0.2026 (0.4070)  loss_box_reg: 0.0023 (0.0020)  loss_objectness: 0.5479 (0.6194)  loss_rpn_box_reg: 0.2942 (0.4031)  time: 0.1971  data: 0.0088  max mem: 6724\n",
      "Epoch: [0]  [ 60/203]  eta: 0:01:43  lr: 0.001513  loss: 0.7984 (1.3207)  loss_classifier: 0.1635 (0.3682)  loss_box_reg: 0.0012 (0.0019)  loss_objectness: 0.4569 (0.5864)  loss_rpn_box_reg: 0.1808 (0.3643)  time: 0.1958  data: 0.0076  max mem: 6724\n",
      "Epoch: [0]  [ 70/203]  eta: 0:01:26  lr: 0.001761  loss: 0.6595 (1.2357)  loss_classifier: 0.1558 (0.3426)  loss_box_reg: 0.0012 (0.0018)  loss_objectness: 0.3936 (0.5550)  loss_rpn_box_reg: 0.1306 (0.3363)  time: 0.1964  data: 0.0076  max mem: 6966\n",
      "Epoch: [0]  [ 80/203]  eta: 0:01:13  lr: 0.002008  loss: 0.6385 (1.1700)  loss_classifier: 0.1558 (0.3241)  loss_box_reg: 0.0009 (0.0018)  loss_objectness: 0.3496 (0.5271)  loss_rpn_box_reg: 0.1306 (0.3170)  time: 0.1947  data: 0.0078  max mem: 6966\n",
      "Epoch: [0]  [ 90/203]  eta: 0:01:02  lr: 0.002255  loss: 0.6305 (1.1123)  loss_classifier: 0.1736 (0.3088)  loss_box_reg: 0.0010 (0.0018)  loss_objectness: 0.3154 (0.5013)  loss_rpn_box_reg: 0.1556 (0.3004)  time: 0.1921  data: 0.0083  max mem: 6981\n",
      "Epoch: [0]  [100/203]  eta: 0:00:53  lr: 0.002502  loss: 0.6030 (1.0629)  loss_classifier: 0.1736 (0.2970)  loss_box_reg: 0.0009 (0.0017)  loss_objectness: 0.2755 (0.4787)  loss_rpn_box_reg: 0.1498 (0.2855)  time: 0.1961  data: 0.0094  max mem: 6981\n",
      "Epoch: [0]  [110/203]  eta: 0:00:45  lr: 0.002750  loss: 0.5918 (1.0187)  loss_classifier: 0.1759 (0.2862)  loss_box_reg: 0.0009 (0.0017)  loss_objectness: 0.2579 (0.4586)  loss_rpn_box_reg: 0.1478 (0.2723)  time: 0.1928  data: 0.0075  max mem: 6981\n",
      "Epoch: [0]  [120/203]  eta: 0:00:38  lr: 0.002997  loss: 0.5299 (0.9764)  loss_classifier: 0.1613 (0.2758)  loss_box_reg: 0.0009 (0.0016)  loss_objectness: 0.2407 (0.4396)  loss_rpn_box_reg: 0.1198 (0.2594)  time: 0.1807  data: 0.0068  max mem: 6981\n",
      "Epoch: [0]  [130/203]  eta: 0:00:32  lr: 0.003244  loss: 0.4752 (0.9377)  loss_classifier: 0.1493 (0.2660)  loss_box_reg: 0.0007 (0.0016)  loss_objectness: 0.2169 (0.4221)  loss_rpn_box_reg: 0.1073 (0.2480)  time: 0.1841  data: 0.0074  max mem: 7078\n",
      "Epoch: [0]  [140/203]  eta: 0:00:26  lr: 0.003492  loss: 0.4629 (0.9042)  loss_classifier: 0.1417 (0.2572)  loss_box_reg: 0.0006 (0.0015)  loss_objectness: 0.2138 (0.4071)  loss_rpn_box_reg: 0.1036 (0.2384)  time: 0.1957  data: 0.0070  max mem: 7078\n",
      "Epoch: [0]  [150/203]  eta: 0:00:21  lr: 0.003739  loss: 0.4869 (0.8785)  loss_classifier: 0.1518 (0.2516)  loss_box_reg: 0.0006 (0.0015)  loss_objectness: 0.2151 (0.3946)  loss_rpn_box_reg: 0.1149 (0.2309)  time: 0.1930  data: 0.0066  max mem: 7078\n",
      "Epoch: [0]  [160/203]  eta: 0:00:16  lr: 0.003986  loss: 0.4481 (0.8508)  loss_classifier: 0.1518 (0.2448)  loss_box_reg: 0.0006 (0.0014)  loss_objectness: 0.1982 (0.3822)  loss_rpn_box_reg: 0.0939 (0.2223)  time: 0.1950  data: 0.0062  max mem: 7078\n",
      "Epoch: [0]  [170/203]  eta: 0:00:12  lr: 0.004233  loss: 0.3917 (0.8250)  loss_classifier: 0.1251 (0.2382)  loss_box_reg: 0.0005 (0.0014)  loss_objectness: 0.1808 (0.3710)  loss_rpn_box_reg: 0.0767 (0.2144)  time: 0.1923  data: 0.0061  max mem: 7078\n",
      "Epoch: [0]  [180/203]  eta: 0:00:08  lr: 0.004481  loss: 0.3617 (0.8017)  loss_classifier: 0.1145 (0.2324)  loss_box_reg: 0.0004 (0.0014)  loss_objectness: 0.1777 (0.3607)  loss_rpn_box_reg: 0.0754 (0.2072)  time: 0.1914  data: 0.0071  max mem: 7078\n",
      "Epoch: [0]  [190/203]  eta: 0:00:04  lr: 0.004728  loss: 0.4153 (0.7863)  loss_classifier: 0.1406 (0.2292)  loss_box_reg: 0.0005 (0.0014)  loss_objectness: 0.1870 (0.3531)  loss_rpn_box_reg: 0.0871 (0.2027)  time: 0.1970  data: 0.0074  max mem: 7078\n",
      "Epoch: [0]  [200/203]  eta: 0:00:01  lr: 0.004975  loss: 0.4153 (0.7669)  loss_classifier: 0.1406 (0.2242)  loss_box_reg: 0.0006 (0.0014)  loss_objectness: 0.1870 (0.3444)  loss_rpn_box_reg: 0.0892 (0.1969)  time: 0.1919  data: 0.0064  max mem: 7078\n",
      "Epoch: [0]  [202/203]  eta: 0:00:00  lr: 0.005000  loss: 0.4472 (0.7638)  loss_classifier: 0.1406 (0.2236)  loss_box_reg: 0.0006 (0.0014)  loss_objectness: 0.1912 (0.3429)  loss_rpn_box_reg: 0.0911 (0.1959)  time: 0.1939  data: 0.0064  max mem: 7078\n",
      "Epoch: [0] Total time: 0:01:11 (0.3529 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:35  model_time: 0.0444 (0.0444)  evaluator_time: 0.1306 (0.1306)  time: 0.7111  data: 0.5337  max mem: 7078\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.0304 (0.0324)  evaluator_time: 0.0382 (0.0449)  time: 0.0754  data: 0.0034  max mem: 7078\n",
      "Test: Total time: 0:00:04 (0.0939 s / it)\n",
      "Averaged stats: model_time: 0.0304 (0.0324)  evaluator_time: 0.0382 (0.0449)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "Epoch: [1]  [  0/203]  eta: 0:02:56  lr: 0.005000  loss: 0.4591 (0.4591)  loss_classifier: 0.1539 (0.1539)  loss_box_reg: 0.0007 (0.0007)  loss_objectness: 0.2029 (0.2029)  loss_rpn_box_reg: 0.1017 (0.1017)  time: 0.8705  data: 0.7155  max mem: 7078\n",
      "Epoch: [1]  [ 10/203]  eta: 0:00:40  lr: 0.005000  loss: 0.4006 (0.4400)  loss_classifier: 0.1325 (0.1456)  loss_box_reg: 0.0006 (0.0009)  loss_objectness: 0.1824 (0.1934)  loss_rpn_box_reg: 0.0849 (0.1002)  time: 0.2104  data: 0.0712  max mem: 7078\n",
      "Epoch: [1]  [ 20/203]  eta: 0:00:33  lr: 0.005000  loss: 0.4151 (0.4360)  loss_classifier: 0.1369 (0.1473)  loss_box_reg: 0.0006 (0.0010)  loss_objectness: 0.1826 (0.1914)  loss_rpn_box_reg: 0.0849 (0.0962)  time: 0.1473  data: 0.0095  max mem: 7078\n",
      "Epoch: [1]  [ 30/203]  eta: 0:00:30  lr: 0.005000  loss: 0.4907 (0.4480)  loss_classifier: 0.1502 (0.1514)  loss_box_reg: 0.0005 (0.0013)  loss_objectness: 0.2004 (0.1940)  loss_rpn_box_reg: 0.1181 (0.1013)  time: 0.1567  data: 0.0167  max mem: 7078\n",
      "Epoch: [1]  [ 40/203]  eta: 0:00:27  lr: 0.005000  loss: 0.4160 (0.4312)  loss_classifier: 0.1446 (0.1457)  loss_box_reg: 0.0005 (0.0014)  loss_objectness: 0.1837 (0.1888)  loss_rpn_box_reg: 0.0872 (0.0953)  time: 0.1548  data: 0.0138  max mem: 7078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 50/203]  eta: 0:00:25  lr: 0.005000  loss: 0.4007 (0.4403)  loss_classifier: 0.1301 (0.1488)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1830 (0.1917)  loss_rpn_box_reg: 0.0861 (0.0984)  time: 0.1463  data: 0.0068  max mem: 7078\n",
      "Epoch: [1]  [ 60/203]  eta: 0:00:23  lr: 0.005000  loss: 0.3894 (0.4312)  loss_classifier: 0.1298 (0.1463)  loss_box_reg: 0.0005 (0.0012)  loss_objectness: 0.1842 (0.1888)  loss_rpn_box_reg: 0.0769 (0.0949)  time: 0.1484  data: 0.0088  max mem: 7078\n",
      "Epoch: [1]  [ 70/203]  eta: 0:00:21  lr: 0.005000  loss: 0.3894 (0.4278)  loss_classifier: 0.1298 (0.1457)  loss_box_reg: 0.0005 (0.0013)  loss_objectness: 0.1842 (0.1882)  loss_rpn_box_reg: 0.0733 (0.0926)  time: 0.1492  data: 0.0102  max mem: 7078\n",
      "Epoch: [1]  [ 80/203]  eta: 0:00:19  lr: 0.005000  loss: 0.3998 (0.4234)  loss_classifier: 0.1333 (0.1444)  loss_box_reg: 0.0006 (0.0015)  loss_objectness: 0.1892 (0.1867)  loss_rpn_box_reg: 0.0733 (0.0909)  time: 0.1466  data: 0.0084  max mem: 7078\n",
      "Epoch: [1]  [ 90/203]  eta: 0:00:17  lr: 0.005000  loss: 0.4369 (0.4268)  loss_classifier: 0.1553 (0.1467)  loss_box_reg: 0.0006 (0.0015)  loss_objectness: 0.1894 (0.1871)  loss_rpn_box_reg: 0.0866 (0.0915)  time: 0.1467  data: 0.0067  max mem: 7078\n",
      "Epoch: [1]  [100/203]  eta: 0:00:16  lr: 0.005000  loss: 0.3930 (0.4197)  loss_classifier: 0.1332 (0.1443)  loss_box_reg: 0.0004 (0.0015)  loss_objectness: 0.1795 (0.1852)  loss_rpn_box_reg: 0.0674 (0.0886)  time: 0.1453  data: 0.0066  max mem: 7078\n",
      "Epoch: [1]  [110/203]  eta: 0:00:14  lr: 0.005000  loss: 0.4080 (0.4213)  loss_classifier: 0.1423 (0.1450)  loss_box_reg: 0.0004 (0.0014)  loss_objectness: 0.1894 (0.1859)  loss_rpn_box_reg: 0.0804 (0.0890)  time: 0.1504  data: 0.0130  max mem: 7078\n",
      "Epoch: [1]  [120/203]  eta: 0:00:13  lr: 0.005000  loss: 0.4316 (0.4263)  loss_classifier: 0.1565 (0.1473)  loss_box_reg: 0.0005 (0.0014)  loss_objectness: 0.1978 (0.1875)  loss_rpn_box_reg: 0.0874 (0.0901)  time: 0.1711  data: 0.0316  max mem: 7078\n",
      "Epoch: [1]  [130/203]  eta: 0:00:11  lr: 0.005000  loss: 0.3917 (0.4229)  loss_classifier: 0.1358 (0.1462)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1801 (0.1863)  loss_rpn_box_reg: 0.0798 (0.0890)  time: 0.1776  data: 0.0348  max mem: 7078\n",
      "Epoch: [1]  [140/203]  eta: 0:00:09  lr: 0.005000  loss: 0.3438 (0.4168)  loss_classifier: 0.1182 (0.1444)  loss_box_reg: 0.0003 (0.0013)  loss_objectness: 0.1594 (0.1842)  loss_rpn_box_reg: 0.0696 (0.0869)  time: 0.1584  data: 0.0160  max mem: 7078\n",
      "Epoch: [1]  [150/203]  eta: 0:00:08  lr: 0.005000  loss: 0.3092 (0.4096)  loss_classifier: 0.1116 (0.1417)  loss_box_reg: 0.0003 (0.0013)  loss_objectness: 0.1540 (0.1820)  loss_rpn_box_reg: 0.0520 (0.0847)  time: 0.1440  data: 0.0067  max mem: 7078\n",
      "Epoch: [1]  [160/203]  eta: 0:00:06  lr: 0.005000  loss: 0.3749 (0.4110)  loss_classifier: 0.1270 (0.1426)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1684 (0.1823)  loss_rpn_box_reg: 0.0741 (0.0849)  time: 0.1448  data: 0.0070  max mem: 7078\n",
      "Epoch: [1]  [170/203]  eta: 0:00:05  lr: 0.005000  loss: 0.4181 (0.4125)  loss_classifier: 0.1455 (0.1433)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1873 (0.1829)  loss_rpn_box_reg: 0.0862 (0.0850)  time: 0.1473  data: 0.0070  max mem: 7078\n",
      "Epoch: [1]  [180/203]  eta: 0:00:03  lr: 0.005000  loss: 0.3997 (0.4123)  loss_classifier: 0.1404 (0.1435)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1847 (0.1828)  loss_rpn_box_reg: 0.0726 (0.0848)  time: 0.1477  data: 0.0067  max mem: 7078\n",
      "Epoch: [1]  [190/203]  eta: 0:00:02  lr: 0.005000  loss: 0.3793 (0.4127)  loss_classifier: 0.1404 (0.1438)  loss_box_reg: 0.0004 (0.0013)  loss_objectness: 0.1724 (0.1829)  loss_rpn_box_reg: 0.0726 (0.0847)  time: 0.1517  data: 0.0099  max mem: 7078\n",
      "Epoch: [1]  [200/203]  eta: 0:00:00  lr: 0.005000  loss: 0.4160 (0.4136)  loss_classifier: 0.1499 (0.1443)  loss_box_reg: 0.0003 (0.0012)  loss_objectness: 0.1845 (0.1832)  loss_rpn_box_reg: 0.0774 (0.0849)  time: 0.1569  data: 0.0165  max mem: 7078\n",
      "Epoch: [1]  [202/203]  eta: 0:00:00  lr: 0.005000  loss: 0.4160 (0.4135)  loss_classifier: 0.1499 (0.1441)  loss_box_reg: 0.0003 (0.0012)  loss_objectness: 0.1845 (0.1832)  loss_rpn_box_reg: 0.0774 (0.0849)  time: 0.1567  data: 0.0163  max mem: 7078\n",
      "Epoch: [1] Total time: 0:00:31 (0.1559 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:29  model_time: 0.0414 (0.0414)  evaluator_time: 0.0693 (0.0693)  time: 0.5996  data: 0.4865  max mem: 7078\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.0295 (0.0314)  evaluator_time: 0.0388 (0.0421)  time: 0.0737  data: 0.0038  max mem: 7078\n",
      "Test: Total time: 0:00:04 (0.0897 s / it)\n",
      "Averaged stats: model_time: 0.0295 (0.0314)  evaluator_time: 0.0388 (0.0421)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "Epoch: [2]  [  0/203]  eta: 0:02:34  lr: 0.005000  loss: 0.2321 (0.2321)  loss_classifier: 0.0784 (0.0784)  loss_box_reg: 0.0001 (0.0001)  loss_objectness: 0.1291 (0.1291)  loss_rpn_box_reg: 0.0244 (0.0244)  time: 0.7627  data: 0.6156  max mem: 7078\n",
      "Epoch: [2]  [ 10/203]  eta: 0:00:38  lr: 0.005000  loss: 0.3187 (0.3484)  loss_classifier: 0.1014 (0.1204)  loss_box_reg: 0.0003 (0.0009)  loss_objectness: 0.1494 (0.1619)  loss_rpn_box_reg: 0.0549 (0.0652)  time: 0.1989  data: 0.0628  max mem: 7078\n",
      "Epoch: [2]  [ 20/203]  eta: 0:00:31  lr: 0.005000  loss: 0.3514 (0.3818)  loss_classifier: 0.1246 (0.1354)  loss_box_reg: 0.0004 (0.0010)  loss_objectness: 0.1639 (0.1740)  loss_rpn_box_reg: 0.0705 (0.0714)  time: 0.1446  data: 0.0073  max mem: 7078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/gpfs/fs1/home/r/rbond/jorlo/dev/ML-clusters/visionutils/engine.py:30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     27\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 30\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:384\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    382\u001b[0m     labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_targets_to_anchors(anchors, targets)\n\u001b[1;32m    383\u001b[0m     regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[0;32m--> 384\u001b[0m     loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_targets\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     losses \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_objectness\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_objectness,\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_rpn_box_reg\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_rpn_box_reg,\n\u001b[1;32m    390\u001b[0m     }\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m boxes, losses\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:314\u001b[0m, in \u001b[0;36mRegionProposalNetwork.compute_loss\u001b[0;34m(self, objectness, pred_bbox_deltas, labels, regression_targets)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m, objectness: Tensor, pred_bbox_deltas: Tensor, labels: List[Tensor], regression_targets: List[Tensor]\n\u001b[1;32m    301\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m        objectness (Tensor)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m        box_loss (Tensor)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     sampled_pos_inds, sampled_neg_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfg_bg_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     sampled_pos_inds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mcat(sampled_pos_inds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    316\u001b[0m     sampled_neg_inds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mcat(sampled_neg_inds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torchvision/models/detection/_utils.py:45\u001b[0m, in \u001b[0;36mBalancedPositiveNegativeSampler.__call__\u001b[0;34m(self, matched_idxs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m matched_idxs_per_image \u001b[38;5;129;01min\u001b[39;00m matched_idxs:\n\u001b[1;32m     44\u001b[0m     positive \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(matched_idxs_per_image \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 45\u001b[0m     negative \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatched_idxs_per_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m     num_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size_per_image \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive_fraction)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# protect against not enough positive examples\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c6a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "511ad237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1_0_0_mask.npz\n",
      "1 1_0_10_mask.npz\n",
      "2 1_0_13_mask.npz\n",
      "3 1_0_14_mask.npz\n",
      "4 1_0_15_mask.npz\n",
      "5 1_0_16_mask.npz\n",
      "6 1_0_17_mask.npz\n",
      "7 1_0_18_mask.npz\n",
      "8 1_0_19_mask.npz\n",
      "9 1_0_1_mask.npz\n",
      "10 1_0_2_mask.npz\n",
      "11 1_0_3_mask.npz\n",
      "12 1_0_4_mask.npz\n",
      "13 1_0_5_mask.npz\n",
      "14 1_0_6_mask.npz\n",
      "15 1_0_7_mask.npz\n",
      "16 1_0_8_mask.npz\n",
      "17 1_0_9_mask.npz\n",
      "18 1_10_10_mask.npz\n",
      "19 1_10_11_mask.npz\n",
      "20 1_10_12_mask.npz\n",
      "21 1_10_13_mask.npz\n",
      "22 1_10_14_mask.npz\n",
      "23 1_10_16_mask.npz\n",
      "24 1_10_17_mask.npz\n",
      "25 1_10_18_mask.npz\n",
      "26 1_10_19_mask.npz\n",
      "27 1_10_1_mask.npz\n",
      "28 1_10_20_mask.npz\n",
      "29 1_10_21_mask.npz\n",
      "30 1_10_22_mask.npz\n",
      "31 1_10_23_mask.npz\n",
      "32 1_10_24_mask.npz\n",
      "33 1_10_25_mask.npz\n",
      "34 1_10_2_mask.npz\n",
      "35 1_10_3_mask.npz\n",
      "36 1_10_4_mask.npz\n",
      "37 1_10_5_mask.npz\n",
      "38 1_10_6_mask.npz\n",
      "39 1_10_7_mask.npz\n",
      "40 1_10_8_mask.npz\n",
      "41 1_10_9_mask.npz\n",
      "42 1_11_0_mask.npz\n",
      "43 1_11_10_mask.npz\n",
      "44 1_11_11_mask.npz\n",
      "45 1_11_12_mask.npz\n",
      "46 1_11_13_mask.npz\n",
      "47 1_11_14_mask.npz\n",
      "48 1_11_17_mask.npz\n",
      "49 1_11_18_mask.npz\n",
      "50 1_11_19_mask.npz\n",
      "51 1_11_1_mask.npz\n",
      "52 1_11_20_mask.npz\n",
      "53 1_11_21_mask.npz\n",
      "54 1_11_22_mask.npz\n",
      "55 1_11_23_mask.npz\n",
      "56 1_11_24_mask.npz\n",
      "57 1_11_25_mask.npz\n",
      "58 1_11_26_mask.npz\n",
      "59 1_11_2_mask.npz\n",
      "60 1_11_3_mask.npz\n",
      "61 1_11_4_mask.npz\n",
      "62 1_11_5_mask.npz\n",
      "63 1_11_6_mask.npz\n",
      "64 1_11_7_mask.npz\n",
      "65 1_11_8_mask.npz\n",
      "66 1_11_9_mask.npz\n",
      "67 1_12_10_mask.npz\n",
      "68 1_12_11_mask.npz\n",
      "69 1_12_12_mask.npz\n",
      "70 1_12_13_mask.npz\n",
      "71 1_12_14_mask.npz\n",
      "72 1_12_16_mask.npz\n",
      "73 1_12_17_mask.npz\n",
      "74 1_12_18_mask.npz\n",
      "75 1_12_19_mask.npz\n",
      "76 1_12_1_mask.npz\n",
      "77 1_12_20_mask.npz\n",
      "78 1_12_21_mask.npz\n",
      "79 1_12_22_mask.npz\n",
      "80 1_12_23_mask.npz\n",
      "81 1_12_24_mask.npz\n",
      "82 1_12_25_mask.npz\n",
      "83 1_12_2_mask.npz\n",
      "84 1_12_4_mask.npz\n",
      "85 1_12_5_mask.npz\n",
      "86 1_12_6_mask.npz\n",
      "87 1_12_7_mask.npz\n",
      "88 1_12_8_mask.npz\n",
      "89 1_12_9_mask.npz\n",
      "90 1_13_10_mask.npz\n",
      "91 1_13_11_mask.npz\n",
      "92 1_13_12_mask.npz\n",
      "93 1_13_13_mask.npz\n",
      "94 1_13_15_mask.npz\n",
      "95 1_13_16_mask.npz\n",
      "96 1_13_17_mask.npz\n",
      "97 1_13_18_mask.npz\n",
      "98 1_13_19_mask.npz\n",
      "99 1_13_1_mask.npz\n",
      "100 1_13_20_mask.npz\n",
      "101 1_13_21_mask.npz\n",
      "102 1_13_22_mask.npz\n",
      "103 1_13_23_mask.npz\n",
      "104 1_13_2_mask.npz\n",
      "105 1_13_3_mask.npz\n",
      "106 1_13_4_mask.npz\n",
      "107 1_13_5_mask.npz\n",
      "108 1_13_6_mask.npz\n",
      "109 1_13_7_mask.npz\n",
      "110 1_13_8_mask.npz\n",
      "111 1_13_9_mask.npz\n",
      "112 1_14_0_mask.npz\n",
      "113 1_14_10_mask.npz\n",
      "114 1_14_12_mask.npz\n",
      "115 1_14_13_mask.npz\n",
      "116 1_14_14_mask.npz\n",
      "117 1_14_15_mask.npz\n",
      "118 1_14_16_mask.npz\n",
      "119 1_14_17_mask.npz\n",
      "120 1_14_18_mask.npz\n",
      "121 1_14_19_mask.npz\n",
      "122 1_14_1_mask.npz\n",
      "123 1_14_2_mask.npz\n",
      "124 1_14_3_mask.npz\n",
      "125 1_14_4_mask.npz\n",
      "126 1_14_5_mask.npz\n",
      "127 1_14_6_mask.npz\n",
      "128 1_14_7_mask.npz\n",
      "129 1_14_8_mask.npz\n",
      "130 1_14_9_mask.npz\n",
      "131 1_1_0_mask.npz\n",
      "132 1_1_10_mask.npz\n",
      "133 1_1_11_mask.npz\n",
      "134 1_1_12_mask.npz\n",
      "135 1_1_14_mask.npz\n",
      "136 1_1_15_mask.npz\n",
      "137 1_1_16_mask.npz\n",
      "138 1_1_17_mask.npz\n",
      "139 1_1_18_mask.npz\n",
      "140 1_1_19_mask.npz\n",
      "141 1_1_1_mask.npz\n",
      "142 1_1_20_mask.npz\n",
      "143 1_1_21_mask.npz\n",
      "144 1_1_22_mask.npz\n",
      "145 1_1_2_mask.npz\n",
      "146 1_1_3_mask.npz\n",
      "147 1_1_4_mask.npz\n",
      "148 1_1_5_mask.npz\n",
      "149 1_1_6_mask.npz\n",
      "150 1_1_7_mask.npz\n",
      "151 1_1_8_mask.npz\n",
      "152 1_1_9_mask.npz\n",
      "153 1_2_0_mask.npz\n",
      "154 1_2_10_mask.npz\n",
      "155 1_2_11_mask.npz\n",
      "156 1_2_12_mask.npz\n",
      "157 1_2_13_mask.npz\n",
      "158 1_2_15_mask.npz\n",
      "159 1_2_16_mask.npz\n",
      "160 1_2_17_mask.npz\n",
      "161 1_2_18_mask.npz\n",
      "162 1_2_19_mask.npz\n",
      "163 1_2_1_mask.npz\n",
      "164 1_2_20_mask.npz\n",
      "165 1_2_21_mask.npz\n",
      "166 1_2_22_mask.npz\n",
      "167 1_2_24_mask.npz\n",
      "168 1_2_2_mask.npz\n",
      "169 1_2_3_mask.npz\n",
      "170 1_2_4_mask.npz\n",
      "171 1_2_5_mask.npz\n",
      "172 1_2_6_mask.npz\n",
      "173 1_2_7_mask.npz\n",
      "174 1_2_8_mask.npz\n",
      "175 1_2_9_mask.npz\n",
      "176 1_3_0_mask.npz\n",
      "177 1_3_10_mask.npz\n",
      "178 1_3_11_mask.npz\n",
      "179 1_3_12_mask.npz\n",
      "180 1_3_13_mask.npz\n",
      "181 1_3_14_mask.npz\n",
      "182 1_3_16_mask.npz\n",
      "183 1_3_17_mask.npz\n",
      "184 1_3_18_mask.npz\n",
      "185 1_3_19_mask.npz\n",
      "186 1_3_1_mask.npz\n",
      "187 1_3_20_mask.npz\n",
      "188 1_3_21_mask.npz\n",
      "189 1_3_22_mask.npz\n",
      "190 1_3_23_mask.npz\n",
      "191 1_3_24_mask.npz\n",
      "192 1_3_2_mask.npz\n",
      "193 1_3_3_mask.npz\n",
      "194 1_3_4_mask.npz\n",
      "195 1_3_5_mask.npz\n",
      "196 1_3_6_mask.npz\n",
      "197 1_3_7_mask.npz\n",
      "198 1_3_8_mask.npz\n",
      "199 1_3_9_mask.npz\n",
      "200 1_4_0_mask.npz\n",
      "201 1_4_10_mask.npz\n",
      "202 1_4_11_mask.npz\n",
      "203 1_4_12_mask.npz\n",
      "204 1_4_13_mask.npz\n",
      "205 1_4_14_mask.npz\n",
      "206 1_4_17_mask.npz\n",
      "207 1_4_18_mask.npz\n",
      "208 1_4_19_mask.npz\n",
      "209 1_4_1_mask.npz\n",
      "210 1_4_20_mask.npz\n",
      "211 1_4_23_mask.npz\n",
      "212 1_4_24_mask.npz\n",
      "213 1_4_2_mask.npz\n",
      "214 1_4_3_mask.npz\n",
      "215 1_4_4_mask.npz\n",
      "216 1_4_5_mask.npz\n",
      "217 1_4_6_mask.npz\n",
      "218 1_4_7_mask.npz\n",
      "219 1_4_8_mask.npz\n",
      "220 1_4_9_mask.npz\n",
      "221 1_5_0_mask.npz\n",
      "222 1_5_10_mask.npz\n",
      "223 1_5_11_mask.npz\n",
      "224 1_5_12_mask.npz\n",
      "225 1_5_13_mask.npz\n",
      "226 1_5_14_mask.npz\n",
      "227 1_5_15_mask.npz\n",
      "228 1_5_17_mask.npz\n",
      "229 1_5_18_mask.npz\n",
      "230 1_5_19_mask.npz\n",
      "231 1_5_1_mask.npz\n",
      "232 1_5_20_mask.npz\n",
      "233 1_5_21_mask.npz\n",
      "234 1_5_22_mask.npz\n",
      "235 1_5_23_mask.npz\n",
      "236 1_5_24_mask.npz\n",
      "237 1_5_25_mask.npz\n",
      "238 1_5_2_mask.npz\n",
      "239 1_5_3_mask.npz\n",
      "240 1_5_4_mask.npz\n",
      "241 1_5_5_mask.npz\n",
      "242 1_5_6_mask.npz\n",
      "243 1_5_7_mask.npz\n",
      "244 1_5_8_mask.npz\n",
      "245 1_5_9_mask.npz\n",
      "246 1_6_0_mask.npz\n",
      "247 1_6_10_mask.npz\n",
      "248 1_6_11_mask.npz\n",
      "249 1_6_12_mask.npz\n",
      "250 1_6_13_mask.npz\n",
      "251 1_6_14_mask.npz\n",
      "252 1_6_15_mask.npz\n",
      "253 1_6_17_mask.npz\n",
      "254 1_6_18_mask.npz\n",
      "255 1_6_19_mask.npz\n",
      "256 1_6_1_mask.npz\n",
      "257 1_6_20_mask.npz\n",
      "258 1_6_21_mask.npz\n",
      "259 1_6_22_mask.npz\n",
      "260 1_6_23_mask.npz\n",
      "261 1_6_24_mask.npz\n",
      "262 1_6_26_mask.npz\n",
      "263 1_6_27_mask.npz\n",
      "264 1_6_28_mask.npz\n",
      "265 1_6_2_mask.npz\n",
      "266 1_6_3_mask.npz\n",
      "267 1_6_4_mask.npz\n",
      "268 1_6_5_mask.npz\n",
      "269 1_6_6_mask.npz\n",
      "270 1_6_7_mask.npz\n",
      "271 1_6_8_mask.npz\n",
      "272 1_6_9_mask.npz\n",
      "273 1_7_0_mask.npz\n",
      "274 1_7_10_mask.npz\n",
      "275 1_7_11_mask.npz\n",
      "276 1_7_12_mask.npz\n",
      "277 1_7_13_mask.npz\n",
      "278 1_7_14_mask.npz\n",
      "279 1_7_15_mask.npz\n",
      "280 1_7_16_mask.npz\n",
      "281 1_7_18_mask.npz\n",
      "282 1_7_19_mask.npz\n",
      "283 1_7_1_mask.npz\n",
      "284 1_7_20_mask.npz\n",
      "285 1_7_21_mask.npz\n",
      "286 1_7_22_mask.npz\n",
      "287 1_7_23_mask.npz\n",
      "288 1_7_24_mask.npz\n",
      "289 1_7_25_mask.npz\n",
      "290 1_7_27_mask.npz\n",
      "291 1_7_28_mask.npz\n",
      "292 1_7_2_mask.npz\n",
      "293 1_7_3_mask.npz\n",
      "294 1_7_4_mask.npz\n",
      "295 1_7_5_mask.npz\n",
      "296 1_7_6_mask.npz\n",
      "297 1_7_7_mask.npz\n",
      "298 1_7_8_mask.npz\n",
      "299 1_7_9_mask.npz\n",
      "300 1_8_0_mask.npz\n",
      "301 1_8_10_mask.npz\n",
      "302 1_8_11_mask.npz\n",
      "303 1_8_12_mask.npz\n",
      "304 1_8_13_mask.npz\n",
      "305 1_8_14_mask.npz\n",
      "306 1_8_15_mask.npz\n",
      "307 1_8_18_mask.npz\n",
      "308 1_8_19_mask.npz\n",
      "309 1_8_1_mask.npz\n",
      "310 1_8_20_mask.npz\n",
      "311 1_8_21_mask.npz\n",
      "312 1_8_22_mask.npz\n",
      "313 1_8_23_mask.npz\n",
      "314 1_8_24_mask.npz\n",
      "315 1_8_26_mask.npz\n",
      "316 1_8_2_mask.npz\n",
      "317 1_8_3_mask.npz\n",
      "318 1_8_4_mask.npz\n",
      "319 1_8_5_mask.npz\n",
      "320 1_8_6_mask.npz\n",
      "321 1_8_7_mask.npz\n",
      "322 1_8_8_mask.npz\n",
      "323 1_8_9_mask.npz\n",
      "324 1_9_0_mask.npz\n",
      "325 1_9_10_mask.npz\n",
      "326 1_9_11_mask.npz\n",
      "327 1_9_12_mask.npz\n",
      "328 1_9_13_mask.npz\n",
      "329 1_9_14_mask.npz\n",
      "330 1_9_17_mask.npz\n",
      "331 1_9_18_mask.npz\n",
      "332 1_9_19_mask.npz\n",
      "333 1_9_1_mask.npz\n",
      "334 1_9_20_mask.npz\n",
      "335 1_9_21_mask.npz\n",
      "336 1_9_22_mask.npz\n",
      "337 1_9_23_mask.npz\n",
      "338 1_9_24_mask.npz\n",
      "339 1_9_25_mask.npz\n",
      "340 1_9_2_mask.npz\n",
      "341 1_9_3_mask.npz\n",
      "342 1_9_4_mask.npz\n",
      "343 1_9_5_mask.npz\n",
      "344 1_9_6_mask.npz\n",
      "345 1_9_7_mask.npz\n",
      "346 1_9_8_mask.npz\n",
      "347 1_9_9_mask.npz\n",
      "348 37_0_10_mask.npz\n",
      "349 37_0_11_mask.npz\n",
      "350 37_0_12_mask.npz\n",
      "351 37_0_13_mask.npz\n",
      "352 37_0_14_mask.npz\n",
      "353 37_0_15_mask.npz\n",
      "354 37_0_1_mask.npz\n",
      "355 37_0_2_mask.npz\n",
      "356 37_0_3_mask.npz\n",
      "357 37_0_4_mask.npz\n",
      "358 37_0_5_mask.npz\n",
      "359 37_0_6_mask.npz\n",
      "360 37_0_7_mask.npz\n",
      "361 37_0_8_mask.npz\n",
      "362 37_0_9_mask.npz\n",
      "363 37_1_0_mask.npz\n",
      "364 37_1_10_mask.npz\n",
      "365 37_1_11_mask.npz\n",
      "366 37_1_12_mask.npz\n",
      "367 37_1_13_mask.npz\n",
      "368 37_1_14_mask.npz\n",
      "369 37_1_15_mask.npz\n",
      "370 37_1_1_mask.npz\n",
      "371 37_1_2_mask.npz\n",
      "372 37_1_3_mask.npz\n",
      "373 37_1_4_mask.npz\n",
      "374 37_1_5_mask.npz\n",
      "375 37_1_6_mask.npz\n",
      "376 37_1_7_mask.npz\n",
      "377 37_1_8_mask.npz\n",
      "378 37_1_9_mask.npz\n",
      "379 37_2_0_mask.npz\n",
      "380 37_2_10_mask.npz\n",
      "381 37_2_11_mask.npz\n",
      "382 37_2_12_mask.npz\n",
      "383 37_2_13_mask.npz\n",
      "384 37_2_14_mask.npz\n",
      "385 37_2_15_mask.npz\n",
      "386 37_2_1_mask.npz\n",
      "387 37_2_2_mask.npz\n",
      "388 37_2_3_mask.npz\n",
      "389 37_2_4_mask.npz\n",
      "390 37_2_5_mask.npz\n",
      "391 37_2_6_mask.npz\n",
      "392 37_2_7_mask.npz\n",
      "393 37_2_8_mask.npz\n",
      "394 37_2_9_mask.npz\n",
      "395 37_3_0_mask.npz\n",
      "396 37_3_10_mask.npz\n",
      "397 37_3_11_mask.npz\n",
      "398 37_3_12_mask.npz\n",
      "399 37_3_13_mask.npz\n",
      "400 37_3_14_mask.npz\n",
      "401 37_3_15_mask.npz\n",
      "402 37_3_1_mask.npz\n",
      "403 37_3_2_mask.npz\n",
      "404 37_3_3_mask.npz\n",
      "405 37_3_4_mask.npz\n",
      "406 37_3_5_mask.npz\n",
      "407 37_3_6_mask.npz\n",
      "408 37_3_7_mask.npz\n",
      "409 37_3_8_mask.npz\n",
      "410 37_3_9_mask.npz\n",
      "411 46_0_0_mask.npz\n",
      "412 46_0_10_mask.npz\n",
      "413 46_0_1_mask.npz\n",
      "414 46_0_2_mask.npz\n",
      "415 46_0_3_mask.npz\n",
      "416 46_0_4_mask.npz\n",
      "417 46_0_5_mask.npz\n",
      "418 46_0_6_mask.npz\n",
      "419 46_0_7_mask.npz\n",
      "420 46_0_8_mask.npz\n",
      "421 46_0_9_mask.npz\n",
      "422 46_1_0_mask.npz\n",
      "423 46_1_10_mask.npz\n",
      "424 46_1_1_mask.npz\n",
      "425 46_1_2_mask.npz\n",
      "426 46_1_3_mask.npz\n",
      "427 46_1_4_mask.npz\n",
      "428 46_1_5_mask.npz\n",
      "429 46_1_6_mask.npz\n",
      "430 46_1_7_mask.npz\n",
      "431 46_1_8_mask.npz\n",
      "432 46_1_9_mask.npz\n",
      "433 46_2_0_mask.npz\n",
      "434 46_2_10_mask.npz\n",
      "435 46_2_1_mask.npz\n",
      "436 46_2_2_mask.npz\n",
      "437 46_2_3_mask.npz\n",
      "438 46_2_4_mask.npz\n",
      "439 46_2_5_mask.npz\n",
      "440 46_2_6_mask.npz\n",
      "441 46_2_7_mask.npz\n",
      "442 46_2_8_mask.npz\n",
      "443 46_2_9_mask.npz\n",
      "444 46_3_0_mask.npz\n",
      "445 46_3_10_mask.npz\n",
      "446 46_3_11_mask.npz\n",
      "447 46_3_1_mask.npz\n",
      "448 46_3_2_mask.npz\n",
      "449 46_3_3_mask.npz\n",
      "450 46_3_4_mask.npz\n",
      "451 46_3_5_mask.npz\n",
      "452 46_3_6_mask.npz\n",
      "453 46_3_7_mask.npz\n",
      "454 46_3_8_mask.npz\n",
      "455 46_3_9_mask.npz\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(mask_list):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d5d6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37_2_7_mask.npz\n"
     ]
    }
   ],
   "source": [
    "for j in range(455):\n",
    "    mask_list = list(sorted(os.listdir(os.path.join('/project/r/rbond/jorlo/datasets/ACT_tiles/', \"masks\"))))\n",
    "    mask_path = os.path.join('/project/r/rbond/jorlo/datasets/ACT_tiles/', \"masks\", mask_list[j])\n",
    "    with np.load(mask_path) as data:\n",
    "        mask = data['arr_0']\n",
    "    # convert the PIL Image into a numpy array\n",
    "    mask = np.array(mask)\n",
    "    # instances are encoded as different colors\n",
    "    obj_ids = np.unique(mask)\n",
    "    # first id is the background, so remove it\n",
    "    obj_ids = obj_ids[1:]\n",
    "    # split the color-encoded mask into a set\n",
    "    # of binary masks\n",
    "    masks = mask == obj_ids[:, None, None]\n",
    "    # get bounding box coordinates for each mask\n",
    "    num_objs = len(obj_ids)\n",
    "    boxes = []\n",
    "    for i in range(num_objs):\n",
    "        pos = np.where(masks[i])\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        if xmin==xmax and ymin == ymax:\n",
    "            print(mask_list[j])\n",
    "        \n",
    "        \n",
    "\n",
    "    # convert everything into a torch.Tensor\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b26c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/project/r/rbond/jorlo/datasets/ACT_tiles/'\n",
    "tile = '1_0_0'\n",
    "\n",
    "\n",
    "with np.load(path+'freq_tiles/'+tile+'.npz') as data:\n",
    "        img = data['arr_0']\n",
    "with np.load(path+'masks/'+tile+'_mask.npz') as data:\n",
    "        mask = data['arr_0']\n",
    "img = np.array(img)\n",
    "mask = np.array(mask)\n",
    "# instances are encoded as different colors\n",
    "obj_ids = np.unique(mask)\n",
    "# first id is the background, so remove it\n",
    "obj_ids = obj_ids[1:]\n",
    "# split the color-encoded mask into a set\n",
    "# of binary masks\n",
    "masks = mask == obj_ids[:, None, None]\n",
    "# get bounding box coordinates for each mask\n",
    "num_objs = len(obj_ids)\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(masks[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22bf149b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 634, 72, 638],\n",
       " [383, 840, 387, 844],\n",
       " [466, 137, 470, 141],\n",
       " [583, 810, 587, 814],\n",
       " [657, 300, 661, 304],\n",
       " [818, 387, 822, 391],\n",
       " [889, 634, 893, 638],\n",
       " [975, 603, 979, 607]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f642f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffda8f44eb0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAATUElEQVR4nO3df6jV9f3A8df1mkeN620ZVxSvpjDQtPDHlZFaaxSO0kgYbYW1qA0mu/5KCHW2jdz04n6IkNO4MsRNNP/YIgdrTRppziS7asU2lC3Iu5y4RtzbD7jl9Xz/+H53+d5ZznP15Tnn9njA54/74fPx8+Kj+OR9PueeU1MsFosBAJfZgHIPAED/JDAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQYuCVvuC5c+fi1KlTUVdXFzU1NVf68gBcgmKxGO+9916MGjUqBgy48Brligfm1KlT0djYeKUvC8Bl1N7eHqNHj77gMVc8MHV1dRERMTvuioFx1ZW+PACX4Gx8HAfitz3/l1/IFQ/Mv18WGxhXxcAagQGoKv/36ZUX84jDQ34AUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFH0KzObNm2PcuHExePDgmD59erz00kuXey4AqlzJgdm9e3csW7YsVq9eHUePHo1bbrkl7rzzzjh58mTGfABUqZIDs2HDhvjGN74R3/zmN2PixImxcePGaGxsjC1btmTMB0CVKikwH330UbS1tcWcOXN67Z8zZ04cPHjwE8/p6uqKzs7OXhsA/V9JgXnnnXeiu7s7RowY0Wv/iBEj4vTp0594TktLS9TX1/dsvs0S4LOhTw/5//OLZorF4qd++cyqVauio6OjZ2tvb+/LJQGoMiV9o+V1110XtbW1561Wzpw5c96q5t8KhUIUCoW+TwhAVSppBTNo0KCYPn167N27t9f+vXv3xsyZMy/rYABUt5JWMBERy5cvjwcffDCampri5ptvjtbW1jh58mQsXLgwYz4AqlTJgfna174W//rXv2LNmjXxj3/8IyZPnhy//e1vY+zYsRnzAVClaorFYvFKXrCzszPq6+vjtrgnBtZcdSUvDcAlOlv8OF6MZ6OjoyOGDRt2wWN9FhkAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApCgpMC0tLTFjxoyoq6uLhoaGmD9/fhw/fjxrNgCqWEmB2bdvXzQ3N8ehQ4di7969cfbs2ZgzZ0588MEHWfMBUKUGlnLw7373u14/b9u2LRoaGqKtrS1uvfXWyzoYANWtpMD8p46OjoiIuPbaaz/1mK6urujq6ur5ubOz81IuCUCV6PND/mKxGMuXL4/Zs2fH5MmTP/W4lpaWqK+v79kaGxv7ekkAqkifA7No0aJ4/fXXY9euXRc8btWqVdHR0dGztbe39/WSAFSRPr1Etnjx4tizZ0/s378/Ro8efcFjC4VCFAqFPg0HQPUqKTDFYjEWL14czzzzTLz44osxbty4rLkAqHIlBaa5uTl27twZzz77bNTV1cXp06cjIqK+vj6GDBmSMiAA1amkZzBbtmyJjo6OuO2222LkyJE92+7du7PmA6BKlfwSGQBcDJ9FBkAKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApLikwLS0tUVNTE8uWLbtM4wDQX/Q5MIcPH47W1ta46aabLuc8APQTfQrM+++/HwsWLIitW7fG5z73ucs9EwD9QJ8C09zcHHPnzo077rjjvx7b1dUVnZ2dvTYA+r+BpZ7w9NNPx5EjR+Lw4cMXdXxLS0s88cQTJQ8GQHUraQXT3t4eS5cujR07dsTgwYMv6pxVq1ZFR0dHz9be3t6nQQGoLiWtYNra2uLMmTMxffr0nn3d3d2xf//+2LRpU3R1dUVtbW2vcwqFQhQKhcszLQBVo6TA3H777fHGG2/02vfwww/HhAkTYsWKFefFBYDPrpICU1dXF5MnT+617+qrr47hw4eftx+Azza/yQ9AipLfRfafXnzxxcswBgD9jRUMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQIpL/iwyKLfnTx0r9wj00ZdHTSn3CCSyggEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBhY7gGoLs+fOlbuEehHKvHf05dHTSn3CP2GFQwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIUXJg3n777XjggQdi+PDhMXTo0JgyZUq0tbVlzAZAFSvp+2DefffdmDVrVnzpS1+K5557LhoaGuJvf/tbXHPNNUnjAVCtSgrM+vXro7GxMbZt29az7/rrr7/cMwHQD5T0EtmePXuiqakp7r333mhoaIipU6fG1q1bL3hOV1dXdHZ29toA6P9KCsybb74ZW7Zsic9//vPx/PPPx8KFC2PJkiXxi1/84lPPaWlpifr6+p6tsbHxkocGoPLVFIvF4sUePGjQoGhqaoqDBw/27FuyZEkcPnw4Xn755U88p6urK7q6unp+7uzsjMbGxrgt7omBNVddwuiUQyV+hzpcTl8eNaXcI1S0s8WP48V4Njo6OmLYsGEXPLakFczIkSPjhhtu6LVv4sSJcfLkyU89p1AoxLBhw3ptAPR/JQVm1qxZcfz48V77Tpw4EWPHjr2sQwFQ/UoKzKOPPhqHDh2KdevWxV//+tfYuXNntLa2RnNzc9Z8AFSpkgIzY8aMeOaZZ2LXrl0xefLk+MEPfhAbN26MBQsWZM0HQJUq6fdgIiLmzZsX8+bNy5gFgH7EZ5EBkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQIqSAnP27Nl4/PHHY9y4cTFkyJAYP358rFmzJs6dO5c1HwBVamApB69fvz6eeuqp2L59e0yaNCleffXVePjhh6O+vj6WLl2aNSMAVaikwLz88stxzz33xNy5cyMi4vrrr49du3bFq6++mjIcANWrpJfIZs+eHS+88EKcOHEiIiJee+21OHDgQNx1112fek5XV1d0dnb22gDo/0pawaxYsSI6OjpiwoQJUVtbG93d3bF27dq4//77P/WclpaWeOKJJy55UACqS0krmN27d8eOHTti586dceTIkdi+fXv85Cc/ie3bt3/qOatWrYqOjo6erb29/ZKHBqDylbSCeeyxx2LlypVx3333RUTEjTfeGG+99Va0tLTEQw899InnFAqFKBQKlz4pAFWlpBXMhx9+GAMG9D6ltrbW25QBOE9JK5i777471q5dG2PGjIlJkybF0aNHY8OGDfHII49kzQdAlSopME8++WR897vfjW9/+9tx5syZGDVqVHzrW9+K733ve1nzAVClSgpMXV1dbNy4MTZu3Jg0DgD9hc8iAyCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEhR0meRwZdHTSn3COd5/tSxco9AH1XivycuHysYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQCA0AKgQEghcAAkEJgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFIIDAApBAaAFAIDQAqBASCFwACQQmAASCEwAKQQGABSCAwAKQQGgBQDr/QFi8ViREScjY8jilf66vRHne+dK/cI9NHZ4sflHoESnY3//Tv79//lF1JTvJijLqO///3v0djYeCUvCcBl1t7eHqNHj77gMVc8MOfOnYtTp05FXV1d1NTU9PnP6ezsjMbGxmhvb49hw4Zdxgn7F/fp4rhPF8d9ujj9+T4Vi8V47733YtSoUTFgwIWfslzxl8gGDBjwX6tXimHDhvW7v8AM7tPFcZ8ujvt0cfrrfaqvr7+o4zzkByCFwACQomoDUygU4vvf/34UCoVyj1LR3KeL4z5dHPfp4rhP/+uKP+QH4LOhalcwAFQ2gQEghcAAkEJgAEhRtYHZvHlzjBs3LgYPHhzTp0+Pl156qdwjVZSWlpaYMWNG1NXVRUNDQ8yfPz+OHz9e7rEqWktLS9TU1MSyZcvKPUrFefvtt+OBBx6I4cOHx9ChQ2PKlCnR1tZW7rEqytmzZ+Pxxx+PcePGxZAhQ2L8+PGxZs2aOHfus/tZeVUZmN27d8eyZcti9erVcfTo0bjlllvizjvvjJMnT5Z7tIqxb9++aG5ujkOHDsXevXvj7NmzMWfOnPjggw/KPVpFOnz4cLS2tsZNN91U7lEqzrvvvhuzZs2Kq666Kp577rn485//HD/96U/jmmuuKfdoFWX9+vXx1FNPxaZNm+Ivf/lL/OhHP4of//jH8eSTT5Z7tLKpyrcpf+ELX4hp06bFli1bevZNnDgx5s+fHy0tLWWcrHL985//jIaGhti3b1/ceuut5R6norz//vsxbdq02Lx5c/zwhz+MKVOmxMaNG8s9VsVYuXJl/PGPf/QqwX8xb968GDFiRPz85z/v2feVr3wlhg4dGr/85S/LOFn5VN0K5qOPPoq2traYM2dOr/1z5syJgwcPlmmqytfR0REREddee22ZJ6k8zc3NMXfu3LjjjjvKPUpF2rNnTzQ1NcW9994bDQ0NMXXq1Ni6dWu5x6o4s2fPjhdeeCFOnDgRERGvvfZaHDhwIO66664yT1Y+V/zDLi/VO++8E93d3TFixIhe+0eMGBGnT58u01SVrVgsxvLly2P27NkxefLkco9TUZ5++uk4cuRIHD58uNyjVKw333wztmzZEsuXL4/vfOc78corr8SSJUuiUCjE17/+9XKPVzFWrFgRHR0dMWHChKitrY3u7u5Yu3Zt3H///eUerWyqLjD/9p8f9V8sFi/p4//7s0WLFsXrr78eBw4cKPcoFaW9vT2WLl0av//972Pw4MHlHqdinTt3LpqammLdunURETF16tT405/+FFu2bBGY/2f37t2xY8eO2LlzZ0yaNCmOHTsWy5Yti1GjRsVDDz1U7vHKouoCc91110Vtbe15q5UzZ86ct6ohYvHixbFnz57Yv3//Zf2ahP6gra0tzpw5E9OnT+/Z193dHfv3749NmzZFV1dX1NbWlnHCyjBy5Mi44YYbeu2bOHFi/OpXvyrTRJXpsccei5UrV8Z9990XERE33nhjvPXWW9HS0vKZDUzVPYMZNGhQTJ8+Pfbu3dtr/969e2PmzJllmqryFIvFWLRoUfz617+OP/zhDzFu3Lhyj1Rxbr/99njjjTfi2LFjPVtTU1MsWLAgjh07Ji7/Z9asWee9xf3EiRMxduzYMk1UmT788MPzvoCrtrb2M/025apbwURELF++PB588MFoamqKm2++OVpbW+PkyZOxcOHCco9WMZqbm2Pnzp3x7LPPRl1dXc+Kr76+PoYMGVLm6SpDXV3dec+krr766hg+fLhnVf/Po48+GjNnzox169bFV7/61XjllVeitbU1Wltbyz1aRbn77rtj7dq1MWbMmJg0aVIcPXo0NmzYEI888ki5RyufYpX62c9+Vhw7dmxx0KBBxWnTphX37dtX7pEqSkR84rZt27Zyj1bRvvjFLxaXLl1a7jEqzm9+85vi5MmTi4VCoThhwoRia2truUeqOJ2dncWlS5cWx4wZUxw8eHBx/PjxxdWrVxe7urrKPVrZVOXvwQBQ+aruGQwA1UFgAEghMACkEBgAUggMACkEBoAUAgNACoEBIIXAAJBCYABIITAApBAYAFL8D8KZQZrbmRXWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask[835:845,380:390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eea0247f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffdc28f9b20>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[835:845,380:390,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970eafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "ml-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3'\n",
    "pos_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/clusters/downsampled_cluster_images.pk', 'rb'))\n",
    "neg_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/downsampled_randoms_images.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(test_data):\n",
    "    train_predictions_baseline = model.predict(test_data)\n",
    "    freezeout = 0\n",
    "    for test_subset in test_data.take(1):\n",
    "        test_predictions_baseline = model.predict(test_subset)\n",
    "\n",
    "        freezeout = test_subset\n",
    "        test_labels = test_subset[1].numpy()\n",
    "    for i in range(len(test_labels)):\n",
    "        if test_labels[i] != np.argmax(tf.nn.softmax(test_predictions_baseline), axis = 1)[i]:\n",
    "            print('Sample {} misclasified with probability {}'.format(i, np.max(tf.nn.softmax(test_predictions_baseline), axis = 1)[i]))\n",
    "    \n",
    "    return freezeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3', no manual image inspection\n",
    "pos_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/clusters/downsampled_nocuts_cluster_images.pk', 'rb'))\n",
    "neg_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/downsampled_nocuts_randoms_images.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3', no manual image inspection\n",
    "pos_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/clusters/downsampled_nocuts_cluster_images.pk', 'rb'))\n",
    "neg_train_images = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/downsampled_nocuts_randoms_images.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_im_num = pos_train_images.shape[0]+neg_train_images.shape[0]\n",
    "reduce_num = 0\n",
    "smooth = 0\n",
    "#initialize a new array of px, py, bands images, total number neg+pos\n",
    "if reduce_num: \n",
    "    all_images = np.zeros((total_im_num, int(pos_train_images.shape[1]/reduce_num),int(pos_train_images.shape[2]/reduce_num),pos_train_images.shape[3]))\n",
    "else:\n",
    "    all_images = np.zeros((total_im_num, pos_train_images.shape[1], pos_train_images.shape[2],pos_train_images.shape[3]))\n",
    "    \n",
    "i = 0\n",
    "#put the true clusters first and the randoms second\n",
    "while i < pos_train_images.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (pos_train_images[i,...,j]-np.mean(pos_train_images[i,...,j]))/np.std(pos_train_images[i,...,j])\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        all_images[i,...,j] = temp\n",
    "    i += 1\n",
    "\n",
    "while i < total_im_num:\n",
    "    #all_images[i,...] = np.log(np.abs(neg_train_images[i- pos_train_images.shape[0], ...]))\n",
    "    for j in range(5):\n",
    "        cur_im = neg_train_images[i- pos_train_images.shape[0], ...,j]\n",
    "        temp = (cur_im - np.mean(cur_im))/np.std(cur_im)\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        all_images[i,...,j] = temp\n",
    "    i += 1\n",
    "\n",
    "#Make image labels\n",
    "labels = pos_train_images.shape[0]*[1]+neg_train_images.shape[0]*[0]\n",
    "all_max = np.amax(all_images)\n",
    "input_shape = (all_images.shape[1], all_images.shape[2], all_images.shape[3])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_rgb = np.zeros((total_im_num,pos_train_images.shape[1],pos_train_images.shape[2],3))\n",
    "i = 0\n",
    "#put the true clusters first and the randoms second\n",
    "while i < pos_train_images.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(3):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        all_images_rgb[i,...,j] = (pos_train_images[i,...,j]-np.mean(pos_train_images[i,...,j]))/np.std(pos_train_images[i,...,j])\n",
    "    i += 1\n",
    "\n",
    "while i < total_im_num:\n",
    "    #all_images[i,...] = np.log(np.abs(neg_train_images[i- pos_train_images.shape[0], ...]))\n",
    "    for j in range(3):\n",
    "        cur_im = neg_train_images[i- pos_train_images.shape[0], ...,j]\n",
    "        all_images_rgb[i,...,j] = (cur_im - np.mean(cur_im))/np.std(cur_im)\n",
    "    i += 1\n",
    "\n",
    "#Make image labels\n",
    "labels = pos_train_images.shape[0]*[1]+neg_train_images.shape[0]*[0]\n",
    "all_max = np.amax(all_images)\n",
    "input_shape_rgb = (all_images_rgb.shape[1], all_images_rgb.shape[2], all_images_rgb.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_images, labels))\n",
    "\n",
    "train_size = int(0.7 * total_im_num)\n",
    "val_size = int(0.15 * total_im_num)\n",
    "test_size = int(0.15 * total_im_num)\n",
    "\n",
    "dataset = dataset.shuffle(total_im_num)\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = dataset.skip(val_size)\n",
    "test_dataset = dataset.take(test_size)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rgb = tf.data.Dataset.from_tensor_slices((all_images_rgb, labels))\n",
    "\n",
    "train_size = int(0.7 * total_im_num)\n",
    "val_size = int(0.15 * total_im_num)\n",
    "test_size = int(0.15 * total_im_num)\n",
    "\n",
    "dataset_rgb = dataset_rgb.shuffle(total_im_num)\n",
    "train_dataset_rgb = dataset_rgb.take(train_size)\n",
    "\n",
    "test_dataset_rgb = dataset_rgb.skip(train_size)\n",
    "val_dataset_rgb = dataset_rgb.skip(val_size)\n",
    "test_dataset_rgb = dataset_rgb.take(test_size)\n",
    "\n",
    "train_dataset_rgb = train_dataset_rgb.shuffle(buffer_size=1024).batch(64)\n",
    "val_dataset_rgb = val_dataset_rgb.shuffle(buffer_size=1024).batch(64)\n",
    "test_dataset_rgb = test_dataset_rgb.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_mode('simple_cnn')\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset, epochs = 10, validation_data = val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True randoms\n",
    "randoms = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/more-randoms/uncut_morerandoms_images.pk', 'rb'))\n",
    "randoms_labels = np.array([0]*randoms.shape[0])\n",
    "\n",
    "randoms_baseline = model.predict(randoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(randoms_baseline.shape[0]):\n",
    "    indv_pred = randoms_baseline[i]\n",
    "    print(np.argmax(tf.nn.softmax(indv_pred)), 100*np.max(tf.nn.softmax(indv_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x = randoms, y = randoms_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redmapper\n",
    "redmapper = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/redmapper/uncut_redmapper_images.pk', 'rb'))\n",
    "redmapper_labels = np.array([1]*redmapper.shape[0])\n",
    "\n",
    "randoms_baseline = model.predict(redmapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x = redmapper, y = redmapper_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indv_pred = model.predict((np.expand_dims(reserve, axis = 0)))\n",
    "print(np.argmax(tf.nn.softmax(indv_pred)), 100*np.max(tf.nn.softmax(indv_pred)))\n",
    "plt.imshow(reserve[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in test_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        \n",
    "        print(truth.numpy(), np.argmax(tf.nn.softmax(pred)[0]), 100*np.max(tf.nn.softmax(pred)[0]))\n",
    "#predictions = model.predict(test_dataset)\n",
    "#print(tf.nn.softmax(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in test_dataset:\n",
    "    print(thing[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        weights, bias= layer.get_weights()\n",
    "        #print(layer.name, filters.shape)\n",
    "        \n",
    "        #normalize filter values between  0 and 1 for visualization\n",
    "        f_min, f_max = weights.min(), weights.max()\n",
    "        filters = (weights - f_min) / (f_max - f_min)  \n",
    "        print(filters.shape[3])\n",
    "        filter_cnt=1\n",
    "        \n",
    "        #plotting all the filters\n",
    "        for i in range(filters.shape[3]):\n",
    "            #get the filters\n",
    "            filt=filters[:,:,:, i]\n",
    "            #plotting each of the channel, color image RGB channels\n",
    "            for j in range(filters.shape[0]):\n",
    "                ax= plt.subplot(filters.shape[3], filters.shape[0], filter_cnt  )\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                plt.imshow(filt[:,:, j])\n",
    "                filter_cnt+=1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "#Load the input image\n",
    "test_data =np.concatenate([x for x, y in test_dataset], axis=0)\n",
    "\n",
    "x = np.expand_dims(reserve, axis = 0)\n",
    "# Let's run input image through our vislauization network\n",
    "# to obtain all intermediate representations for the image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "# Retrieve are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  print(feature_map.shape)\n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    # Plot Feature maps for the conv / maxpool layers, not the fully-connected layers\n",
    "   \n",
    "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "    \n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    # Postprocess the feature to be visually palatable\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      # Tile each filter into a horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "# Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis' )\n",
    "    plt.savefig('plots/gpu_test/feature_map/{}.pdf'.format(layer_name))\n",
    "    \n",
    "\n",
    "\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    print(i)\n",
    "    print(test_labels[i])\n",
    "    print(tf.nn.softmax(predictions[i]))\n",
    "    for j in range(5):\n",
    "        plt.imshow(np.log(np.abs(test_data[i,..., j])))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_samples(sample1, sample2, reduce_num = 0, smooth = 0):\n",
    "    total_im_num = sample1.shape[0]+sample2.shape[0]\n",
    "    if reduce_num: \n",
    "        all_images = np.zeros((total_im_num, int(sample1.shape[1]/reduce_num),int(sample1.shape[2]/reduce_num),sample1.shape[3]))\n",
    "    else:\n",
    "        all_images = np.zeros((total_im_num, sample1.shape[1], sample1.shape[2],sample1.shape[3]))\n",
    "\n",
    "    i = 0\n",
    "    k = 0\n",
    "    #put the true clusters first and the randoms second\n",
    "    while i < sample1.shape[0]:\n",
    "        #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "        for j in range(5):\n",
    "            #Normalize images by subtracking mean then dividing by std\n",
    "            temp = (sample1[i,...,j]-np.mean(sample1[i,...,j]))/np.std(sample1[i,...,j])\n",
    "            if reduce_num:\n",
    "                temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "            if smooth:\n",
    "                kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "                temp = convolve(temp, kernel)\n",
    "            #print(np.amax(temp))\n",
    "            #print(i)\n",
    "            all_images[i,...,j] = temp\n",
    "        i += 1\n",
    "\n",
    "    k += i\n",
    "    i = 0\n",
    "    while i < sample2.shape[0]:\n",
    "        #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "        for j in range(5):\n",
    "            #Normalize images by subtracking mean then dividing by std\n",
    "            temp = (sample2[i,...,j]-np.mean(sample2[i,...,j]))/np.std(sample2[i,...,j])\n",
    "            if reduce_num:\n",
    "                temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "            if smooth:\n",
    "                kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "                temp = convolve(temp, kernel)\n",
    "            #print(np.amax(temp))\n",
    "            #print(i)\n",
    "            all_images[k + i,...,j] = temp\n",
    "        i += 1\n",
    "    labels = sample1.shape[0]*[1] + sample2.shape[0]*[0]\n",
    "    return all_images, np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Use data augmentation only on the training set\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "    # Use buffered prefecting on all datasets\n",
    "    return ds.prefetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3', no manual image inspection\n",
    "\n",
    "\n",
    "#pos_train_images_act = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/clusters/downsampled_nocuts_cluster_images.pk', 'rb'))\n",
    "pos_train_images_act = np.random.rand(1,171,171,5)\n",
    "pos_train_images_des = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/redmapper/uncut_redmapper_images.pk', 'rb'))\n",
    "#pos_train_images_des = np.random.rand(1,171,171,5)\n",
    "#neg_train_images_1 = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/downsampled_nocuts_randoms_images.pk', 'rb'))\n",
    "neg_train_images_1 = np.random.rand(1,171,171,5)\n",
    "neg_train_images_2 = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/more-randoms/uncut_morerandoms_images.pk', 'rb'))\n",
    "#neg_train_images_2 = np.zeros((1,171,171,5))\n",
    "\n",
    "pos_im = np.concatenate((pos_train_images_des,pos_train_images_act), axis = 0)\n",
    "neg_im = np.concatenate((neg_train_images_1,neg_train_images_2))\n",
    "\n",
    "train_size = int(0.7 * pos_im.shape[0])\n",
    "val_size = int(0.15 * pos_im.shape[0])\n",
    "test_size = int(0.15 * pos_im.shape[0])\n",
    "train_pos = pos_im[:train_size]\n",
    "val_pos = pos_im[train_size:train_size + val_size]\n",
    "test_pos = pos_im[train_size + val_size:]\n",
    "\n",
    "train_size = int(0.7 * neg_im.shape[0])\n",
    "val_size = int(0.15 * neg_im.shape[0])\n",
    "test_size = int(0.15 * neg_im.shape[0])\n",
    "train_neg = neg_im[:train_size]\n",
    "val_neg = neg_im[train_size:train_size + val_size]\n",
    "test_neg = neg_im[train_size + val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_im_num = pos_train_images_act.shape[0] + pos_train_images_des.shape[0] + neg_train_images_1.shape[0] + neg_train_images_2.shape[0]\n",
    "reduce_num = 0\n",
    "smooth = 0\n",
    "#initialize a new array of px, py, bands images, total number neg+pos\n",
    "if reduce_num: \n",
    "    all_images = np.zeros((total_im_num, int(pos_train_images_act.shape[1]/reduce_num),int(pos_train_images_act.shape[2]/reduce_num),pos_train_images_act.shape[3]))\n",
    "else:\n",
    "    all_images = np.zeros((total_im_num, pos_train_images_act.shape[1], pos_train_images_act.shape[2],pos_train_images_act.shape[3]))\n",
    "    \n",
    "i = 0\n",
    "k = 0\n",
    "#put the true clusters first and the randoms second\n",
    "while i < pos_train_images_act.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (pos_train_images_act[i,...,j]-np.mean(pos_train_images_act[i,...,j]))/np.std(pos_train_images_act[i,...,j])\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        #print(np.amax(temp))\n",
    "        #print(i)\n",
    "        all_images[i,...,j] = temp\n",
    "    i += 1\n",
    "\n",
    "k += i\n",
    "i = 0\n",
    "while i < pos_train_images_des.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (pos_train_images_des[i,...,j]-np.mean(pos_train_images_des[i,...,j]))/np.std(pos_train_images_des[i,...,j])\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        #print(np.amax(temp))\n",
    "        #print(i)\n",
    "        all_images[k + i,...,j] = temp\n",
    "    i += 1\n",
    "    \n",
    "k += i\n",
    "i = 0  \n",
    "while i < neg_train_images_1.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (neg_train_images_1[i,...,j]-np.mean(neg_train_images_1[i,...,j]))/np.std(neg_train_images_1[i,...,j])\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        #print(np.amax(temp))\n",
    "        #print(i)\n",
    "        all_images[k + i,...,j] = temp\n",
    "    i += 1\n",
    "    \n",
    "k += i\n",
    "i = 0 \n",
    "while i < neg_train_images_2.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (neg_train_images_2[i,...,j]-np.mean(neg_train_images_2[i,...,j]))/np.std(neg_train_images_2[i,...,j])\n",
    "        if reduce_num:\n",
    "            temp = block_reduce(temp, reduce_num, func=np.mean)\n",
    "        if smooth:\n",
    "            kernel = Gaussian2DKernel(x_stddev=smooth)\n",
    "\n",
    "            temp = convolve(temp, kernel)\n",
    "        all_images[k + i,...,j] = temp\n",
    "        #print(np.amax(temp))\n",
    "        #print(i)\n",
    "    i += 1\n",
    "#Make image labels\n",
    "labels = pos_train_images_act.shape[0]*[1] + pos_train_images_des.shape[0]*[1] + neg_train_images_1.shape[0]*[0] + neg_train_images_2.shape[0]*[0]\n",
    "input_shape = (all_images.shape[1], all_images.shape[2], all_images.shape[3])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample, train_labels = combine_samples(train_pos, train_neg)\n",
    "val_sample, val_labels = combine_samples(val_pos, val_neg)\n",
    "test_sample, test_labels = combine_samples(test_pos, test_neg)\n",
    "input_shape = (train_sample.shape[1], train_sample.shape[2], train_sample.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([8, 16, 32, 64]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.1))\n",
    "\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_images, labels))\n",
    "\n",
    "train_size = int(0.7 * total_im_num)\n",
    "val_size = int(0.15 * total_im_num)\n",
    "test_size = int(0.15 * total_im_num)\n",
    "\n",
    "dataset = dataset.shuffle(total_im_num).cache()\n",
    "train_dataset = dataset.take(train_size).cache()\n",
    "test_dataset = dataset.skip(train_size).take(test_size).cache()\n",
    "val_dataset = dataset.skip(train_size+test_size).take(val_size).cache()\n",
    "\n",
    "STEPS_PER_EPOCH = int(train_size/batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).repeat().batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sample, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sample, val_labels))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sample, test_labels))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=360,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    horizontal_flip=True, \n",
    "    vertical_flip = True\n",
    ")\n",
    "\n",
    "datagen.fit(train_sample)\n",
    "history = model.fit_generator(datagen.flow(train_sample, train_labels, batch_size=batch_size), steps_per_epoch=len(train_sample) / batch_size, epochs=10, validation_data = val_dataset)\n",
    "\n",
    "\n",
    "#history = model.fit_generator(train_dataset, epochs = 10, validation_data = val_dataset)\n",
    "#history = model.fit_generator(train_dataset, epochs = 10, validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(val_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            hparams = {\n",
    "              HP_NUM_UNITS: num_units,\n",
    "              HP_DROPOUT: dropout_rate,\n",
    "              HP_OPTIMIZER: optimizer,\n",
    "              }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('logs/hparam_tuning/' + run_name, hparams)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True randoms\n",
    "#raw_randoms = np.concatenate((reserve_pos,reserve_neg))\n",
    "raw_randoms = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/downsampled_nocuts_randoms_images.pk', 'rb'))\n",
    "#raw_randoms = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/more-randoms/uncut_morerandoms_images.pk', 'rb'))\n",
    "randoms_labels = np.array([0]*raw_randoms.shape[0])\n",
    "randoms = np.zeros(raw_randoms.shape)\n",
    "i=0\n",
    "while i < randoms.shape[0]:\n",
    "    #all_images[i,...] = np.log(np.abs(pos_train_images[i,...]))\n",
    "    for j in range(5):\n",
    "        #Normalize images by subtracking mean then dividing by std\n",
    "        temp = (raw_randoms[i,...,j]-np.mean(raw_randoms[i,...,j]))/np.std(raw_randoms[i,...,j])\n",
    "        randoms[i,...,j] = temp\n",
    "    i += 1\n",
    "    \n",
    "randoms_baseline = model.predict(randoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = np.zeros((randoms.shape[0],5))\n",
    "sums = np.zeros((randoms.shape[0],5))\n",
    "probs = []\n",
    "\n",
    "for i in range(randoms_baseline.shape[0]):\n",
    "    indv_pred = randoms_baseline[i]\n",
    "    print(np.argmax(tf.nn.softmax(indv_pred)), 100*np.max(tf.nn.softmax(indv_pred)))\n",
    "    #plt.imshow(np.log(np.abs(imap[...])))\n",
    "    #plt.show()\n",
    "    #plt.close()   \n",
    "    probs.append(tf.nn.softmax(indv_pred).numpy()[0])\n",
    "    for j in range(5):\n",
    "        imap = randoms[i,...,j]\n",
    "        plt.imshow(np.log(np.abs(imap[...])))\n",
    "        plt.show()\n",
    "        plt.close()   \n",
    "        maxes[i,j] = np.amax(imap)\n",
    "        sums[i,j] = np.sum(imap)\n",
    "probs = np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in test_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        imap = thing[0][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        print(truth.numpy(), np.argmax(tf.nn.softmax(pred)[0]), 100*np.max(tf.nn.softmax(pred)[0]))\n",
    "        plt.imshow(np.log(np.abs(imap[...,3])))\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3', no manual image inspection\n",
    "\n",
    "\n",
    "pos_train_images = pk.load(open('/home/r/rbond/jorlo/dev/ML-clusters/pks/act_clusters_real.pk', 'rb'))\n",
    "\n",
    "neg_train_images = pk.load(open('/home/r/rbond/jorlo/dev/ML-clusters/pks/randoms_real.pk', 'rb'))\n",
    "\n",
    "\n",
    "train_size = int(0.7 * pos_train_images.shape[0])\n",
    "val_size = int(0.15 * pos_train_images.shape[0])\n",
    "test_size = int(0.15 * pos_train_images.shape[0])\n",
    "train_pos = pos_train_images[:train_size]\n",
    "val_pos = pos_train_images[train_size:train_size + val_size]\n",
    "test_pos = pos_train_images[train_size + val_size:]\n",
    "\n",
    "train_size = int(0.7 * neg_train_images.shape[0])\n",
    "val_size = int(0.15 * neg_train_images.shape[0])\n",
    "test_size = int(0.15 * neg_train_images.shape[0])\n",
    "train_neg = neg_train_images[:train_size]\n",
    "val_neg = neg_train_images[train_size:train_size + val_size]\n",
    "test_neg = neg_train_images[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.concatenate((train_pos,train_neg))\n",
    "val_dataset = np.concatenate((val_pos,val_neg))\n",
    "test_dataset = np.concatenate((test_pos,test_neg))\n",
    "\n",
    "train_labels = np.array(train_pos.shape[0]*[1] + train_neg.shape[0]*[0])\n",
    "val_labels = np.array(val_pos.shape[0]*[1] + val_neg.shape[0]*[0])\n",
    "test_labels = np.array(test_pos.shape[0]*[1] + test_neg.shape[0]*[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model('dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, train_labels, epochs=10, \n",
    "                    validation_data=(val_dataset, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampled images, 3x3', no manual image inspection\n",
    "\n",
    "\n",
    "pos_train_images = pk.load(open('/home/r/rbond/jorlo/dev/ML-clusters/pks/act_clusters_smooth.pk', 'rb'))\n",
    "\n",
    "neg_train_images = pk.load(open('/home/r/rbond/jorlo/dev/ML-clusters/pks/randoms_smooth.pk', 'rb'))\n",
    "\n",
    "for i in range(pos_train_images.shape[0]):\n",
    "    pos_train_images[i,...] = normalize_map(pos_train_images[i,...])\n",
    "for i in range(neg_train_images.shape[0]):\n",
    "    neg_train_images[i,...] = normalize_map(neg_train_images[i,...])\n",
    "\n",
    "\n",
    "train_size = int(0.7 * pos_train_images.shape[0])\n",
    "val_size = int(0.15 * pos_train_images.shape[0])\n",
    "test_size = int(0.15 * pos_train_images.shape[0])\n",
    "train_pos = pos_train_images[:train_size]\n",
    "val_pos = pos_train_images[train_size:train_size + val_size]\n",
    "test_pos = pos_train_images[train_size + val_size:]\n",
    "\n",
    "train_size = int(0.7 * neg_train_images.shape[0])\n",
    "val_size = int(0.15 * neg_train_images.shape[0])\n",
    "test_size = int(0.15 * neg_train_images.shape[0])\n",
    "train_neg = neg_train_images[:train_size]\n",
    "val_neg = neg_train_images[train_size:train_size + val_size]\n",
    "test_neg = neg_train_images[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.concatenate((train_pos,train_neg))\n",
    "val_dataset = np.concatenate((val_pos,val_neg))\n",
    "test_dataset = np.concatenate((test_pos,test_neg))\n",
    "\n",
    "train_labels = np.array(train_pos.shape[0]*[1] + train_neg.shape[0]*[0])\n",
    "val_labels = np.array(val_pos.shape[0]*[1] + val_neg.shape[0]*[0])\n",
    "test_labels = np.array(test_pos.shape[0]*[1] + test_neg.shape[0]*[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, train_labels, epochs=10, \n",
    "                    validation_data=(val_dataset, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_im = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/redmapper/uncut_redmapper_images.pk', 'rb'))\n",
    "neg_im = pk.load(open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/more-randoms/uncut_morerandoms_images.pk', 'rb'))\n",
    "\n",
    "train_size = int(0.7 * pos_im.shape[0])\n",
    "val_size = int(0.15 * pos_im.shape[0])\n",
    "test_size = int(0.15 * pos_im.shape[0])\n",
    "train_pos = pos_im[:train_size]\n",
    "val_pos = pos_im[train_size:train_size + val_size]\n",
    "test_pos = pos_im[train_size + val_size:]\n",
    "\n",
    "train_size = int(0.7 * neg_im.shape[0])\n",
    "val_size = int(0.15 * neg_im.shape[0])\n",
    "test_size = int(0.15 * neg_im.shape[0])\n",
    "train_neg = neg_im[:train_size]\n",
    "val_neg = neg_im[train_size:train_size + val_size]\n",
    "test_neg = neg_im[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.concatenate((train_pos,train_neg))\n",
    "val_dataset = np.concatenate((val_pos,val_neg))\n",
    "test_dataset = np.concatenate((test_pos,test_neg))\n",
    "\n",
    "train_dataset_3d = train_dataset[...,:3]\n",
    "val_dataset_3d = val_dataset[...,:3]\n",
    "test_dataset_3d = test_dataset[...,:3]\n",
    "\n",
    "train_labels = np.array(train_pos.shape[0]*[1] + train_neg.shape[0]*[0])\n",
    "val_labels = np.array(val_pos.shape[0]*[1] + val_neg.shape[0]*[0])\n",
    "test_labels = np.array(test_pos.shape[0]*[1] + test_neg.shape[0]*[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_3d = tf.keras.applications.mobilenet.preprocess_input(train_dataset_3d)\n",
    "val_dataset_3d = tf.keras.applications.mobilenet.preprocess_input(val_dataset_3d)\n",
    "test_dataset_3d = tf.keras.applications.mobilenet.preprocess_input(test_dataset_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset_final = tf.data.Dataset.from_tensor_slices((train_dataset_3d, train_labels))\n",
    "val_dataset_final = tf.data.Dataset.from_tensor_slices((val_dataset_3d, val_labels))\n",
    "test_dataset_final = tf.data.Dataset.from_tensor_slices((test_dataset_3d, test_labels))\n",
    "\n",
    "\n",
    "train_dataset_final = train_dataset_final.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset_final = val_dataset_final.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset_final = test_dataset_final.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "\n",
    "base_model = tf.keras.applications.InceptionResNetV2(input_shape=(171,171,3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "image_batch, label_batch = next(iter(train_dataset_final))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(171, 171, 3))\n",
    "x = inputs\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset_final, epochs=10)              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_maker(name, type_im, scale = 3):\n",
    "    bands = ['g', 'i', 'r', 'z', 'y']\n",
    "    for band in bands:\n",
    "        if type_im.lower() == 'clusters':\n",
    "            hi_data = fits.open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/clusters/'+str(name)+'/{}_{}.fits'.format(name, band))\n",
    "        elif type_im.lower() == 'randoms':\n",
    "            hi_data = fits.open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/randoms/'+str(name)+'/{}_{}.fits'.format(name, band))\n",
    "        elif type_im.lower() == 'redmapper':\n",
    "            hi_data = fits.open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/redmapper/'+str(name)+'/{}_{}.fits'.format(name, band))\n",
    "        elif type_im.lower() == 'morerandoms':\n",
    "            hi_data = fits.open('/gpfs/fs0/project/r/rbond/jorlo/datasets/cluster-test/more-randoms/'+str(name)+'/{}_{}.fits'.format(name, band))\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            print('Image type must be valid')\n",
    "            return\n",
    "        header = hi_data[0].header\n",
    "        w = wcs.WCS(header)\n",
    "        hdata = hi_data[0].data\n",
    "        px = py = int(hdata.shape[0])\n",
    "        pix_width = 0.267*u.arcsec\n",
    "        pix_width.to(u.arcmin)\n",
    "        size = u.Quantity([scale, scale], u.arcmin)\n",
    "        \n",
    "        try:\n",
    "            cutout = Cutout2D(hdata, (px,py), size,  wcs = w).data\n",
    "        except:\n",
    "            return np.array((0,0))\n",
    "        #print('test')\n",
    "        #if 0 in cutout:\n",
    "            #print(cutout)\n",
    "            #return np.array((0,0))\n",
    "        if band == 'g':\n",
    "            temp = cutout\n",
    "        elif band == 'i':\n",
    "            temp = np.stack((temp, cutout), axis = -1)\n",
    "        else:\n",
    "            temp = np.append(temp, np.expand_dims(cutout, axis = -1), axis = -1)\n",
    "\n",
    "    return(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fc5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 16:06:41.844581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31016 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0004:04:00.0, compute capability: 7.0\n",
      "2022-12-05 16:06:41.847171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31016 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0004:05:00.0, compute capability: 7.0\n",
      "2022-12-05 16:06:41.849574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31016 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0035:03:00.0, compute capability: 7.0\n",
      "2022-12-05 16:06:41.851976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 31016 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0035:04:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "import os, sys, pathlib, h5py, time\n",
    "\n",
    "from models import make_model\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from astropy.nddata import block_reduce\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab36b2",
   "metadata": {},
   "source": [
    "# Load and perform some cuts on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d05c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/project/r/rbond/jorlo/datasets/act_freq_stamps/'\n",
    "\n",
    "with np.load(data_dir + 'all_clusters.npz') as data:\n",
    "    stamps = data['arr_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a8844ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4195, 41, 41, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a44df28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuts out any maps that have nans in them\n",
    "flags = []\n",
    "for i in range(stamps.shape[0]):\n",
    "        if np.any(np.isnan(stamps[i,...])):\n",
    "                flags.append(i)\n",
    "\n",
    "stamps = np.delete(stamps, flags, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a19f59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ffc4c4b92b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAD6CAYAAADa6jBfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzU0lEQVR4nO2de3Bc93Xfv2ffizdAPAgSJMGnJVKWKJuRNFWaSLZiKY4b2Zm6I7dN3Yln6OnYHWeStpaTzsRJRjNux49M68QtXauW29iKOo5ixZFj06pT2ZUsmZL1oiiSEAiSeBBPLrAL7HtP/9iLBotzLnEXWC6xF+czs7PYs/d3f4978dvfPed3ziFmhmEYhl8J3OgGGIZhXE9skjMMw9fYJGcYhq+xSc4wDF9jk5xhGL7GJjnDMHzNhiY5InqAiM4S0RARPVyrRhmGYdQKWu8+OSIKAjgH4FcAjAL4GYCPMPObbmWCTc0c7uiqPE9T0XOdgURQnnN2UW9fSB6b64oJWSmu9z+QJSELLyrHFqWsGNd/O4pRKeOQck5ZtTsFeTCVqiiv1OVWngreymv9BACKeGsY5/TxC+SVY5VDOapf01BY3mulkuxAKS/vHVLqBvSxCijjFMzqfaecdrCsv9AkZQBQWiXOL8yhkF6s5g4S3H9vM8/Oefu/fOm17PeZ+YGN1He9CW2g7B0Ahph5GACI6HEADwJwneTCHV0Y/NjvVMgC755Xj9Xm3uan2oSs87Hn1fLBzm1CNvbP3iFkC0f0u7fpQljI+p/PCFkoJctfvblFPWfikJTlupWbKewyGSgTWighL2E46XKPK+KScgeE9d8NRK8qE3pYnjS5T29/eI88sTbJFC81q+XjU/LYQpM8Lrs3q5bv7ZX32mI2ImVjrUIWm9QnmZAyVrE5OU4d59Nq+fClGSErbZP3+dw729Xyma7KMRn65hfV46phZq6IF74/4OnYcP/b3Ruu8DqzkUluJ4DLKz6PArhzY80xDOPGwyhyNY8Dm5uNTHLackH8hBHRcQDHASDU1rmB6gzDqAcMoCT/lRuWjUxyowB2rfg8AGB89UHMfALACQCI7djln5EzDB9Tgq3kgLKh4SAR7QUwBuAhAP/0WgWoBARzlbKlS1L/AQClqBzkqKJ/oduPqOUzPXEhW9oh59hou9SzAUCuXQ5Nukfqb2IBzUCh3yDxKanXyXfK8v2Dc2r5TF62aSEhdY8RXc2JrDwU2YGclBV1nV5+Quopg8rwkaI7BIDMnDT8QDG8UEwfP035runESmPyOgHAbETqSqMxqVMNLkprRtsFvU3ReSnPtcryVw/J+xEA+PAuIcsq90Su3cWYklolqMGmMAYjb4+rADMXiOiTAL4PIAjgUWY+XbOWGYZxQ2AARXtcLcPMTwN4ukZtMQxjk2A6OcMwfAsDKPoozqRNcoZhCPyjkbNJzjCMVTDYdHLrpRQG0r2VgxeddTMHSXlqtxz4hX1ydzgAkHaNFFlpWPdOCCiOCIkDsk1NLdKS13ZRWiwBIJqQLjxM0gdqPKyYQQFQVDaKwrJTiy6b1QutSvm0tFiGr3o30aW3e//Nj1+W1tmQ4giQ1x0eVMthaEnKmif0f9DcZWnhLDRLk337vCwfXnI5p2JJXeyXssw2vXy+Q7kmLcp9ktE9LgL5yjHV3NyqhRnI+2eOs5WcYRirIRSrcqDe3NgkZxhGBQygZCs5wzD8jK3kDMPwLeXNwDbJrQ+W8bfcXJA05XN+u1Tod/ck1fIz09JdrOmcVPK71V9Q7BFqWJ8OeTOkF6WCHdDdveKzUsZn9fKZbnm5tDHZ1rugll9Swgrl35SGm7ZhtTiyXYpsu2KhcVF+h5bkF11npVsVK65yAJDuksp3LVQUB/Xy0QXFrU8xMpRCsvz8oK74z/Ro8QClLOQS/iqguOqpgaJcxrQUrP1zJQPI18KCsUmwlZxhGBUwCEUfZUawSc4wDEGJ/fO46p/p2jCMmrCsk/PyWgsiepSIpojojRWyzxLRGBG94rzev+K7zzg5Y84S0f216I+t5AzDWAWhWDud3NcBfBnAN1bJv8TMn6+olegwyiHbjgDYAeCHRHSImb0nglGo6yQXKALRucrZP5R220mu/Eoou/Nnh/Vow7FZJRmJUlVeD2enJk2JJKRMM0bMHdF/4YIZLR+DPE5NGOMmV2K/aQYGACgW5Y1baJaDku10ucGV8QvNyz4VXRLJaOMfSciBDl+cVss3dcs8B7O3SdnVm9Xial/D87KvoSU5ptltumdHqVcafmhGjn/HqN6muJIwZmGXEsuwTy+/2uOjqiRGLpQjA9dmkmPmZ4lo0OPhDwJ4nJmzAC4Q0RDKuWT0RC4escdVwzAqYCbkOOjptQE+SUSvOY+zyysVLW/Mzo1UAtgkZxiGQgnk6QWgm4hOrXgd93D6rwDYD+AogAkAX3DknvLGVIvp5AzDqKBsePC8/plh5mNVnZ95cvlvIvoqgO86Hz3ljakWW8kZhrGKsuHBy2tdZyfqX/HxQwCWLa9PAXiIiKJO7piDAF7cUFewwZUcEY0ASAIoAihUO6MbhrH5qKXhgYi+BeAelB9rRwH8AYB7iOioU9UIgI8DADOfJqInUE5QXwDwiY1aVoHaPK7ey8wyDbgLq3WVbj8GEcXqGD4nmxvM6I/sRSU50tJ2eaxb/bFZqR7QLFfpPsUtS4kHBgChGc1dS9aTb3PpU0TK4xekJS94WreuZnbKtkaUrPaLPfptERqR2baiyjhpblHlL6Qo36rEmItL9zs3ikoCsMIO1TEKA9uvCtnYVIeQBc/Kmyc2rd8o6aAca1YykGW79PJRxd2raVr+Xwdz3iYdN8t8tRRrtBmYmT+iiL92jeMfAfBITSp3MJ2cYRgVMAh59s/UsNE1KQP4ARG95NGqYhjGJmfZ8ODl1QhsdLq+m5nHiagXwEkieouZn115gDP5HQeAUJu+cdcwjM0Dg2r2uLoZ2NBUzMzjzvsUgCdR3p28+pgTzHyMmY+FmtyC9xuGsZkoIeDp1QiseyVHRM0AAsycdP5+H4A/ulYZDgL51kqlbGxWP7b5itRSFxV9enK3PtBLg4pflvLjFBvVY7cFFd314oBsU3SHVNxn5hRtOIDYtNIARaQlN3EjflaeoPWSrn0uhWVfw4ekW1JPW0otP3q1V5ZPKu5zLq5FeSVGX+KAbNNSj+7DVFTsEUt9sv+c1m/ry2MyQVD4iqy/9aI0HDRN62OanJN1zR2V5dN36mOa3CuNHO3n5JjGlLiDAJBrW9X/GizAmFFL39UbzkYeV/sAPElEy+f5JjP/bU1aZRjGDaNseNiQy9amYt2THDMPA7ithm0xDGOT0ChGBS/4x05sGEZNYJCvgmbaJGcYhsBWcuulBIQWK38hOOC2u1/KFncoA/9uPRPNL/RdEbKfndsrZM1j3oMcFONS+RsLSllAiXsHAKGMlBUUzwwq6L+i2k76dI/mcaAbU7S6UlMyoF4i7GI5KGlx1mSbik16ea39OcUTIJjR+19SulWKKF4cM/r4x2aVeH5KcptwWrmmOb1PWiIiysn2/8NBl+xAg1L0IzoihWdcEul0V37WEvtUSznvqk1yhmH4Fm+hzRsFm+QMw6ignJLQrKuGYfgUZrLHVcMw/I1tBjYMw7eU48mZTm5dUAkIpStlqd36sYUWafWijrSQ7WqVblUAMJtR/GQVq2W2y/vFDGTkr9vilKwn4GKcXNypxLNTVB+Ud2mTkpkrPSDdjdL7XdzClJhkYSXGXTCt119oku0vKNmqos1SBgD5rLzdSjlpRnftvzKuAcUSS8o4Abpb2WrrJAAklVVMbEaPcadlm9MyvU2k29TyB1plZrJYv7ynlxIuaeVWd7Umc1NNUxLecGwlZxhGBeUtJLaSMwzDp5jvqmEYvqdRwih5wSY5wzAqKIdassfVdVGKAqm9lUrxm45cVo99V6eUv7mwXchevTiglucFqdAOtEuFeOyXdLewTF5Rkk9IzXV0Si7r3Vxr8t3SSBBqkVpqzumPCrykyINS8e2m+C/F5Y1bXJTnDM/rN3hA8xbLyvLZrOI/BiCUkMcGlaa6PSlp7l5BaYtCtlt31YvelhCyW7qnhOz8bI+QzY90qOcMLyhjpVR/5ryeCH6kq0vIiBQDT6vep7ahyhVXoEaJbEwnZxiGbylHIbHHVcMwfErZrcsmOcMwfIu/VnJr9oSIHiWiKSJ6Y4Wsi4hOEtF5593ScBmGjyiBPL0aAS8rua8D+DKAb6yQPQzgGWb+HBE97Hz+9FonisZy2H/zeIXs1/peV489Eh0TsnRRar5/XtBdJgJKTK9tnTKZyG/tfU4tP1OQO8wfS98l6xlrErKIbstAKSyHO9Yjd7fv3K6fYHxB7ppPTsh25qZlmwCAFMMLd0nDR07xQgCAiGKQCC3KPrkpv0lxxMh1SIV6vkN3GSmlvMee02iJyexEAUXJn1qUiYhCLl4g2v+51qbo23qMv/ykHL9cv7wm5FJ9OFXZfm2Mq8Vv1tU1V3JOHtW5VeIHATzm/P0YgA/WtlmGYdxIShzw9FqLap8EiegzRDRERGeJ6P5a9GW9D959zDwBAM67zFVnGEZDspzjwcvLA18H8MAq2fKT4EEAzzifQUSHATwE4IhT5s+IaMOuF9ddu0hEx4noFBGdys0rm5oMw9hUMIACBzy91jxXdU+CDwJ4nJmzzHwBwBCUhPXVst5JbpKI+gHAeZc7Kh2Y+QQzH2PmY5F2fZOoYRibi1o9rrrg9iS4E8BKL4BRR7Yh1ruF5CkAHwXwOef9O14KlZiQLVRW+a1Lv6Ae29t0k5C1hKXieHBgRi0/2ymV75qS+btTt6rl0wVdUbyavLITPZLUl/HN41K+oLSzv1/3AumKLgnZ89PSCyOseGEAQC6ohFVqU4wRSsIZAIhelbKAoujWkusAQLZXnrekJL3hmK49LyhJazIkb+FIQq8/8az0mJkLSVlYqZ5cwmdphgfN8BJUkhgBACmPfKWovE6liH5NkoOV5Yt6RKjq8P4oCgDdRHRqxecTzHxinTVrlXrPNOXCmpMcEX0LwD0od2YUwB+gPLk9QUQfA3AJwIc32hDDMDYHVQbNnGHmY1VWMUlE/cw8sepJcBTArhXHDQAYF6WrZM1Jjpk/4vLVezdauWEYm5Pr7Lvq9iT4FIBvEtEXAewAcBDAixutzDweDMOooJZBM6t5EmTm00T0BIA3ARQAfIKZN7zzzyY5wzAqYBAKpdpsvKj2SZCZHwHwSE0qd7BJzjAMQaO4bHmhrpNcPhvC5eHKWF3Rad0SONEi9xdHd0u3rN1diskPQKviwjM+IrOWJC9638dc2CPNZsFd0my2VJJuQQDQdkHKmoekC9VzrXvV8l1tStIeJTmP5tIGAJSVv85FJR5cxMWFSTFOIzUgj40fXb0tqkxrWI7f9Dl5TWJXdLeyTI98cik2S7NnbEhfhfS8nBSyXIc0R06/S8rSfbp5VU26o4i0dgJQk/OEFpX2uxTP7a/ce8pRNzNwFbDFkzMMw8dYIhvDMHyPTXKGYfgWBqFYI8PDZsAmOcMwBGZ4WC8lQnCp8heiqOuYVdeizBWZrf78JSUtOoBgVl6kljlFST6te43km+SxS0rSmD19s0I2nJOJUAAAb8vOdp6TyvREQe/TTK/sf1xJpBKUNhcAQKFZHltIK4Yfl/s7peQMir1TGn5+bc9ptfzTlw4LWdt5uWJontS3RiXysq35FnlNwkv6NS3GpbvUwh55TVL7pIEk3KH7ZbGy4unulAaOw52TavmRlExkM3xOcTVb0A10+eSqPhU3PjmxGR4Mw/A7bJOcYRj+pSoH/U2PTXKGYQhsJWcYhm9hBoolm+TWR5hR6qmMX7Z7u747fqAlIWTPDe0Tss5n9QBa8Vm583t+n1TeTt+pK7lDStKXrlYZz20pr8SdU7wIAD32WmhRCuMzuvmeg0oiGdkkhNIuIbiUbCjpgKwr1+0yJkrsuc4mGe1ZMzAAwOJrUsnemZDXKdes9z8g87sgNiv7lG13iSf3K9ITJb9PGhQiSkC5XEq3kIXi0kixp00aYw4164aHbEn+C460bRMymtXvqY43KstPuyXcqRKzrhqG4VsY9rhqGIavMcODYRg+hzccdHzzYJOcYRgCPz2urumg5pIc9rNENEZErziv91/fZhqGUS/K1tWAp1cj4GUl93UAXwbwjVXyLzHz56urjkGrgpJFg0pqIwDvarskZAt7pHXs/A5pcQUAKsoLkNojrWa/cccpIQOAffFpIXtq4jYhO3d+h5DFR/VhLSoZl5K7pXV2sV//Fc11SUtkVLHExmdcsm3Ny/LFqLTaFXbq+XHjcWldvTgk4/G1DOv9b1IswfP7ZV/z7XpMtJASTi+ckuUXd+rluw/LzG7tUWldPTfcL2RNF/TsbcWYlL+Qkffkq616Zr2i4oZV0qzzLvNJKFN5rbWYf+thSz2uMvOzRDRYh7YYhrFJ2FKPq9fgk0T0mvM421mzFhmGcUNhEJi9vRqB9U5yXwGwH8BRABMAvuB2IBEdJ6JTRHSquKA8bxiGselgj69GYF2THDNPMnORmUsAvgrgjmsce4KZjzHzsWCbDBVkGMYmgwEukadXI7CuLSTL2a+djx8C8Ma1jv//FALAbKUb1oXLu9RD/2xQJjg5sF0aA2Lv1t3CZnbLmGzRNhloLVvSFcpT+TYhm0i2CllEScQT1EOPYWm7vClynVJJXuqQCn4ACITksfmMnjRHI3pViZOmxCnL5HQXomQuLmRNl+UtFJ3Xf+O1pDfZ3UrwOxe3uOCS0tZuJZ7cHv2JoSMmDSqzS/KHl3Lytz+g28cQG1Nk09IFLNfmEjixWWm/lsdGHxIkbqr8XPi+fly1NMqjqBfWnORcksPeQ0RHUV6xjgD4+PVromEY9WarWVe15LBfuw5tMQxjE2C+q4Zh+BsGYJOcYRh+Zks9rtYUYpRilcrz1rddvAOuNgnZ2ZvkTvSe7fNqedXIkJSx504Ov0Mtr2Wrz6Sl8pgVu0WuU79Dcj1Sex1sUYKkpXRjCM3I+kNK/LDMNv1XuBSS5bUYd8Ereoy+YlwxfLTLvuY61OIotMv+B+ZlX5vGdKM/KW1N7ZfCzibd8rOUVwwCBanRD3TKe2cpqxt4qODNSFFysTtohJKKF0ibfk81HUpU1h3VYwFWR20tp0Q0AiAJoAigwMzHiKgLwF8AGERZr/9PmFkG4qsBjeF8ZhhGfan9Rrl7mfkoMx9zPj8M4BlmPgjgGefzdcEmOcMwKmHUw+PhQQCPOX8/BuCDG222GzbJGYYh8b6S6172aHJex13O9gMiemnF933Le22ddxnpoUaY4cEwDAXPq7SZFY+gbtzNzONE1AvgJBG9tbG2VYet5AzDkJQ8vjzAzOPO+xSAJ1F2A50kon6g7EEFYKqm7V9BXVdy0VgeBw5OVMgmeqX7FACklxRzVFo2d+6MzGwEANFZJc6a0tvsId0a1deUErKAEqxrYkaxuqVdrIMxWVcwKO8USrjEYxtXrG6KO/DCfv3uC2Zl+Zj0lEN0Tv8VTytuaYV+aYlkxeIIAKEZxZJ6RZ4z4uIWlm+Wx1JOcRXL6+PXqbh1QXqqIa1Y0d1sltku2dZ8q5Rxm2JFB0BLsq0BZfxKYX1MgoHKa706XuO6qOE+OSJqBhBg5qTz9/sA/BGApwB8FMDnnPfv1KRCBXtcNQxDUMN9cn0AnqRyOswQgG8y898S0c8APEFEHwNwCcCHa1bjKmySMwxDUrMIwzwMQITUZuZZAO+tTS3XxiY5wzAk5tZlGIafqVWuiM1AXSe5SKCIwZbK+G//5cC31GOblB+Sfzv6ASF76QeH1fLtw1L5ntopFbrNnXrssX/U+6qQPTd/QMiuFOX2nti0/iu4pLhV0YBUSBd69Hhy6YLiVqbEGeNOXcmdz8v+B9PSGBDS89ggkFcMD0rst+C8flvFZpWkLcqhbol8SLGnROZlnxYUl0AA2N6aFLJ0XnGhuyjLtypGHwBI98nZINCtxMhzmTVKSTkA+RZ5bElxiQOAxNVKy1NRcVOrGiagQQJiesFWcoZhSGwlZxiGr7FJzjAMX+OjSW5Njwci2kVEPyKiM0R0mog+5ci7iOgkEZ133i0toWH4geXNwF5eDYCXlVwBwO8y88tE1ArgJSI6CeBfohwq5XNE9DDKoVI+fa0TLeYjeHFid4XsSo+uJL47JuffX+o8J2Q/PTSolp8jeV4Oyp+npQW9/pNz0qBRKCk70WNSGx5y8XiIT0k57ZV76e+7Rc8L9ELPHiGbf116fMTP6vHgclrsNyVbfSms37yhRSkP5Lw/DGjeGcWo4h3gsrs/vCDHL7wgjyuN68HbhsI98lglaU+L4vERyOttKjRJeXOzjGeXmldcKwCEk7JP2jkjLboxKpda1dcarcD8ZF1dcyXHzBPM/LLzdxLAGQA7UcdQKYZh1BkfJV6tSidHRIMAbgfwAlaFSnEiDBiG4QP8tJLzPMkRUQuAbwP4bWZecHzRvJQ7DuA4AIR72tfTRsMw6k2D6Nu84CnUEhGFUZ7g/pyZ/9IRewqVwswnmPkYMx8Ltun6L8MwNhFeH1UbZLXnxbpKKOdZPcPMX1zx1XKoFOA6h0oxDKPO+GiS8/K4ejeA3wTwOhG94sh+D+U4UFWFSinlA1gYb62Q/euAlrsa6GmW7lYFlnPy/r4ZtXy2W3bt4uVuIWt+TV9dvnr2JiEL3Cozg62OjwcAQyyzigFAfFy2qZSX1r3bWy6q5a9kWoXswoi0rva8LN2XAGDq3S1CdvXd0rqbi+uPKrErsv2s3EH5bt2tDDl5/aIzsv+0pP/2ckhxd4ooMfKm9PbnctLCmemT7lKp/Ur7lboBYPsOmWDqYIcM0vc67VDLL45LS3goJdvv5q4Vb6+05FLIYyTLNdBc6BqVNSc5Zv4J3GMh1yVUimEYdaZBVmleMI8HwzAqIN6i1lXDMLYQPrKu2iRnGIbEVnLrJMgIdlS6pyykdHeXq9NSyU4ZqXwNdOjuLgM9UiGsucYEcroLUHxSXuWZQZm05oM3Pytkl7ouq+f8X6+/S8h4Udb/11MiWjQA4PKCt32Gbj/CqjI5oNzNiqsVAORb5QmKLVIWbNJjn3FKjl84qSSi6XSJvbZHuktlU/IWbjmvxIgDEJTFEdkmhffvPyPPGVRixLnQGZZGs5wWOA/Ai23ymsZH5X2ev6rfp+37K/3agtr1XAf2uGoYhn/hLWZdNQxjC2IrOcMwfI1NcoZh+BnTya2T1mgW9+4/XyE71HxFPfbc4nYhe+aM9EIIjigZ7AFcnJdBUZq3S4Xw4rv0rC2L03InOinx6M4syp3s++JKWnoAu7fPCdnFMemx8MaYi8dEXBpOUnfK3fkLe6VnAwB1SzelFY+Dgm65CCjywJxSflo3JmnlF3dLj4tbbtU9Pu7ZJuMJ/vXEO4VsNKF7F2jxBG/qk9fqztZhIRvK9KnnfG5mn5DlSnJMikosQgBgxUhTjCsJh1y8QBKpSo+dYtGTO/qWwlZyhmFIbCVnGIZv8Zl11da2hmFIahiFhIgeIKKzRDTkpEqoKzbJGYZRAeHv/VfXeq15LqIggD8F8KsADgP4CBHpGeGvEzbJGYYhqd1K7g4AQ8w8zMw5AI+jnB+mbtRVJxcNFLA3Xhn/7R+3vqoeO950Xsgm0m1CdjqzSy1PRWnJI+Wn57bdo2r5zE7pGnQ50SFkT585opbXCIWlJbGpTboVNcd0V7Xe5pSQdW2X7Z/bp6TFAnB+Umar4lEZTy+mZBUDgNiMHL9oUsryLvHo5m6Rx/7yL7wpZP++/3tq+StF2dbvB+SiQHM1AwCOyvGPBaV1+mJWxh18dvqAes6Lr0pLbiQhxy/Trbcp0CXdxbIDsk2U0uPJlUYrrzXna7BuqW0Ukp0AVvo5jgK4s2Zn94AZHgzDkHg3PHQT0akVn08w84kVn7VfvLrabm2SMwxDUMVKboaZj13j+1EAKx+3BgCMr7NZ68J0coZhSGqnk/sZgINEtJeIIgAeQjk/TN3wkshmFxH9iIjOENFpIvqUI/8sEY0R0SvO6/3Xv7mGYVx3apiti5kLAD4J4PsoJ6Z/gplPX49mu+HlcbUA4HeZ+WUiagXwEhGddL77EjN/3mtlqUIUP5ndXyE7v6TnpE4XpeJfUxLfdrPuAjSWlHG6Zueku9PpnO5CdaRfJqjZ2yXdst4Y3SNk7Wd0JXFWenCh+S7pVnTfjrNq+UvpLiF7dXKnkKXTeuyxkhJorhSTd2rAJQ9N86RU3IfSUpY+pNdPO6UL3cEmmcnylazulvVcUir/RxNKjD3ZpDIF+Zt+ZVEas2abpOFGGzsACC5JeeuIHNNwSl9PJNulvG97Qsim52R8RQCInKt0oSM9lF/V1NJ3lZmfBvB07c5YHV4S2UwAmHD+ThLRGZQtJoZh+BUfuXVVpZMjokEAtwN4wRF9koheI6JHiaiz1o0zDOPGQCVvr0bA8yRHRC0Avg3gt5l5AcBXAOwHcBTlld4XXModJ6JTRHQqN69H/DAMYxNRQ53cZsDTJEdEYZQnuD9n5r8EAGaeZOYiM5cAfBXlnc0CZj7BzMeY+VikXQ/BYxjG5oGqeDUCa+rkiIgAfA3AGWb+4gp5v6OvA4APAXhjrXNlsmGcGa5UKp9L6E3QFOKdu2Vymr0d0hgAAEtZabgIKdnKCxFdSZ7pleV/uVt6YVzcJZ/SCxf1J/fQkpQtZWX9edYNF2/OyJhmhRdlXU3SMQIAkNwrny8CvXLHfWqvPiYclNeKilKW3u6SiEbxQvmLYZncpymqe3wEFW04Kf9pHHd5jlLi2c2nZTzCdEn2/52d+tau8UPS8JFakgauaEIfk9AVWVeqTd6n7W3KzQNgvqPyWJdbp3oaZJXmBS/W1bsB/CaA14noFUf2eyg72h5FeThGAHz8OrTPMIwbwJaKDMzMP4G+Mr1hJmHDMK4zW2mSMwxji+GzoJk2yRmGIbGVnGEYfmZL6eRqCpPIDsVum1iapX9KMiW3oLx8ab+QAUBsQpqZFOMgMCgzeAHAL257W8huj48I2cQuaV176tZb1XPSrGK1XJKy/zOhxy67ekW6IHXMy+OCWZc7VBH3dC0I2baduiVvZr90d7qalDHeCrN6BrWIklltKSJl2b26efgWxdWuKy7bOhrSfZvSiiVb47VZ6VY22KZb8X9pUN4n/ze4V8jyL8prBwDtZ+VFWSgqx94srxMAhAcq71+K1Og50yY5wzD8jK3kDMPwL4xqgmZuemySMwyjguVENn7BJjnDMCQ2ya0PCpUQ6a1UFO/rmVWPzZek4WDovIz91vq2ix+Lsn05eZMMlHb37ktq8R0R6UI2V1TcdQJSyd3brSuJpyAVygGlnYmk7uNLWWmlySthxjLbdK9C2i4DJOxqTQhZd1Q3xrSEpQtYsSTbNDultz+Yle0qKElndnYk1fKdEdn+SynFrS2iB8Tb3irPO7MkjSljl2Xgv/Gw7qp318FhIbtnz5CQfW9KN0a1jMrxa70gx2l2m56cqKWv0kijJWtaD8T+meVsJWcYRiUNFGHECzbJGYYhMJ2cYRi+xty6DMPwN7aSWx/RUEEYGu7sGlGP/asRqaiNzEojQ7pXvxrhA1LJvKNF7o7/+YSeruL1KWnkuLlnUj12NZoXAADwkhzuYGdGyJrjUsEPAFeD0juAlKQtHNLHJB6XcdpySjy458cH1fKJaWl4gZLghdp1xX9+m+xXc5OULeVlLD8A+PHFfUKWTcgxCTTpHg/xftmu5ogck2ll+AIzeptebZXeEffuloaHwYP6vTO2IMvHpuWYhhK6gS0VrDRIlJRkPVXD9rhqGIbfsUnOMAy/YpuBDcPwPVTyzyy35gM8EcWI6EUiepWIThPRHzryLiI6SUTnnXdLSWgYfmALZuvKAngPM9+GcvrBB4joLgAPA3iGmQ8CeMb5bBiGD/BT3lUvOR4YwLLvSNh5MYAHAdzjyB8D8HcAPn2tc5VASBcqrVQvJXarxyZmpCWP2uWoHr5Fd8t6T/dZIfvv5+8SstCzMh4cALAyMq/cKYWaW1qpqP92kJItqliQVjPNVaoagmndrSs1I12Dzir1Z2Z0t6zYFdn/fJu8JjtumVHL37v9nJCdT/UK2U+HZDw2AAhOyCxWIaWrhZLe/9G5DiHb1ipd2PoGpEvfTEKxLANIJ6V19/kre4TsYJc+Jrmj0uo6PtItZMGkbl2l9Kp7xaXvVdMgqzQveM27GnQydU0BOMnMLwDoW05J6LzLu9UwjIaE2NtrQ3UQfZaIxojoFef1/hXffYaIhojoLBHdv5F6PBkemLkI4CgRdQB4kohu8VoBER0HcBwAor2KN7lhGJsLBlA/B/0vMfPnVwqI6DCAhwAcAbADwA+J6JAzD1VNVc9FzJxA+bH0AQCTRNTvNKof5VWeVuYEMx9j5mPhDn2TrGEYm4sbrJN7EMDjzJxl5gsAhgDcsd6TebGu9jgrOBBRHMB9AN4C8BSAjzqHfRTAd9bbCMMwNg/L++Q8Pq52E9GpFa/jVVb3SSJ6jYgeXbFDYyeAyyuOGXVk68LL42o/gMeIKIjypPgEM3+XiJ4H8AQRfQzAJQAfXutEhWIA08lKBW4iqCu5+/oTQnZb95iQ3dv+llr+JwuHhGzpgozn1rWgL8vTvVKBW1KUuiXFrWlbh56IRVM9F5PSXSiZ0JOeICjbmjogXZUop/92BecVI0NRjn+oXbo6AUCuVdbFypiEg/pTRWdIKvl3xhOyfEx3y8p1yNs1orjFtUT08osL0kgwpcQIPNwvjQE7WpSMQQBeH5X/e3PT8voNB/T7rCmsuMCFlSUS6deUW1aNtXKPVA1zNY+rM8x8zO1LIvohgO3KV78P4CsA/hjlB+Q/BvAFAL8FPZn9ujvmxbr6GoDbFfksgPeut2LDMDYvtfJ4YOb7PNVH9FUA33U+jgLYteLrAQDj621DDbx5DcPwHXXYDLys03f4EIA3nL+fAvAQEUWJaC+AgwBeXG895tZlGIagTr6r/5GIjqI8XY4A+DgAMPNpInoCwJsACgA+sV7LKmCTnGEYq2EAxes/yzHzb17ju0cAPFKLeuo6yQUCjJZYZfyw9qhUHAPAe3qlx8K9LW8K2VtZGfcNAJ6bGJT1Kx4Hs0f1i9m6JyFke1qlQWE+K5XZmjECAFqbZV8TiuEhnNC1CLluqVA/dEBmlS+wXn5kQiZoiSlK/l8/8LpafkBJ7vPUFRn3b2JeN5w8F9svZDe3XhGy2wdG1fLT26SRYGdzQsgKShIkAPh5YUDIMko8uvNh6XHQo3hGAECTEg8vE5SGg6VsRC0/PSk9bpqG5LFKviQAgMjjU6O5yaKQGIbhbyxbl2EYfsZWcoZh+JcGCqPkBZvkDMOogABQHQwP9aK+iWyCBQy2z1U2wMUBbjgtlb8zeem+tliU4XcAoC0mFcKFQwkh29GmZ7t/V+dlIZvOyQADz48NyjbNS2U2oO/kj3TIdubcouUo3gWXr3YIWV+7noH+5l1Syf+OVrm7/97WM2r5DEsjSXNIekfk87rifywlley9UWnMiQf1RDhaCKqxxQ4h64nrHid7ts0J2XBB3mdLkzIk1cVpPYN9c680SBzsmxayBcVABQDpEXlPNU/ICSaQ1yedQlOlkYJcb57qINPJGYbhW+xx1TAMf1OV7+qmxyY5wzAEZl01DMPf2ErOMAzfwmZdXTe5YhDjqyxseZekLbPz0prFyrG9Xbp19FCHtHC9o0NaN9NFaTEEgImstAQm89KSS9q6vopkIgPbEkKWbdcvy5iS4KT0prTOjXTrEZgPvEO6gGmWzJPzR9TyFxalW9hcRtbV1aa7QIUD0pJ+dkGmBplb0mMMXr2qJJNRxjrfr1t37+wZEbK2iHS1+9miTKQTmtHvk8WotJommmT7F13cujgs75/FHfI+D7kkJ4qtClLo5v5VNf6Z42wlZxiGxLaQGIbhb2ySMwzDtzCABkkc7QUviWxiRPQiEb1KRKeJ6A8duWvORMMwGhcCg9jbqxHwspLLAngPM6eIKAzgJ0T0Pec7kTPxWuSzIVy+0FMhoyZdUxoMyZ8SUhTX8y5K6rdJKum7YktCVlJzZgDJnDQyaG5FvUqMOTdXsXhIKvljimw6rWdrp7gcq2JUKtkDLolsRial4WAyKesqFHTFfTQs69/VkRCylpB0VQOAybQ0kmhGhnxRrz+kJKgpKce6xfOLKlr5Qy0yk+alHauDtAGTLGUAgIysf/ztHiELZFyMUVE5UeRukfepPqJAcLhy/Eq6faR6Sv5ZynlJZMMAlv+Tw86rMaZwwzCqZ6s9rgIAEQWJ6BWUE0ifZOYXnK+0nImGYTQ4fnpc9TTJMXORmY+inBrsDiK6BeWcifsBHAUwgXLORAERHV9OPFtM6funDMPYZCznXl3r1QBUlZKQmRMA/g7AA8w86Ux+JQBfBSDjIJXLnGDmY8x8LNiih6sxDGMz4XGCa5BJbk2dHBH1AMgzc4KI4gDuA/AfiKifmZe30K/MmXiNk0F4/gZcMn53d8qYaFrSm9klfeIcm+4QsnGSss52fXXZ1yLrjyiK6ybFcNAf07Oth0lmVTuXkjv+p1N6n+ItUv3cc7uMkaYZMwA99lzyijQGuBmDDu+V8ej+QeewkF3KdqnlNcNDR1xe08EW2ScACAXk+A0npYFpPqPHbnthdlDI+uLyOq+OeQgAkaCeEW98pkMKJ6TRKpzS1xN5RfkVDMm6ul28SGbDq46N1UCZVqdsXfXCi3W1H8BjRBREeeX3BDN/l4j+h5Yz0TCMxqdR9G1e8GJdfQ3A7YrcNWeiYRgNzlaa5AzD2GIwgJJNcoZh+JbGMSp4oSrrqmEYW4Q6WFeJ6MOOq2iJiI6t+u4zRDRERGeJ6P4V8ncT0evOd/+JiNaMa1bfbF3RPA4dqIxpNr7Qph47tyAtjNFOaXXa3zkjZACQU1yTkpdkXTMpfQh6Dkl3rZuUzFbDS9K69+Mr+9VzxkLSaqnFo2MXt6RAQB57sF3Gzbun4y21/OnOnUJ2MnqTkBVd4uFtj0lLZDSgW3I1NLe2npgc53e2jqrll5TMbOfmFev0pIwFCADTxQ4hm+mV99mRHiWrWYd0/wL0Po3FZf2LV3X3w8C8vP+Kw9LVbqxDL786W5ga37BaGECxLi4PbwD4DQD/daWQiA4DeAjAEQA7APyQiA4xcxHl/bnHAfwUwNMAHgDwPVwDW8kZhrEKBrjk7bWRWpjPMPNZ5asHATzOzFlmvgBgCGUnhH4Abcz8vONu+g0AH1yrHtPJGYYhubE6uZ0or9SWGXVkeefv1fJrYpOcYRiVVGdd7SaiUys+n2DmE8sfiOiHALYr5X6fmb/jck5NX8LXkF8Tm+QMw5B4X8nNMPMxty+Z+b511D4KYNeKzwMAxh35gCK/JnWd5CKBInY0V7o8DU3I2FsAgCmpZL6iKFVv7RpTi6c6ZPm33pYK4dBVPQDX5d4OITvQKpX8I/PShWn6nDRGAEApInUYnQPSBSyiGCgA4OqcVEg/VxoUsr6oHs/ufW3S864zLN2F/mb8nWr5Z0YOCdmPw/uETIuxBwD9TbKv3RF57HBavyfOzMsFwaUrcvwDCf225pC8f7J5eWymKGVawh8A2BaT4xdWXMDGQ7qBLbHYIWSxK1JVXlIMFACQzlS6ypVyeiy+qrmxj6tPAfgmEX0RZcPDQQAvMnORiJJEdBeAFwD8CwD/ea2T2UrOMIxKmIGi7qtbS4joQyhPUj0A/oaIXmHm+5n5NBE9AeBNAAUAn3AsqwDwrwB8HUAcZavqNS2rgE1yhmFo1GElx8xPAnjS5btHADyiyE8BuKWaemySMwxD4iOPB5vkDMNYBZvv6npJZqP48XClNwDPuWQWb5E6gbZmGXvsnJKBHQBGZpSYZspO/mJcv5jFtGzXyzO7hGx+Ue5EL0VdNkkqsfPSSmb1YtHFU2VBGknS81L2TJM0EADA7uiskMVIGjkm52XcNwAIvCzl2sb4tw/ru/N3v+OqPKdiTHK7phen5DVVFe2dupEgHJfytiZ5T2WKckwnM/q/yly6SciuKvdEOiUNYQDU7fiZbjkmymUCAEQSlSdQQhZWDwO8wY2+mwlbyRmGIamPW1ddsEnOMIxKmLdWSkLDMLYgPjI8eHbQd9IS/pyIvut87iKik0R03nm3lISG4RO4VPL0agSqiULyKQBnVnx+GMAzzHwQwDPOZ8MwGp4tlq0LAIhoAMCvobw573cc8YMA7nH+fgzlVIWfvuaJcgHQpUrLE7fr5qCdu6UlsCUis1WdH9MtcZiW1qxShzRRdfTKGGkAEFYyJi1k5Dk7WpaErKdXun8BQK4oLYGX5uQCOD+vZ5sixTrMMdlOzVUJAF5J7RayVEFadzMLuiVQS2ymhb5jl3h0miVVy2DmlhmruUle/2I8J4+LShkAtESkPKxkAFvMyzGZT+vXJLUo5XnNEpt3WU8orn6FJu8m0uJCZV1cC6+uLRr+/E8A/DsAK/cQ9C2nJGTmCSJymW0Mw2gkGADXwa2rXqz5uEpEHwAwxcwvracCIjpORKeI6FRpUc8daRjGJoLrEzSzXnhZyd0N4NeJ6P0AYgDaiOh/AphcTjDtROxU40M7saVOAEB0YJd/1sCG4WPYR4+ra67kmPkzzDzAzIMox13/38z8z1EOh/JR57CPAnALgGcYRqPho5UccRUWEiK6B8C/YeYPENE2AE8A2A3gEoAPM/PcGuWnAVx0PnYD0LPQNC7Wp8bAz33aw8wuQRq9QUR/65zPCzPM/MBG6rveVDXJ1bRiolPXiijaiFifGgPr09bCsnUZhuFrbJIzDMPX3MhJ7sTahzQc1qfGwPq0hbhhOjnDMIx6YI+rhmH4mrpPckT0ABGdJaIhImpIp34iepSIpojojRWyho7KQkS7iOhHRHSGiE4T0accecP2i4hiRPQiEb3q9OkPHXnD9mkZiwrknbpOckQUBPCnAH4VwGEAHyGiw/VsQ434OoDVe4MaPSpLAcDvMvPNAO4C8Ann2jRyv7IA3sPMtwE4CuABJ2dnI/dpGYsK5JF6r+TuADDEzMPMnAPwOMrRTBoKZn4WwOqNzw+iHI0FzvsH69mmjcLME8z8svN3EuV/oJ1o4H5xmeXs1WHnxWjgPgEVUYH+2wpxQ/fpelLvSW4ngMsrPo86Mj9QEZUFQMNGZSGiQQC3o5ylvKH75TzWvYKyb/VJZm74PuHvowKt9Ktq9D5dN+o9yWmBxsy8u4kgohYA3wbw28y8cKPbs1GYucjMRwEMALiDiKpKTLzZ2GhUoK1IvSe5UQAr8/oNABivcxuuF5NONBZcKyrLZoaIwihPcH/OzH/piBu+XwDAzAmUA7s+gMbu03JUoBGU1T3vWRkVCGjIPl1X6j3J/QzAQSLaS0QRlKOaPFXnNlwvGjoqCxERgK8BOMPMX1zxVcP2i4h6iKjD+TsO4D4Ab6GB+2RRgaqn7puBnbh0fwIgCOBRZn6krg2oAUT0LZRDv3cDmATwBwD+ClVGZdlMENEvAvgxgNfx97qe30NZL9eQ/SKiW1FWwgdR/kF/gpn/aD0RdDYjG40KtFUwjwfDMHyNeTwYhuFrbJIzDMPX2CRnGIavsUnOMAxfY5OcYRi+xiY5wzB8jU1yhmH4GpvkDMPwNf8PnjVNuO8y9KQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(stamps[50,...,0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74cc6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps /= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5d5df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = stamps.shape[0]\n",
    "train_size = int(0.7 * tot)\n",
    "val_size = int(0.15 * tot)\n",
    "test_size = int(0.15 * tot)\n",
    "\n",
    "train_stamps = stamps[:train_size]\n",
    "val_stamps = stamps[train_size:train_size + val_size]\n",
    "test_stamps = stamps[train_size + val_size:]\n",
    "\n",
    "input_shape = train_stamps.shape[1:]\n",
    "\n",
    "act_catalog = fits.open('/gpfs/fs0/project/r/rbond/jorlo/cluster_catalogs/DR5_cluster-catalog_v1.0b2.fits')\n",
    "\n",
    "labels = act_catalog[1].data['M500Cal']\n",
    "\n",
    "train_labels = labels[:train_size]\n",
    "val_labels = labels[train_size:train_size + val_size]\n",
    "test_labels = labels[train_size + val_size:]\n",
    "\n",
    "#This just sets the # of samples we include in a trianing epoch, which is called the batch size. Autotune is a bit\n",
    "#of magic that allows tf to dynamically set some hyperparameters in an optimal way. See https://www.tensorflow.org/guide/data_performance\n",
    "batch_size = 500\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_stamps, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_stamps, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_stamps, test_labels))\n",
    "\n",
    "#We shuffle our data (i.e. just mix up the order) and batch it\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "#Preloads data into memory\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154a052",
   "metadata": {},
   "source": [
    "# A simple Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "016c57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "feb4850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 5043)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                322816    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 327,041\n",
      "Trainable params: 327,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27e72e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 42ms/step - loss: 2.1290 - val_loss: 1.2636\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.0758 - val_loss: 1.0853\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.8692 - val_loss: 1.1297\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.7232 - val_loss: 0.9943\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6428 - val_loss: 0.9955\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6350 - val_loss: 1.0176\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5879 - val_loss: 1.0797\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5684 - val_loss: 0.9812\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.5025 - val_loss: 1.0069\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.4503 - val_loss: 1.0037\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.4320 - val_loss: 1.0050\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.3684 - val_loss: 1.0094\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.3396 - val_loss: 1.0182\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.2973 - val_loss: 1.0129\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.2731 - val_loss: 0.9899\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2465 - val_loss: 0.9825\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2264 - val_loss: 0.9801\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2047 - val_loss: 0.9879\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.2065 - val_loss: 0.9902\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.2089 - val_loss: 1.0046\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.2151 - val_loss: 0.9948\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.2236 - val_loss: 1.0102\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2185 - val_loss: 0.9924\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.1959 - val_loss: 0.9941\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1982 - val_loss: 0.9877\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.1923 - val_loss: 1.0319\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2000 - val_loss: 1.0186\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2223 - val_loss: 1.0068\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2085 - val_loss: 0.9939\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.1930 - val_loss: 1.0348\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1699 - val_loss: 1.0160\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1560 - val_loss: 1.0025\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1571 - val_loss: 1.0047\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.1663 - val_loss: 1.0039\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1923 - val_loss: 1.0203\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.1774 - val_loss: 1.0524\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.1877 - val_loss: 0.9989\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.1741 - val_loss: 1.0271\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.1896 - val_loss: 1.0456\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.1770 - val_loss: 1.0407\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.1549 - val_loss: 1.0173\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.1762 - val_loss: 1.0225\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2069 - val_loss: 1.0100\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2330 - val_loss: 1.0111\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2628 - val_loss: 1.0041\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.2585 - val_loss: 1.0293\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.2421 - val_loss: 1.0515\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2103 - val_loss: 1.0665\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.2045 - val_loss: 1.0638\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.1850 - val_loss: 1.0123\n",
      "Distributed time:  9.545839071273804\n",
      "2/2 - 0s - loss: 1.0127 - 62ms/epoch - 31ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "history = model.fit(train_dataset, epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c83abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.322206]\n",
      "[-4.1848288]\n",
      "[0.62935257]\n",
      "[-1.5596745]\n",
      "[-0.9097085]\n",
      "[-2.0816574]\n",
      "[-7.2058144]\n",
      "[0.2578566]\n",
      "[1.8290396]\n",
      "[-5.819723]\n",
      "[0.09744024]\n",
      "[-2.5317466]\n",
      "[-0.79503274]\n",
      "[0.76748323]\n",
      "[1.2818494]\n",
      "[-0.5636425]\n",
      "[-0.73969793]\n",
      "[0.26605558]\n",
      "[0.42869115]\n",
      "[-0.4438734]\n",
      "[-0.04729819]\n",
      "[-0.05511546]\n",
      "[5.629614]\n",
      "[-0.17271757]\n",
      "[-2.7422109]\n",
      "[2.0656886]\n",
      "[-0.04578567]\n",
      "[0.01168871]\n",
      "[3.7524602]\n",
      "[0.6338148]\n",
      "[-2.6980171]\n",
      "[1.6342628]\n",
      "[-1.0446422]\n",
      "[-2.5664685]\n",
      "[0.4532194]\n",
      "[-0.01426482]\n",
      "[2.643563]\n",
      "[-0.21050858]\n",
      "[-0.5266087]\n",
      "[-0.42185497]\n",
      "[2.1604018]\n",
      "[-0.7545538]\n",
      "[0.51746583]\n",
      "[-0.36840558]\n",
      "[0.2689941]\n",
      "[-2.9526591]\n",
      "[-0.9209652]\n",
      "[-1.187269]\n",
      "[3.5167344]\n",
      "[-0.26876974]\n",
      "[1.5624397]\n",
      "[0.09751749]\n",
      "[-0.49260902]\n",
      "[0.46579313]\n",
      "[-0.01437378]\n",
      "[0.28501725]\n",
      "[-0.5516205]\n",
      "[-0.30986452]\n",
      "[-8.576283]\n",
      "[-1.455832]\n",
      "[-3.5819361]\n",
      "[2.2401173]\n",
      "[0.13022041]\n",
      "[1.4675786]\n",
      "[-1.3272288]\n",
      "[-1.6913655]\n",
      "[5.266282]\n",
      "[-0.20435]\n",
      "[0.16095781]\n",
      "[0.5388055]\n",
      "[-1.4406161]\n",
      "[-0.75655866]\n",
      "[0.21480632]\n",
      "[2.2076998]\n",
      "[-0.06651735]\n",
      "[-1.9713862]\n",
      "[-0.1962421]\n",
      "[-1.2290521]\n",
      "[0.70322514]\n",
      "[3.4200792]\n",
      "[0.06759953]\n",
      "[-0.27656698]\n",
      "[-1.7402384]\n",
      "[0.2596302]\n",
      "[-1.1752055]\n",
      "[-0.24361229]\n",
      "[0.6961615]\n",
      "[10.605163]\n",
      "[-6.5584393]\n",
      "[-1.4052272]\n",
      "[-1.3643816]\n",
      "[0.420619]\n",
      "[1.6426616]\n",
      "[3.2454078]\n",
      "[1.4370599]\n",
      "[-4.233574]\n",
      "[-0.71529436]\n",
      "[-0.02931595]\n",
      "[-1.5771744]\n",
      "[2.94842]\n",
      "[-0.66605353]\n",
      "[-3.5580924]\n",
      "[-0.2581072]\n",
      "[0.14728379]\n",
      "[-0.32323265]\n",
      "[1.4477115]\n",
      "[1.6789286]\n",
      "[-0.4734223]\n",
      "[7.24271]\n",
      "[-0.19345045]\n",
      "[-1.1586349]\n",
      "[-1.0322976]\n",
      "[-0.93070936]\n",
      "[1.7070947]\n",
      "[3.084279]\n",
      "[1.059155]\n",
      "[-1.1325967]\n",
      "[6.114567]\n",
      "[-1.3862116]\n",
      "[1.601038]\n",
      "[-0.6257081]\n",
      "[0.6257808]\n",
      "[0.1318202]\n",
      "[4.265142]\n",
      "[0.4410882]\n",
      "[0.27035642]\n",
      "[0.47124577]\n",
      "[-1.3045259]\n",
      "[2.2284803]\n",
      "[0.20542908]\n",
      "[0.02471924]\n",
      "[-0.556967]\n",
      "[0.55942225]\n",
      "[2.2519226]\n",
      "[0.7810283]\n",
      "[1.3671312]\n",
      "[-0.2933793]\n",
      "[-1.2240291]\n",
      "[-0.9585731]\n",
      "[1.2918575]\n",
      "[-0.56376195]\n",
      "[-5.520463]\n",
      "[0.44757771]\n",
      "[-6.9132175]\n",
      "[-0.602772]\n",
      "[-0.6408675]\n",
      "[0.35797262]\n",
      "[1.2100217]\n",
      "[0.58089685]\n",
      "[-1.1858001]\n",
      "[4.38157]\n",
      "[-0.8290615]\n",
      "[0.5461478]\n",
      "[-0.13316941]\n",
      "[-0.6019008]\n",
      "[-0.7211964]\n",
      "[1.8614497]\n",
      "[2.616649]\n",
      "[-3.1213717]\n",
      "[0.2973981]\n",
      "[-1.4388108]\n",
      "[0.22802114]\n",
      "[3.4568853]\n",
      "[-0.49526453]\n",
      "[-0.45937586]\n",
      "[-5.385838]\n",
      "[1.561239]\n",
      "[0.15006566]\n",
      "[0.32568145]\n",
      "[-1.4364853]\n",
      "[0.63975286]\n",
      "[0.08863306]\n",
      "[1.1052735]\n",
      "[-0.2769866]\n",
      "[-1.3966284]\n",
      "[-1.036278]\n",
      "[0.7770033]\n",
      "[1.3621764]\n",
      "[0.09839177]\n",
      "[2.3317513]\n",
      "[-0.66286993]\n",
      "[-0.1638372]\n",
      "[-1.1517689]\n",
      "[1.2510116]\n",
      "[-0.5989964]\n",
      "[-0.47156715]\n",
      "[-2.162031]\n",
      "[-0.44509697]\n",
      "[0.5514629]\n",
      "[-0.53401065]\n",
      "[-1.1101058]\n",
      "[0.80551934]\n",
      "[0.77594185]\n",
      "[-1.2232652]\n",
      "[0.5448792]\n",
      "[0.32727456]\n",
      "[-0.59274364]\n",
      "[-0.5570924]\n",
      "[-2.545181]\n",
      "[-0.27965617]\n",
      "[2.582076]\n",
      "[-0.87055326]\n",
      "[0.00618386]\n",
      "[0.11006141]\n",
      "[-0.7462914]\n",
      "[6.9292803]\n",
      "[-1.0526724]\n",
      "[1.7618341]\n",
      "[0.8910098]\n",
      "[0.23633766]\n",
      "[0.52602315]\n",
      "[-0.52538085]\n",
      "[2.7366672]\n",
      "[1.9551191]\n",
      "[2.8822315]\n",
      "[-8.202896]\n",
      "[0.4032297]\n",
      "[5.2527695]\n",
      "[-0.805382]\n",
      "[-0.6526375]\n",
      "[-0.24143267]\n",
      "[-1.845468]\n",
      "[-1.430716]\n",
      "[1.5496554]\n",
      "[-0.09981513]\n",
      "[-0.57653475]\n",
      "[0.25159597]\n",
      "[0.01833391]\n",
      "[7.35843]\n",
      "[1.571038]\n",
      "[-1.0788863]\n",
      "[-3.3612618]\n",
      "[-5.0635815]\n",
      "[0.6847334]\n",
      "[-3.7967849]\n",
      "[1.1912129]\n",
      "[-1.6647649]\n",
      "[1.0247655]\n",
      "[5.669855]\n",
      "[0.7702606]\n",
      "[-0.50750923]\n",
      "[0.3699081]\n",
      "[-0.51730204]\n",
      "[-6.400178]\n",
      "[-0.6863506]\n",
      "[0.20071077]\n",
      "[-3.2989383]\n",
      "[0.02833796]\n",
      "[-1.2460575]\n",
      "[-0.22257805]\n",
      "[-0.9747822]\n",
      "[1.3765354]\n",
      "[0.3623178]\n",
      "[-0.9118087]\n",
      "[-0.18231869]\n",
      "[-0.49877286]\n",
      "[6.977965]\n",
      "[3.0748549]\n",
      "[0.6400356]\n",
      "[-0.51710606]\n",
      "[-0.9234562]\n",
      "[0.66702247]\n",
      "[-0.48976398]\n",
      "[-3.0100806]\n",
      "[1.7345333]\n",
      "[-1.9638448]\n",
      "[3.8897007]\n",
      "[-2.998986]\n",
      "[1.1904755]\n",
      "[3.3179064]\n",
      "[0.29360414]\n",
      "[-0.24797225]\n",
      "[3.843442]\n",
      "[7.841127]\n",
      "[-0.3510673]\n",
      "[-0.1702373]\n",
      "[1.3041177]\n",
      "[0.6486342]\n",
      "[1.1441422]\n",
      "[1.4512241]\n",
      "[-5.593652]\n",
      "[2.0503836]\n",
      "[1.2288222]\n",
      "[1.1353033]\n",
      "[0.3678062]\n",
      "[-0.42643738]\n",
      "[4.602729]\n",
      "[0.10099959]\n",
      "[-0.70914245]\n",
      "[1.1988838]\n",
      "[-2.2463183]\n",
      "[4.295635]\n",
      "[2.066225]\n",
      "[-1.4164991]\n",
      "[0.7333255]\n",
      "[-0.83328986]\n",
      "[0.8054669]\n",
      "[-0.24098086]\n",
      "[-0.64317703]\n",
      "[-1.5081682]\n",
      "[1.0548393]\n",
      "[2.2131054]\n",
      "[1.6933295]\n",
      "[-0.23114395]\n",
      "[1.558886]\n",
      "[-0.6490021]\n",
      "[-0.2903726]\n",
      "[5.045474]\n",
      "[0.45011282]\n",
      "[1.4085684]\n",
      "[-0.35451794]\n",
      "[1.0505874]\n",
      "[1.2435625]\n",
      "[-0.29213834]\n",
      "[-0.01939702]\n",
      "[0.9661174]\n",
      "[2.5697877]\n",
      "[-1.0226645]\n",
      "[-1.3438721]\n",
      "[0.79987526]\n",
      "[-0.8483343]\n",
      "[-0.6870396]\n",
      "[1.1754146]\n",
      "[-1.534368]\n",
      "[-0.82960844]\n",
      "[-1.2627273]\n",
      "[-0.22737312]\n",
      "[1.0485246]\n",
      "[-0.70463824]\n",
      "[2.005425]\n",
      "[0.16151047]\n",
      "[-1.7319381]\n",
      "[-0.64742804]\n",
      "[1.3693397]\n",
      "[-0.5004008]\n",
      "[1.8089108]\n",
      "[-0.19998312]\n",
      "[0.4036653]\n",
      "[-1.0380864]\n",
      "[1.2980909]\n",
      "[0.63978934]\n",
      "[-0.05821061]\n",
      "[-1.0185068]\n",
      "[-1.2780602]\n",
      "[-0.74143124]\n",
      "[4.0720453]\n",
      "[1.0605662]\n",
      "[-2.1764565]\n",
      "[-2.1372044]\n",
      "[0.65463734]\n",
      "[-0.5138402]\n",
      "[0.5142052]\n",
      "[-0.14368367]\n",
      "[0.02532649]\n",
      "[2.556276]\n",
      "[0.17126703]\n",
      "[-4.624935]\n",
      "[1.4338157]\n",
      "[0.65487075]\n",
      "[-1.4915912]\n",
      "[-0.2108655]\n",
      "[-1.1068203]\n",
      "[-1.6700947]\n",
      "[-0.05121946]\n",
      "[-0.68113804]\n",
      "[1.5531144]\n",
      "[-3.4933531]\n",
      "[0.33053255]\n",
      "[0.01192236]\n",
      "[-0.47235942]\n",
      "[1.2688704]\n",
      "[-1.3364162]\n",
      "[0.47310257]\n",
      "[0.1787417]\n",
      "[0.48366642]\n",
      "[-1.132813]\n",
      "[0.10232401]\n",
      "[-1.3865824]\n",
      "[-0.6182599]\n",
      "[-4.745825]\n",
      "[-0.12674189]\n",
      "[11.478176]\n",
      "[2.0901158]\n",
      "[0.04264379]\n",
      "[-0.70191884]\n",
      "[-1.3888199]\n",
      "[-0.75856376]\n",
      "[0.65580416]\n",
      "[1.2304118]\n",
      "[-0.04418206]\n",
      "[-1.6266408]\n",
      "[0.62961125]\n",
      "[0.72034264]\n",
      "[-1.9093897]\n",
      "[1.5839529]\n",
      "[-1.3542714]\n",
      "[2.538174]\n",
      "[0.38985538]\n",
      "[2.469791]\n",
      "[-0.7783961]\n",
      "[5.456738]\n",
      "[4.06656]\n",
      "[-0.12965822]\n",
      "[-1.0066373]\n",
      "[-0.8832915]\n",
      "[1.2728345]\n",
      "[-1.9455354]\n",
      "[1.5388539]\n",
      "[-0.96513677]\n",
      "[1.7579823]\n",
      "[0.06132054]\n",
      "[-5.7952347]\n",
      "[-0.5244763]\n",
      "[0.2895534]\n",
      "[-0.38986278]\n",
      "[-0.17123795]\n",
      "[1.1178956]\n",
      "[-2.955979]\n",
      "[0.2756827]\n",
      "[-0.10590696]\n",
      "[0.35862064]\n",
      "[-1.278959]\n",
      "[-0.06889772]\n",
      "[-4.0488496]\n",
      "[0.6835828]\n",
      "[1.8318176]\n",
      "[-2.7849038]\n",
      "[0.26835823]\n",
      "[-1.3641753]\n",
      "[-0.5791695]\n",
      "[-0.8223841]\n",
      "[-0.86511564]\n",
      "[-0.86781335]\n",
      "[-0.5744152]\n",
      "[1.9408076]\n",
      "[1.2938635]\n",
      "[-1.1767304]\n",
      "[0.3658557]\n",
      "[-0.9874797]\n",
      "[1.4626899]\n",
      "[-1.5881941]\n",
      "[0.86683154]\n",
      "[-0.46926403]\n",
      "[-2.3736835]\n",
      "[-1.7506869]\n",
      "[-1.7515242]\n",
      "[-0.9863856]\n",
      "[-1.5199583]\n",
      "[-1.4998047]\n",
      "[0.11493158]\n",
      "[1.0441518]\n",
      "[-2.0899734]\n",
      "[-0.9519253]\n",
      "[0.23894167]\n",
      "[-0.15673923]\n",
      "[1.4803584]\n",
      "[-0.9948044]\n",
      "[2.5601654]\n",
      "[1.3092346]\n",
      "[4.271591]\n",
      "[0.22966218]\n",
      "[2.1106586]\n",
      "[-2.802723]\n",
      "[1.3237491]\n",
      "[-2.0555627]\n",
      "[-2.4943573]\n",
      "[-2.50138]\n",
      "[-1.6890306]\n",
      "[1.5780745]\n",
      "[0.0960927]\n",
      "[0.56659603]\n",
      "[-2.563275]\n",
      "[-1.53789]\n",
      "[0.45811486]\n",
      "[0.04417515]\n",
      "[0.78166556]\n",
      "[0.24979949]\n",
      "[-4.928686]\n",
      "[0.34588742]\n",
      "[0.8208144]\n",
      "[-1.6164966]\n",
      "[0.05450368]\n",
      "[-0.31477976]\n",
      "[-1.6246552]\n",
      "[1.6143906]\n",
      "[5.1872144]\n",
      "[-0.5577738]\n",
      "[0.31443357]\n",
      "[-0.2432363]\n",
      "[-0.66039777]\n",
      "[1.4191885]\n",
      "[1.6378953]\n",
      "[1.120081]\n",
      "[0.5240147]\n",
      "[0.25777745]\n",
      "[-0.2823925]\n",
      "[-3.5050743]\n",
      "[-0.47859097]\n",
      "[-0.30621815]\n",
      "[1.9612339]\n",
      "[-0.04990935]\n",
      "[-1.8136244]\n",
      "[0.3968618]\n",
      "[-0.1714921]\n",
      "[-0.24622226]\n",
      "[-5.145316]\n",
      "[1.1129732]\n",
      "[-0.04610157]\n",
      "[-0.20493007]\n",
      "[1.8366063]\n",
      "[0.79493666]\n",
      "[-3.1432505]\n",
      "[0.9849014]\n",
      "[-1.7517183]\n",
      "[0.03764391]\n",
      "[-3.4998698]\n",
      "[1.1162372]\n",
      "[1.3878064]\n",
      "[0.30848098]\n",
      "[0.46913934]\n",
      "[-4.1955323]\n",
      "[0.21564507]\n",
      "[0.62847614]\n",
      "[-0.33169723]\n",
      "[2.029483]\n",
      "[-0.05316114]\n",
      "[4.7719264]\n",
      "[-1.3437676]\n",
      "[1.359056]\n",
      "[-0.4788313]\n",
      "[2.0144894]\n",
      "[2.6327448]\n",
      "[0.81950974]\n",
      "[1.3493311]\n",
      "[-2.2631521]\n",
      "[-0.09892941]\n",
      "[-0.58834076]\n",
      "[-0.20783234]\n",
      "[-0.3770926]\n",
      "[-0.14074206]\n",
      "[-2.0774565]\n",
      "[-0.41715074]\n",
      "[-1.3119295]\n",
      "[0.14964962]\n",
      "[1.7932374]\n",
      "[2.7586176]\n",
      "[-1.1520758]\n",
      "[-2.3419158]\n",
      "[-0.35883403]\n",
      "[2.0973125]\n",
      "[-3.3192115]\n",
      "[2.0962033]\n",
      "[1.1965766]\n",
      "[1.3646843]\n",
      "[-0.97841644]\n",
      "[-0.46587157]\n",
      "[-0.77358794]\n",
      "[2.0522287]\n",
      "[0.7427833]\n",
      "[-0.17365456]\n",
      "[0.14696503]\n",
      "[4.585335]\n",
      "[1.4942226]\n",
      "[0.6056926]\n",
      "[1.8943517]\n",
      "[-0.4731722]\n",
      "[0.22402406]\n",
      "[1.14045]\n",
      "[-0.07819939]\n",
      "[-0.7071526]\n",
      "[1.3361263]\n",
      "[-0.70472145]\n",
      "[5.0576224]\n",
      "[-0.39591146]\n",
      "[-0.769325]\n",
      "[-1.3500907]\n",
      "[0.9011383]\n",
      "[-0.13075447]\n",
      "[-0.4757583]\n",
      "[-0.44270492]\n",
      "[0.0276289]\n",
      "[1.03776]\n",
      "[-0.14645743]\n",
      "[-1.5089657]\n",
      "[1.516536]\n",
      "[-2.3517275]\n",
      "[0.34607625]\n",
      "[-0.7675986]\n",
      "[0.68051934]\n",
      "[2.3128052]\n",
      "[-0.22645569]\n",
      "[1.0855992]\n",
      "[0.7078681]\n",
      "[-1.3260376]\n",
      "[0.24039721]\n",
      "[0.13035297]\n",
      "[-9.990238]\n",
      "[-0.69144416]\n",
      "[-0.8041582]\n",
      "[-1.3605893]\n",
      "[0.6330857]\n",
      "[1.425839]\n",
      "[0.22939134]\n",
      "[1.8064971]\n",
      "[2.2184358]\n",
      "[-1.2988038]\n",
      "[0.725389]\n",
      "[-2.1269884]\n",
      "[5.2553606]\n",
      "[0.06908131]\n",
      "[1.5441623]\n",
      "[-0.1040864]\n",
      "[3.3633811]\n",
      "[0.32613873]\n",
      "[-7.203437]\n",
      "[-2.3890095]\n",
      "[1.0612931]\n",
      "[-1.3341894]\n",
      "[-1.9707379]\n",
      "[-0.9883423]\n",
      "[0.52573013]\n",
      "[1.4181876]\n",
      "[-1.4993851]\n",
      "[-2.4728625]\n",
      "[5.6775765]\n",
      "[2.3788369]\n",
      "[-1.0991983]\n",
      "[-6.589459]\n",
      "[0.90683293]\n",
      "[-1.5491428]\n",
      "[-2.0968685]\n",
      "[3.0632212]\n",
      "[0.07230353]\n",
      "[0.5486629]\n",
      "[0.410645]\n",
      "[-0.08548164]\n",
      "[0.39756203]\n",
      "[0.36980057]\n",
      "[2.0614817]\n",
      "[1.639411]\n",
      "[0.5053022]\n",
      "[-0.70191073]\n",
      "[0.18952823]\n",
      "[0.04329967]\n",
      "[-1.81825]\n",
      "[-1.4784448]\n",
      "[-0.45709133]\n",
      "[0.88938355]\n",
      "[-0.63242364]\n",
      "[-1.0921638]\n",
      "[0.40636158]\n",
      "[-0.7766266]\n",
      "[1.331018]\n",
      "[0.28355074]\n",
      "[0.07553244]\n",
      "[-4.4060335]\n",
      "[-1.0938537]\n",
      "[-1.1057894]\n",
      "[1.3362937]\n",
      "[0.3459978]\n",
      "[-0.6861732]\n",
      "[-0.51594186]\n",
      "[-2.146345]\n",
      "[0.19105577]\n",
      "[-0.6406157]\n",
      "[-0.46140623]\n",
      "[0.04972816]\n",
      "[-1.5276024]\n",
      "[-1.7801392]\n",
      "[0.5914097]\n",
      "[-0.2814343]\n",
      "[-1.3185298]\n",
      "[-1.7352374]\n",
      "[-0.63942814]\n",
      "[0.73173]\n",
      "[-1.4685667]\n",
      "[1.8658409]\n",
      "[0.633198]\n",
      "[-0.01169157]\n",
      "[-0.9623182]\n",
      "[-0.74554014]\n",
      "[-1.0825508]\n",
      "[0.5102954]\n",
      "[0.47005558]\n",
      "[-2.3803058]\n",
      "[-2.72722]\n",
      "[1.1733494]\n",
      "[0.5512874]\n",
      "[1.2626476]\n",
      "[2.5716178]\n",
      "[-1.1603487]\n",
      "[-1.5416162]\n",
      "[0.3952787]\n",
      "[-0.9894161]\n",
      "[-1.5611553]\n",
      "[1.1251597]\n",
      "[4.9380865]\n",
      "[5.9342113]\n",
      "[-0.5747585]\n",
      "[1.1719034]\n",
      "[1.094002]\n",
      "[0.34872293]\n",
      "[-0.86716056]\n",
      "[4.580085]\n",
      "[-2.4365869]\n",
      "[-0.32491755]\n",
      "[-0.48490167]\n",
      "[3.4730532]\n",
      "[-6.2241564]\n",
      "[-3.0417268]\n",
      "[1.2028184]\n",
      "[-0.1340375]\n",
      "[3.9257655]\n",
      "[-0.28720546]\n",
      "[-0.85341]\n",
      "[-0.42109728]\n",
      "[0.1323719]\n",
      "[0.6996553]\n",
      "[-1.8514705]\n",
      "[0.4607098]\n",
      "[-1.8559229]\n",
      "[0.8125875]\n",
      "[-0.56616235]\n",
      "[1.2803895]\n",
      "[1.3681712]\n",
      "[1.2710495]\n",
      "[0.5741489]\n",
      "[0.5949371]\n",
      "[-2.3436337]\n",
      "[-0.14205313]\n",
      "[-0.3893001]\n",
      "[-1.1603668]\n",
      "[1.0203125]\n",
      "[-0.11452937]\n",
      "[-0.6221945]\n",
      "[-1.132663]\n",
      "[-0.52641535]\n",
      "[-0.3133545]\n",
      "[-0.21488881]\n",
      "[-0.7217355]\n",
      "[-1.0796921]\n",
      "[-0.7241552]\n",
      "[5.998957]\n",
      "[-0.5839369]\n",
      "[-0.45293903]\n",
      "[-0.08660054]\n",
      "[3.415848]\n",
      "[-0.8634796]\n",
      "[-1.2960267]\n",
      "[-0.39843178]\n",
      "[-4.649002]\n",
      "[-0.5259857]\n",
      "[0.21760297]\n",
      "[7.497429]\n",
      "[-0.5855584]\n",
      "[-0.37420964]\n",
      "[3.744593]\n",
      "[-4.116835]\n",
      "[0.28042006]\n",
      "[-0.7176459]\n",
      "[0.92464805]\n",
      "[1.315237]\n",
      "[-1.3169751]\n",
      "[1.5567181]\n",
      "[-0.8528094]\n",
      "[-0.909637]\n",
      "[0.69463706]\n",
      "[-7.615898]\n",
      "[1.790473]\n",
      "[0.02870965]\n",
      "[-2.6762505]\n",
      "[1.360677]\n",
      "[0.02978039]\n",
      "[-3.402328]\n",
      "[-0.8278861]\n",
      "[5.164747]\n",
      "[-5.206933]\n",
      "[0.10202456]\n",
      "[0.3697853]\n",
      "[-0.00342107]\n",
      "[0.13251305]\n",
      "[0.18992352]\n",
      "[-0.19193411]\n",
      "[0.95326424]\n",
      "[-0.9182396]\n",
      "[-4.7892046]\n",
      "[-0.20552516]\n",
      "[0.24685884]\n",
      "[-0.5598893]\n",
      "[-0.02596188]\n",
      "[-0.98057437]\n",
      "[1.839143]\n",
      "[2.1017733]\n",
      "[0.10037947]\n",
      "[2.1194854]\n",
      "[2.5031571]\n",
      "[-0.11447859]\n",
      "[-1.6168199]\n",
      "[0.32653403]\n",
      "[1.8252509]\n",
      "[1.8784833]\n",
      "[-0.0770092]\n",
      "[-0.90371513]\n",
      "[0.17721319]\n",
      "[-4.9893646]\n",
      "[-2.0516188]\n",
      "[-0.16326785]\n",
      "[0.41415668]\n",
      "[2.0246696]\n",
      "[0.15569568]\n",
      "[0.2529621]\n",
      "[0.6854081]\n",
      "[-3.8344085]\n",
      "[-0.6543293]\n",
      "[-0.8700502]\n",
      "[0.08328176]\n",
      "[-0.21545148]\n",
      "[-2.2303991]\n",
      "[-3.8302743]\n",
      "[-1.6373861]\n",
      "[1.4755058]\n",
      "[1.0055008]\n",
      "[-0.00712109]\n",
      "[1.8145366]\n",
      "[-0.47669363]\n",
      "[0.54266095]\n",
      "[-2.2463803]\n",
      "[-1.266228]\n",
      "[-1.0081334]\n",
      "[-0.42025733]\n",
      "[-0.61493754]\n",
      "[-3.7716148]\n",
      "[0.81290555]\n",
      "[-1.8880427]\n",
      "[-2.1041806]\n",
      "[0.26494408]\n",
      "[-0.077842]\n",
      "[0.554445]\n",
      "[0.48428392]\n",
      "[-5.120657]\n",
      "[0.40054512]\n",
      "[-4.795741]\n",
      "[0.49640107]\n",
      "[-0.00721765]\n",
      "[-1.5762475]\n",
      "[4.250231]\n",
      "[0.2956853]\n",
      "[-0.2655964]\n",
      "[-3.1067605]\n",
      "[2.2343874]\n",
      "[-0.40582967]\n",
      "[-0.20486069]\n",
      "[-2.3196356]\n",
      "[-0.60917664]\n",
      "[0.47437525]\n",
      "[0.12287378]\n",
      "[0.7765229]\n",
      "[-0.90414214]\n",
      "[-0.42672706]\n",
      "[1.7567437]\n",
      "[1.8138299]\n",
      "[-5.1591997]\n",
      "[-1.8950856]\n",
      "[-4.3421845]\n",
      "[0.05320621]\n",
      "[0.76444936]\n",
      "[0.06914067]\n",
      "[-0.9506359]\n",
      "[0.63839626]\n",
      "[-0.27224302]\n",
      "[0.30377293]\n",
      "[-0.4043131]\n",
      "[0.70242023]\n",
      "[-2.106898]\n",
      "[0.05205107]\n",
      "[-1.3021734]\n",
      "[-0.28080273]\n",
      "[-0.11066437]\n",
      "[2.4073694]\n",
      "[0.7960565]\n",
      "[1.8968232]\n",
      "[-4.3736835]\n",
      "[-0.32115364]\n",
      "[0.02510738]\n",
      "[0.16951895]\n",
      "[1.4057188]\n",
      "[0.23598862]\n",
      "[-0.9983115]\n",
      "[-5.9290648]\n",
      "[-0.3071028]\n",
      "[-1.8999338]\n",
      "[0.36270523]\n",
      "[-0.83945656]\n",
      "[-0.36101437]\n",
      "[0.2015729]\n",
      "[-0.12027788]\n",
      "[-1.0651009]\n",
      "[1.2836773]\n",
      "[1.3690221]\n",
      "[2.3079097]\n",
      "[3.3525321]\n",
      "[0.5822756]\n",
      "[0.03085971]\n",
      "[0.7895379]\n",
      "[-0.59416556]\n",
      "[-0.9471171]\n",
      "[-0.16707349]\n",
      "[-3.6555343]\n",
      "[0.76099706]\n",
      "[-0.35467243]\n",
      "[-0.8387413]\n",
      "[-0.49106932]\n",
      "[1.02069]\n",
      "[1.7849641]\n",
      "[1.182163]\n",
      "[-1.247808]\n",
      "[-0.55604935]\n",
      "[-6.7964525]\n",
      "[-0.6689236]\n",
      "[0.40914536]\n",
      "[-1.2221615]\n",
      "[0.1106205]\n",
      "[1.2026107]\n",
      "[-0.16995525]\n",
      "[-0.2760551]\n",
      "[-0.30446553]\n",
      "[-0.05816436]\n",
      "[-1.2119548]\n",
      "[-0.4339304]\n",
      "[-0.78469753]\n",
      "[-0.82027435]\n",
      "[-0.7089851]\n",
      "[-1.571074]\n",
      "[0.19548106]\n",
      "[-0.35690165]\n",
      "[0.32187033]\n",
      "[2.1040523]\n",
      "[0.34845495]\n",
      "[-0.9359758]\n",
      "[-3.778314]\n",
      "[-0.99436927]\n",
      "[0.8826816]\n",
      "[-0.55609894]\n",
      "[-1.3145142]\n",
      "[-0.29503703]\n",
      "[-0.21543074]\n",
      "[-7.425467]\n",
      "[-0.6761837]\n",
      "[-0.66901493]\n",
      "[2.5331357]\n",
      "[3.0068076]\n",
      "[-2.5185976]\n",
      "[-0.1752255]\n",
      "[-0.35269213]\n",
      "[-1.1584475]\n",
      "[0.972034]\n",
      "[-0.77426577]\n",
      "[0.66975236]\n",
      "[-0.75563884]\n",
      "[-0.60827327]\n",
      "[-0.19206548]\n",
      "[-0.6876743]\n",
      "[-0.4097302]\n",
      "[2.033738]\n",
      "[-0.33241582]\n",
      "[-0.2714796]\n",
      "[-1.2427771]\n",
      "[-0.6842742]\n",
      "[-0.58111143]\n",
      "[-0.41717124]\n",
      "[0.6636541]\n",
      "[0.25221586]\n",
      "[0.9187639]\n",
      "[-1.1034508]\n",
      "[-2.3951426]\n",
      "[-2.3046978]\n",
      "[1.3374515]\n",
      "[-0.17749858]\n",
      "[-2.4742575]\n",
      "[0.48956728]\n",
      "[2.1422296]\n",
      "[-0.61200404]\n",
      "[1.4834802]\n",
      "[-0.41553164]\n",
      "[1.4070845]\n",
      "[-1.1198778]\n",
      "[0.1172452]\n",
      "[-2.3231468]\n",
      "[-0.63025045]\n",
      "[-0.19111204]\n",
      "[0.05750775]\n",
      "[-0.20259833]\n",
      "[-0.5449655]\n",
      "[0.8496251]\n",
      "[0.18127394]\n",
      "[-1.1869733]\n",
      "[0.15107346]\n",
      "[-0.37507558]\n",
      "[-0.36379838]\n",
      "[7.798992]\n",
      "[-0.55627227]\n",
      "[1.12619]\n",
      "[-1.006567]\n",
      "[0.1017518]\n",
      "[1.4114685]\n",
      "[2.6215856]\n",
      "[-5.090261]\n",
      "[-1.114883]\n",
      "[-0.5460644]\n",
      "[-0.9154768]\n",
      "[-2.2571514]\n",
      "[0.00081635]\n",
      "[0.30093217]\n",
      "[-0.7417455]\n",
      "[-0.26495194]\n",
      "[0.7178891]\n",
      "[-1.8404534]\n",
      "[0.22970462]\n",
      "[0.8819294]\n",
      "[-0.8876622]\n",
      "[2.9443183]\n",
      "[-0.92031074]\n",
      "[-0.6402924]\n",
      "[-1.5274508]\n",
      "[-2.4504716]\n",
      "[-3.8334434]\n",
      "[-0.53080773]\n",
      "[2.642055]\n",
      "[0.6841059]\n",
      "[-0.2127912]\n",
      "[1.2769811]\n",
      "[-1.2147241]\n",
      "[-0.10045385]\n",
      "[-1.1905482]\n",
      "[2.6956148]\n",
      "[-4.0409555]\n",
      "[0.44766903]\n",
      "[-1.3099208]\n",
      "[-0.5394814]\n",
      "[-0.94961786]\n",
      "[-0.51988316]\n",
      "[0.87969804]\n",
      "[-1.7529271]\n",
      "[0.01995945]\n",
      "[-1.8972836]\n",
      "[-2.814196]\n",
      "[0.82887506]\n",
      "[1.1452117]\n",
      "[1.2978225]\n",
      "[0.4245473]\n",
      "[-1.9515142]\n",
      "[0.33462667]\n",
      "[-0.32821035]\n",
      "[-2.2974746]\n",
      "[-3.9281926]\n",
      "[-6.2910786]\n",
      "[-0.65312743]\n",
      "[0.56170154]\n",
      "[1.4334586]\n",
      "[0.84672356]\n",
      "[-0.4222014]\n",
      "[-2.473737]\n",
      "[0.99461365]\n",
      "[0.19603634]\n",
      "[6.929377]\n",
      "[-2.1232765]\n",
      "[0.2912321]\n",
      "[-2.873541]\n",
      "[1.0195053]\n",
      "[-0.16278934]\n",
      "[3.857178]\n",
      "[0.7919836]\n",
      "[1.0181062]\n",
      "[1.7961695]\n",
      "[1.1527848]\n",
      "[-0.76208496]\n",
      "[1.8228672]\n",
      "[0.13921738]\n",
      "[-2.4279492]\n",
      "[2.298784]\n",
      "[1.2685115]\n",
      "[0.46545196]\n",
      "[-0.4239998]\n",
      "[-0.5440166]\n",
      "[-0.38278413]\n",
      "[0.4737091]\n",
      "[-0.26066732]\n",
      "[-2.1172197]\n",
      "[2.3300405]\n",
      "[3.5217311]\n",
      "[-0.05657196]\n",
      "[-9.28545]\n",
      "[2.8366394]\n",
      "[0.42624664]\n",
      "[-1.380111]\n",
      "[0.72148323]\n",
      "[0.8295119]\n",
      "[0.96460485]\n",
      "[0.00246525]\n",
      "[0.2949221]\n",
      "[-0.28308535]\n",
      "[0.00296044]\n",
      "[-0.7192702]\n",
      "[0.48521256]\n",
      "[-2.0341425]\n",
      "[-0.43517828]\n",
      "[-1.1956666]\n",
      "[-9.3299465]\n",
      "[-1.866766]\n",
      "[2.9895504]\n",
      "[-0.8433578]\n",
      "[7.9763246]\n",
      "[0.34501696]\n",
      "[0.78044605]\n",
      "[0.4439447]\n",
      "[0.09127498]\n",
      "[-2.964317]\n",
      "[2.3795655]\n",
      "[-0.3539033]\n",
      "[-0.43995118]\n",
      "[-0.9152117]\n",
      "[-1.428143]\n",
      "[0.1698749]\n",
      "[1.2930477]\n",
      "[2.1935496]\n",
      "[-0.11744285]\n",
      "[-1.1759567]\n",
      "[7.3813667]\n",
      "[-0.6656699]\n",
      "[-1.03706]\n",
      "[-0.4670279]\n",
      "[-0.46206737]\n",
      "[-2.2003236]\n",
      "[0.4560566]\n",
      "[-1.3297231]\n",
      "[-0.27818942]\n",
      "[0.99211407]\n",
      "[-4.5816813]\n",
      "[-0.8438282]\n",
      "[0.23875093]\n",
      "[0.2810619]\n",
      "[-0.47458696]\n",
      "[-1.0526087]\n",
      "[-0.83128047]\n",
      "[-0.98213243]\n",
      "[-1.5183756]\n",
      "[-1.2553835]\n",
      "[1.8130903]\n",
      "[0.4763863]\n",
      "[-0.6962986]\n",
      "[0.36763787]\n",
      "[-1.035068]\n",
      "[2.1041737]\n",
      "[0.39380646]\n",
      "[1.1003067]\n",
      "[0.44072533]\n",
      "[2.457316]\n",
      "[0.2812419]\n",
      "[-0.38093543]\n",
      "[1.5671229]\n",
      "[0.795944]\n",
      "[-0.84762144]\n",
      "[0.13154244]\n",
      "[0.661036]\n",
      "[-0.76414347]\n",
      "[-0.3198588]\n",
      "[-1.3600504]\n",
      "[-0.67317224]\n",
      "[0.5950258]\n",
      "[0.5819018]\n",
      "[-0.3611095]\n",
      "[-2.9936118]\n",
      "[-0.71998906]\n",
      "[-1.3599672]\n",
      "[0.38771868]\n",
      "[1.531975]\n",
      "[1.872478]\n",
      "[2.2090583]\n",
      "[2.9805539]\n",
      "[-0.67797685]\n",
      "[0.30282402]\n",
      "[6.0668173]\n",
      "[1.3483772]\n",
      "[-0.03371453]\n",
      "[4.5658026]\n",
      "[0.804564]\n",
      "[-0.5146496]\n",
      "[3.25276]\n",
      "[0.40747976]\n",
      "[-0.8002155]\n",
      "[-2.4309793]\n",
      "[-1.0569363]\n",
      "[0.2620964]\n",
      "[7.0981607]\n",
      "[1.0596399]\n",
      "[5.3425465]\n",
      "[0.2273221]\n",
      "[-1.137815]\n",
      "[0.15937424]\n",
      "[-0.41985106]\n",
      "[0.04395032]\n",
      "[0.8700962]\n",
      "[0.02398968]\n",
      "[0.25552654]\n",
      "[-2.5303884]\n",
      "[0.3190763]\n",
      "[-1.1478329]\n",
      "[-3.319345]\n",
      "[-0.3368454]\n",
      "[0.20189118]\n",
      "[3.5670564]\n",
      "[1.6250751]\n",
      "[0.3385961]\n",
      "[4.789318]\n",
      "[0.44636416]\n",
      "[-5.2613893]\n",
      "[-1.4252076]\n",
      "[0.21552014]\n",
      "[0.16512966]\n",
      "[1.5995378]\n",
      "[1.122541]\n",
      "[0.93916225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16777611]\n",
      "[0.11116409]\n",
      "[0.8576369]\n",
      "[6.076137]\n",
      "[4.1552033]\n",
      "[1.0162232]\n",
      "[-0.85947824]\n",
      "[0.40429306]\n",
      "[0.17306566]\n",
      "[0.58384323]\n",
      "[-4.795512]\n",
      "[-3.4665961]\n",
      "[0.5508342]\n",
      "[-0.06585741]\n",
      "[0.29432797]\n",
      "[-0.82868457]\n",
      "[-0.5363462]\n",
      "[1.9206531]\n",
      "[5.1957293]\n",
      "[2.3166957]\n",
      "[2.7147062]\n",
      "[0.0967586]\n",
      "[-0.63895464]\n",
      "[-2.173064]\n",
      "[2.1285973]\n",
      "[0.28227925]\n",
      "[0.49567795]\n",
      "[0.7330942]\n",
      "[0.20447516]\n",
      "[0.6084175]\n",
      "[0.7313218]\n",
      "[-0.81278896]\n",
      "[-1.323236]\n",
      "[-1.0934548]\n",
      "[-0.55406666]\n",
      "[2.4549558]\n",
      "[-0.8142967]\n",
      "[0.378119]\n",
      "[0.64066577]\n",
      "[1.0519466]\n",
      "[-0.30157185]\n",
      "[-1.1775756]\n",
      "[1.5421348]\n",
      "[-1.145236]\n",
      "[1.0738957]\n",
      "[-1.9593074]\n",
      "[0.73048973]\n",
      "[-0.5450394]\n",
      "[-0.74257493]\n",
      "[-0.61905956]\n",
      "[-0.27230453]\n",
      "[-0.33935094]\n",
      "[0.39665794]\n",
      "[-0.31826591]\n",
      "[-6.8135834]\n",
      "[-3.7679315]\n",
      "[-5.332387]\n",
      "[3.7163992]\n",
      "[-0.20681858]\n",
      "[1.1022072]\n",
      "[-0.18240547]\n",
      "[0.15366483]\n",
      "[-0.37123346]\n",
      "[1.7866654]\n",
      "[0.87887955]\n",
      "[-0.07040834]\n",
      "[-3.463981]\n",
      "[-1.3060813]\n",
      "[0.09858227]\n",
      "[0.26259875]\n",
      "[-0.38127327]\n",
      "[-1.4416752]\n",
      "[0.54943967]\n",
      "[0.06811094]\n",
      "[2.2239819]\n",
      "[2.7334971]\n",
      "[-0.53772163]\n",
      "[-0.9246609]\n",
      "[-1.4919453]\n",
      "[-0.6205063]\n",
      "[2.2816103]\n",
      "[-1.6726196]\n",
      "[-1.5149603]\n",
      "[1.0796697]\n",
      "[-3.5387425]\n",
      "[1.362323]\n",
      "[-0.9108145]\n",
      "[-2.1633694]\n",
      "[-0.5496111]\n",
      "[1.3132303]\n",
      "[1.3094711]\n",
      "[0.03642368]\n",
      "[1.8630004]\n",
      "[-0.4272852]\n",
      "[-0.53811264]\n",
      "[1.5146649]\n",
      "[-0.49055147]\n",
      "[0.09132147]\n",
      "[-0.06921792]\n",
      "[0.12141562]\n",
      "[3.535527]\n",
      "[1.7443054]\n",
      "[5.3842463]\n",
      "[-0.6996722]\n",
      "[0.05944967]\n",
      "[-6.8149986]\n",
      "[3.082443]\n",
      "[0.24721599]\n",
      "[-1.151242]\n",
      "[-0.43060827]\n",
      "[-3.841519]\n",
      "[-1.2160866]\n",
      "[2.704182]\n",
      "[-0.7994416]\n",
      "[0.40229368]\n",
      "[-2.9421089]\n",
      "[0.37066364]\n",
      "[0.79749393]\n",
      "[0.48517537]\n",
      "[0.16617203]\n",
      "[7.31042]\n",
      "[-1.5117257]\n",
      "[3.5383773]\n",
      "[0.15994143]\n",
      "[-2.536856]\n",
      "[-2.4318476]\n",
      "[1.2658701]\n",
      "[0.71817946]\n",
      "[-0.20203733]\n",
      "[1.2605135]\n",
      "[2.4086285]\n",
      "[-0.29782557]\n",
      "[-0.87733126]\n",
      "[0.281708]\n",
      "[-5.807055]\n",
      "[-0.31294107]\n",
      "[1.225008]\n",
      "[1.3852508]\n",
      "[0.25843406]\n",
      "[0.53202677]\n",
      "[-0.71147704]\n",
      "[-2.5780387]\n",
      "[0.7874725]\n",
      "[-0.599771]\n",
      "[-0.65144396]\n",
      "[-1.170191]\n",
      "[1.1751611]\n",
      "[-0.08905053]\n",
      "[1.408078]\n",
      "[-0.11699462]\n",
      "[-2.9564342]\n",
      "[0.14815283]\n",
      "[1.281249]\n",
      "[3.8089337]\n",
      "[-0.4667778]\n",
      "[-0.07442307]\n",
      "[0.26129246]\n",
      "[1.1758418]\n",
      "[1.5840704]\n",
      "[0.60365105]\n",
      "[0.61237025]\n",
      "[1.1809018]\n",
      "[3.247347]\n",
      "[-0.17088938]\n",
      "[-0.12370825]\n",
      "[-0.6781769]\n",
      "[-1.3827729]\n",
      "[-1.2094159]\n",
      "[-0.31212354]\n",
      "[-0.5285361]\n",
      "[-0.41069937]\n",
      "[-0.38589835]\n",
      "[0.615031]\n",
      "[0.7202506]\n",
      "[-3.9599836]\n",
      "[0.14987183]\n",
      "[-1.6086969]\n",
      "[-0.75780225]\n",
      "[0.5482762]\n",
      "[-0.72861814]\n",
      "[-0.1879673]\n",
      "[0.8964896]\n",
      "[2.270276]\n",
      "[0.38102913]\n",
      "[0.48735952]\n",
      "[1.0549958]\n",
      "[0.7649143]\n",
      "[0.78693914]\n",
      "[0.36861515]\n",
      "[-3.6038604]\n",
      "[0.51756716]\n",
      "[0.02120733]\n",
      "[0.80352974]\n",
      "[1.7972665]\n",
      "[0.41242266]\n",
      "[-2.4781556]\n",
      "[0.22364187]\n",
      "[0.60486484]\n",
      "[1.7250869]\n",
      "[0.35925293]\n",
      "[0.23099136]\n",
      "[1.6659491]\n",
      "[3.6620588]\n",
      "[0.695812]\n",
      "[3.176303]\n",
      "[-1.0904007]\n",
      "[-3.701038]\n",
      "[-0.09545493]\n",
      "[0.07736707]\n",
      "[-1.6332388]\n",
      "[-2.876195]\n",
      "[-0.4174521]\n",
      "[-2.0027287]\n",
      "[-0.94279647]\n",
      "[-0.18378758]\n",
      "[-1.0600593]\n",
      "[-4.860408]\n",
      "[0.30745387]\n",
      "[-0.19901514]\n",
      "[3.3378975]\n",
      "[-0.96012974]\n",
      "[2.1116552]\n",
      "[-1.1441057]\n",
      "[-2.582641]\n",
      "[-1.1146817]\n",
      "[0.13851357]\n",
      "[-1.9351113]\n",
      "[-1.6354833]\n",
      "[-1.0676894]\n",
      "[-1.2749016]\n",
      "[-0.9587264]\n",
      "[0.9000349]\n",
      "[-0.43682528]\n",
      "[0.3575952]\n",
      "[0.88691473]\n",
      "[-0.3136661]\n",
      "[1.386045]\n",
      "[1.440196]\n",
      "[0.27973652]\n",
      "[-4.9855065]\n",
      "[0.79564977]\n",
      "[1.2562034]\n",
      "[0.4910717]\n",
      "[-0.27450895]\n",
      "[-0.73949146]\n",
      "[1.1732502]\n",
      "[1.1994796]\n",
      "[-1.502125]\n",
      "[1.0564117]\n",
      "[0.1230576]\n",
      "[0.4671309]\n",
      "[0.8722956]\n",
      "[-0.645524]\n",
      "[-0.507427]\n",
      "[0.4905784]\n",
      "[-0.4835205]\n",
      "[1.6812344]\n",
      "[-1.2525489]\n",
      "[0.70447516]\n",
      "[0.36207533]\n",
      "[-2.9069035]\n",
      "[0.41697192]\n",
      "[1.1643496]\n",
      "[-0.47929025]\n",
      "[-0.13143921]\n",
      "[0.47446918]\n",
      "[1.1428318]\n",
      "[0.3077395]\n",
      "[2.6888428]\n",
      "[0.77559185]\n",
      "[-3.0087464]\n",
      "[-2.520391]\n",
      "[-0.5770278]\n",
      "[-1.0405295]\n",
      "[2.2835188]\n",
      "[8.628263]\n",
      "[0.4217205]\n",
      "[0.09777474]\n",
      "[0.86680984]\n",
      "[0.07081699]\n",
      "[0.53914857]\n",
      "[0.5588534]\n",
      "[-0.23944688]\n",
      "[-1.0118735]\n",
      "[-1.6973712]\n",
      "[5.544797]\n",
      "[0.26939583]\n",
      "[0.05549288]\n",
      "[2.0420535]\n",
      "[0.24706602]\n",
      "[-2.594866]\n",
      "[2.1083918]\n",
      "[-0.0467]\n",
      "[0.9924946]\n",
      "[0.05247402]\n",
      "[-0.7816949]\n",
      "[0.7770505]\n",
      "[2.6970005]\n",
      "[1.3966837]\n",
      "[0.61267257]\n",
      "[-0.44589925]\n",
      "[0.51650214]\n",
      "[0.06599998]\n",
      "[1.6278224]\n",
      "[1.7828231]\n",
      "[2.211117]\n",
      "[1.1314161]\n",
      "[1.855253]\n",
      "[2.69732]\n",
      "[0.17901278]\n",
      "[1.6936941]\n",
      "[0.10207319]\n",
      "[0.4773867]\n",
      "[-2.0613437]\n",
      "[0.5814233]\n",
      "[-1.6310797]\n",
      "[5.463337]\n",
      "[-2.574938]\n",
      "[0.04727602]\n",
      "[-2.490464]\n",
      "[-0.14779854]\n",
      "[0.4377358]\n",
      "[0.9613347]\n",
      "[0.25195718]\n",
      "[-2.1115477]\n",
      "[0.55002046]\n",
      "[1.3838665]\n",
      "[1.3743]\n",
      "[0.5277474]\n",
      "[0.04980063]\n",
      "[1.8327613]\n",
      "[7.6479087]\n",
      "[1.3844504]\n",
      "[-2.0344582]\n",
      "[5.2180014]\n",
      "[12.964293]\n",
      "[-1.3048358]\n",
      "[-0.80591583]\n",
      "[0.20834827]\n",
      "[3.1245675]\n",
      "[0.7050705]\n",
      "[-0.14916396]\n",
      "[0.7556703]\n",
      "[0.63876843]\n",
      "[-0.66837406]\n",
      "[-0.2374196]\n",
      "[-0.37603164]\n",
      "[-0.06098151]\n",
      "[4.0972958]\n",
      "[1.0777242]\n",
      "[-0.6024058]\n",
      "[0.25314045]\n",
      "[1.6157963]\n",
      "[1.1836038]\n",
      "[2.2815266]\n",
      "[1.3733647]\n",
      "[-0.18090367]\n",
      "[0.9759631]\n",
      "[-0.31018972]\n",
      "[0.9473076]\n",
      "[2.3462193]\n",
      "[1.8243523]\n",
      "[2.810684]\n",
      "[-1.3633747]\n",
      "[0.72463727]\n",
      "[0.23045635]\n",
      "[3.6594498]\n",
      "[-0.07407403]\n",
      "[-3.7123117]\n",
      "[2.045989]\n",
      "[0.06912065]\n",
      "[-0.14951015]\n",
      "[1.5594592]\n",
      "[0.39880967]\n",
      "[2.2325685]\n",
      "[-0.13610792]\n",
      "[0.8560777]\n",
      "[-1.1405258]\n",
      "[-0.17472959]\n",
      "[-0.01721263]\n",
      "[0.41982365]\n",
      "[-1.4142439]\n",
      "[0.4681394]\n",
      "[-1.7627215]\n",
      "[0.24749851]\n",
      "[0.7447157]\n",
      "[-1.7053206]\n",
      "[0.11571741]\n",
      "[-1.4768696]\n",
      "[-0.25798655]\n",
      "[5.746326]\n",
      "[-0.2673695]\n",
      "[0.34479284]\n",
      "[0.00581145]\n",
      "[1.6380525]\n",
      "[8.238237]\n",
      "[0.8166034]\n",
      "[3.2134905]\n",
      "[1.6022673]\n",
      "[1.254406]\n",
      "[-1.7772348]\n",
      "[-1.7311795]\n",
      "[0.03643823]\n",
      "[-1.7095852]\n",
      "[4.594019]\n",
      "[-0.8585372]\n",
      "[1.9464204]\n",
      "[1.8006649]\n",
      "[-0.15298414]\n",
      "[-2.33603]\n",
      "[1.2392671]\n",
      "[2.3261433]\n",
      "[-1.8979328]\n",
      "[2.3681579]\n",
      "[1.6929843]\n",
      "[0.72836494]\n",
      "[0.35884595]\n",
      "[-0.26101065]\n",
      "[0.506969]\n",
      "[-1.3435369]\n",
      "[-2.1462617]\n",
      "[1.863664]\n",
      "[0.11509418]\n",
      "[1.8613324]\n",
      "[-1.6554387]\n",
      "[1.7966154]\n",
      "[0.7016175]\n",
      "[3.8547316]\n",
      "[-2.7505686]\n",
      "[1.0455732]\n",
      "[-0.2368269]\n",
      "[-2.2358868]\n",
      "[2.2844665]\n",
      "[0.5520203]\n",
      "[0.8509729]\n",
      "[0.36454773]\n",
      "[-1.0558782]\n",
      "[1.2678792]\n",
      "[3.2030554]\n",
      "[-1.0134084]\n",
      "[1.3002095]\n",
      "[0.6379237]\n",
      "[1.7020001]\n",
      "[1.7195342]\n",
      "[0.95548034]\n",
      "[0.77364206]\n",
      "[3.6912496]\n",
      "[1.469389]\n",
      "[0.72159886]\n",
      "[-1.2895563]\n",
      "[6.0688353]\n",
      "[4.583865]\n",
      "[0.74429893]\n",
      "[9.508026]\n",
      "[1.7491059]\n",
      "[5.013633]\n",
      "[1.0549498]\n",
      "[1.0603092]\n",
      "[1.1422622]\n",
      "[-0.5406747]\n",
      "[2.2582166]\n",
      "[2.9459305]\n",
      "[1.7960279]\n",
      "[-1.5138054]\n",
      "[-1.0794773]\n",
      "[2.198311]\n",
      "[1.1802745]\n",
      "[-2.493164]\n",
      "[0.8214314]\n",
      "[2.219564]\n",
      "[-1.2903459]\n",
      "[-0.4769678]\n",
      "[-1.1830719]\n",
      "[0.97754455]\n",
      "[0.5419369]\n",
      "[0.71218204]\n",
      "[2.295106]\n",
      "[-1.228955]\n",
      "[0.19185448]\n",
      "[0.876631]\n",
      "[0.25991726]\n",
      "[-0.69525886]\n",
      "[1.9288526]\n",
      "[0.5315256]\n",
      "[-0.14202547]\n",
      "[3.8373423]\n",
      "[0.80473685]\n",
      "[0.2880149]\n",
      "[-0.4901669]\n",
      "[-0.834579]\n",
      "[4.0134563]\n",
      "[1.3253376]\n",
      "[0.20727897]\n",
      "[0.3052199]\n",
      "[4.0901313]\n",
      "[0.70866966]\n",
      "[-2.2931402]\n",
      "[2.852166]\n",
      "[-0.9115658]\n",
      "[2.8911605]\n",
      "[2.9131587]\n",
      "[6.290957]\n",
      "[0.69959164]\n",
      "[0.45251513]\n",
      "[-3.7818348]\n",
      "[0.61315393]\n",
      "[2.7322836]\n",
      "[-0.9875035]\n",
      "[-1.0348613]\n",
      "[1.5229044]\n",
      "[0.79301596]\n",
      "[-0.39502907]\n",
      "[0.37510824]\n",
      "[1.1470454]\n",
      "[3.4761755]\n",
      "[0.9775758]\n",
      "[0.646467]\n",
      "[-0.30155802]\n",
      "[0.9441538]\n",
      "[0.24098277]\n",
      "[-0.97023916]\n",
      "[1.5232182]\n",
      "[0.42325807]\n",
      "[2.8549461]\n",
      "[0.98889256]\n",
      "[-0.70958877]\n",
      "[1.894424]\n",
      "[-0.82438636]\n",
      "[-0.18216753]\n",
      "[4.7228203]\n",
      "[2.4082532]\n",
      "[2.3042698]\n",
      "[-0.18696737]\n",
      "[1.1556151]\n",
      "[0.97771215]\n",
      "[-2.44791]\n",
      "[0.77530384]\n",
      "[2.347473]\n",
      "[-0.71921134]\n",
      "[-1.1984715]\n",
      "[-1.6441984]\n",
      "[-1.1190913]\n",
      "[-0.09458208]\n",
      "[-9.7358]\n",
      "[-1.4554849]\n",
      "[2.2937248]\n",
      "[2.4691737]\n",
      "[3.3075905]\n",
      "[0.41157007]\n",
      "[-0.09331322]\n",
      "[1.4174109]\n",
      "[0.6445348]\n",
      "[-1.0381138]\n",
      "[0.92925]\n",
      "[6.4996605]\n",
      "[-0.9698365]\n",
      "[1.582231]\n",
      "[0.00276828]\n",
      "[0.0179882]\n",
      "[0.6081147]\n",
      "[1.2244415]\n",
      "[3.0066488]\n",
      "[-0.1761589]\n",
      "[0.9309585]\n",
      "[6.9110723]\n",
      "[0.5399866]\n",
      "[0.48927426]\n",
      "[-1.1356444]\n",
      "[0.46261358]\n",
      "[2.0798259]\n",
      "[0.08623338]\n",
      "[0.43003178]\n",
      "[-2.8163688]\n",
      "[-0.2572837]\n",
      "[-1.1796353]\n",
      "[-0.02852464]\n",
      "[-1.5145533]\n",
      "[1.7095003]\n",
      "[1.8514595]\n",
      "[-0.3258567]\n",
      "[2.260752]\n",
      "[-2.944117]\n",
      "[1.1230118]\n",
      "[2.5925198]\n",
      "[4.4715414]\n",
      "[2.1312602]\n",
      "[-0.12726593]\n",
      "[0.21424079]\n",
      "[-0.3777418]\n",
      "[1.17436]\n",
      "[1.3280473]\n",
      "[0.17846704]\n",
      "[2.4753876]\n",
      "[-1.1555319]\n",
      "[-1.8009012]\n",
      "[-2.5043406]\n",
      "[-0.38196206]\n",
      "[1.275629]\n",
      "[0.875273]\n",
      "[0.73404694]\n",
      "[2.3243847]\n",
      "[-0.08446336]\n",
      "[-0.9974382]\n",
      "[-7.193064]\n",
      "[-0.502265]\n",
      "[-0.30876064]\n",
      "[0.12732673]\n",
      "[0.40747833]\n",
      "[0.7846043]\n",
      "[0.9045129]\n",
      "[1.8186989]\n",
      "[-1.3472066]\n",
      "[0.5610895]\n",
      "[0.56519437]\n",
      "[-0.67211246]\n",
      "[1.18026]\n",
      "[13.465616]\n",
      "[-2.4667232]\n",
      "[-6.5866127]\n",
      "[0.35027814]\n",
      "[-3.74141]\n",
      "[0.2575965]\n",
      "[-0.00823045]\n",
      "[-9.388695]\n",
      "[0.9905958]\n",
      "[2.0110042]\n",
      "[-0.23727775]\n",
      "[1.8416944]\n",
      "[1.3032699]\n",
      "[11.099317]\n",
      "[-0.01619434]\n",
      "[2.6918]\n",
      "[0.57829785]\n",
      "[0.54869056]\n",
      "[0.7280102]\n",
      "[3.1859703]\n",
      "[1.8476021]\n",
      "[-0.5187292]\n",
      "[2.787695]\n",
      "[0.92848086]\n",
      "[0.72666883]\n",
      "[-0.34439445]\n",
      "[-3.6087012]\n",
      "[-0.41879916]\n",
      "[2.7187388]\n",
      "[-0.8224318]\n",
      "[-0.97130656]\n",
      "[1.0601907]\n",
      "[-0.21549726]\n",
      "[-1.7606668]\n",
      "[2.3052864]\n",
      "[-1.9301188]\n",
      "[8.7248745]\n",
      "[2.3722546]\n",
      "[0.9815676]\n",
      "[0.13747287]\n",
      "[3.414843]\n",
      "[-2.057469]\n",
      "[-0.19786596]\n",
      "[0.8564174]\n",
      "[-2.9363708]\n",
      "[1.1264617]\n",
      "[-0.46141028]\n",
      "[0.9801314]\n",
      "[2.0226822]\n",
      "[1.262214]\n",
      "[4.7739]\n",
      "[-5.248598]\n",
      "[2.431466]\n",
      "[3.7603474]\n",
      "[2.6311293]\n",
      "[10.221536]\n",
      "[-3.0835109]\n",
      "[1.5048735]\n",
      "[8.456038]\n",
      "[2.2820284]\n",
      "[2.6345718]\n",
      "[0.5352621]\n",
      "[1.4920244]\n",
      "[13.923117]\n",
      "[1.265646]\n",
      "[-1.4621263]\n",
      "[0.65321517]\n",
      "[1.3146825]\n",
      "[-0.18594193]\n",
      "[-0.95316434]\n",
      "[1.9811721]\n",
      "[-1.7992835]\n",
      "[0.9000914]\n",
      "[1.0519543]\n",
      "[0.47898412]\n",
      "[1.3353572]\n",
      "[1.6911986]\n",
      "[-1.2813513]\n",
      "[0.9430957]\n",
      "[-2.4016547]\n",
      "[0.39393115]\n",
      "[-0.83443856]\n",
      "[-1.2547083]\n",
      "[2.2509196]\n",
      "[-0.41141868]\n",
      "[0.72502995]\n",
      "[-4.59129]\n",
      "[-1.0662863]\n",
      "[0.7260916]\n",
      "[-0.45546174]\n",
      "[-1.3901925]\n",
      "[-0.9992678]\n",
      "[-1.1375861]\n",
      "[2.8121214]\n",
      "[-1.1190481]\n",
      "[3.701608]\n",
      "[-6.269452]\n",
      "[1.7574275]\n",
      "[-0.30533552]\n",
      "[-0.5973172]\n",
      "[-0.77861214]\n",
      "[-1.0073562]\n",
      "[-0.27122402]\n",
      "[-2.656574]\n",
      "[1.1578054]\n",
      "[-4.25342]\n",
      "[0.08324671]\n",
      "[0.18853974]\n",
      "[-0.51372623]\n",
      "[2.1867783]\n",
      "[-0.0799005]\n",
      "[-0.143893]\n",
      "[-0.2329278]\n",
      "[0.16874695]\n",
      "[-0.00840235]\n",
      "[-1.629394]\n",
      "[0.6354532]\n",
      "[1.0171702]\n",
      "[-4.0658374]\n",
      "[0.17017388]\n",
      "[1.3584051]\n",
      "[-0.15409017]\n",
      "[1.5401559]\n",
      "[0.69473624]\n",
      "[1.116116]\n",
      "[0.48725963]\n",
      "[-0.4184227]\n",
      "[0.13209105]\n",
      "[-0.52629566]\n",
      "[-0.21996474]\n",
      "[-0.29772115]\n",
      "[-0.55998254]\n",
      "[0.7770107]\n",
      "[-0.33525753]\n",
      "[0.3448844]\n",
      "[0.33497715]\n",
      "[0.04465938]\n",
      "[-2.3755586]\n",
      "[0.5710285]\n",
      "[-0.6617925]\n",
      "[0.9441221]\n",
      "[1.6004183]\n",
      "[-2.8206108]\n",
      "[-0.38645983]\n",
      "[-0.3255601]\n",
      "[3.8882303]\n",
      "[-2.5085013]\n",
      "[3.258254]\n",
      "[1.7388543]\n",
      "[0.3084283]\n",
      "[0.78486013]\n",
      "[0.7163937]\n",
      "[-0.8374772]\n",
      "[-0.1218698]\n",
      "[-1.2377653]\n",
      "[0.3621745]\n",
      "[3.0828316]\n",
      "[0.5462568]\n",
      "[1.0036497]\n",
      "[1.3762038]\n",
      "[-0.7512984]\n",
      "[-2.258282]\n",
      "[-3.1276624]\n",
      "[3.058616]\n",
      "[1.453053]\n",
      "[-0.79631996]\n",
      "[0.4837432]\n",
      "[-0.58002925]\n",
      "[1.2639396]\n",
      "[0.6925049]\n",
      "[1.8712149]\n",
      "[0.798363]\n",
      "[1.7954509]\n",
      "[-0.34552097]\n",
      "[2.572913]\n",
      "[-0.15450692]\n",
      "[1.3540187]\n",
      "[0.6247933]\n",
      "[-0.25410414]\n",
      "[-1.1358306]\n",
      "[0.07598972]\n",
      "[-0.6205759]\n",
      "[-0.43315125]\n",
      "[-0.32825923]\n",
      "[-1.2053974]\n",
      "[-1.115768]\n",
      "[0.09125376]\n",
      "[-0.27384543]\n",
      "[-0.06554031]\n",
      "[-0.5642791]\n",
      "[-1.0998359]\n",
      "[0.36481857]\n",
      "[-0.8496134]\n",
      "[-6.1063013]\n",
      "[-0.7243397]\n",
      "[-0.7776792]\n",
      "[-2.867471]\n",
      "[-0.27120328]\n",
      "[-0.0497601]\n",
      "[0.21601534]\n",
      "[3.329532]\n",
      "[0.5444343]\n",
      "[0.26752448]\n",
      "[0.40560842]\n",
      "[1.5021544]\n",
      "[0.11509037]\n",
      "[1.1422727]\n",
      "[-0.1393075]\n",
      "[0.62952447]\n",
      "[-0.46589613]\n",
      "[0.12806487]\n",
      "[-0.14882994]\n",
      "[-1.0223684]\n",
      "[2.7522516]\n",
      "[-0.467031]\n",
      "[-0.2871325]\n",
      "[-0.4584105]\n",
      "[-2.2941918]\n",
      "[0.8701763]\n",
      "[0.24423552]\n",
      "[2.3802676]\n",
      "[0.0569551]\n",
      "[3.7349021]\n",
      "[0.8722842]\n",
      "[0.26389933]\n",
      "[-0.6123402]\n",
      "[-2.9888906]\n",
      "[1.6678166]\n",
      "[0.15947771]\n",
      "[0.5640509]\n",
      "[1.5875351]\n",
      "[1.5334957]\n",
      "[-3.301605]\n",
      "[0.93293524]\n",
      "[0.16219282]\n",
      "[-1.166048]\n",
      "[0.33979082]\n",
      "[-0.9679694]\n",
      "[0.8918128]\n",
      "[5.166138]\n",
      "[-0.9044297]\n",
      "[0.00207663]\n",
      "[-4.194417]\n",
      "[3.0531418]\n",
      "[1.3256378]\n",
      "[2.5495682]\n",
      "[1.3932385]\n",
      "[-4.005104]\n",
      "[0.72711706]\n",
      "[0.932153]\n",
      "[-0.10829616]\n",
      "[-1.6417022]\n",
      "[4.8855524]\n",
      "[0.30536532]\n",
      "[3.9690423]\n",
      "[5.4163613]\n",
      "[3.3777535]\n",
      "[0.38165426]\n",
      "[4.0418563]\n",
      "[-0.2431035]\n",
      "[-2.5300834]\n",
      "[-0.9845362]\n",
      "[2.6130056]\n",
      "[-3.1860387]\n",
      "[-1.0846617]\n",
      "[-3.74217]\n",
      "[0.12249517]\n",
      "[0.09800839]\n",
      "[0.6268902]\n",
      "[-1.1504884]\n",
      "[0.07462621]\n",
      "[-0.05250835]\n",
      "[1.73768]\n",
      "[1.0344901]\n",
      "[-0.5927844]\n",
      "[0.09891486]\n",
      "[6.933195]\n",
      "[-0.23576784]\n",
      "[0.26363897]\n",
      "[-1.3068273]\n",
      "[-0.4052956]\n",
      "[1.325824]\n",
      "[-2.6502087]\n",
      "[2.395187]\n",
      "[0.55255413]\n",
      "[0.4596064]\n",
      "[1.6540112]\n",
      "[0.26649594]\n",
      "[3.2957115]\n",
      "[0.24908829]\n",
      "[-1.1000006]\n",
      "[-0.8704021]\n",
      "[-1.3350029]\n",
      "[-0.49656415]\n",
      "[0.5670526]\n",
      "[1.0567873]\n",
      "[-0.87506723]\n",
      "[-0.38630772]\n",
      "[-0.00413418]\n",
      "[0.47508454]\n",
      "[-1.5193934]\n",
      "[2.2758667]\n",
      "[-1.1798568]\n",
      "[0.985883]\n",
      "[0.34106112]\n",
      "[1.1312387]\n",
      "[3.9215763]\n",
      "[-0.9332783]\n",
      "[-8.309287]\n",
      "[1.0289068]\n",
      "[-2.972091]\n",
      "[-0.4721346]\n",
      "[-0.7798159]\n",
      "[-0.49749517]\n",
      "[-0.44982696]\n",
      "[1.5665469]\n",
      "[0.20926285]\n",
      "[-0.29661894]\n",
      "[-2.8331146]\n",
      "[0.38895845]\n",
      "[0.28355646]\n",
      "[0.34090614]\n",
      "[0.75171494]\n",
      "[-2.9455178]\n",
      "[-0.6020882]\n",
      "[-1.3212574]\n",
      "[1.7314346]\n",
      "[-1.2549164]\n",
      "[-2.0196538]\n",
      "[-0.6734636]\n",
      "[0.6038704]\n",
      "[-3.8026044]\n",
      "[-0.33463478]\n",
      "[1.5612965]\n",
      "[1.8427558]\n",
      "[-0.32708764]\n",
      "[1.0510116]\n",
      "[0.16912508]\n",
      "[-3.8916657]\n",
      "[1.0514971]\n",
      "[-1.3575299]\n",
      "[0.26419187]\n",
      "[1.4477133]\n",
      "[0.16322088]\n",
      "[0.10747433]\n",
      "[-0.4487996]\n",
      "[0.5242593]\n",
      "[0.3630786]\n",
      "[1.1708918]\n",
      "[-1.502799]\n",
      "[0.8010225]\n",
      "[-0.47626853]\n",
      "[-2.1263134]\n",
      "[0.5175133]\n",
      "[-1.3000677]\n",
      "[-0.28502464]\n",
      "[0.3045392]\n",
      "[0.4334228]\n",
      "[0.68695617]\n",
      "[-2.8270113]\n",
      "[0.96218705]\n",
      "[1.0581322]\n",
      "[0.18524075]\n",
      "[7.1154184]\n",
      "[0.027426]\n",
      "[2.494887]\n",
      "[0.9702747]\n",
      "[-2.6741374]\n",
      "[1.5879107]\n",
      "[1.9225945]\n",
      "[3.4985619]\n",
      "[-0.33302236]\n",
      "[1.2681139]\n",
      "[0.25182796]\n",
      "[0.65215015]\n",
      "[-0.3918352]\n",
      "[0.22452259]\n",
      "[-1.0694804]\n",
      "[3.4988728]\n",
      "[0.56582165]\n",
      "[3.1688952]\n",
      "[-6.9128094]\n",
      "[-1.056893]\n",
      "[2.435562]\n",
      "[-0.5284226]\n",
      "[-1.1959937]\n",
      "[-0.3393786]\n",
      "[-1.9542327]\n",
      "[-3.3226938]\n",
      "[1.686914]\n",
      "[0.8031204]\n",
      "[-0.14399481]\n",
      "[0.37430978]\n",
      "[1.8537617]\n",
      "[0.6933224]\n",
      "[-1.3954701]\n",
      "[1.4750221]\n",
      "[0.5106597]\n",
      "[0.15917778]\n",
      "[-2.1272914]\n",
      "[-1.3916433]\n",
      "[-0.5417328]\n",
      "[1.5009303]\n",
      "[-0.23086238]\n",
      "[3.2891061]\n",
      "[2.7904482]\n",
      "[-0.38512206]\n",
      "[3.2966185]\n",
      "[0.05194855]\n",
      "[0.55927014]\n",
      "[0.2008779]\n",
      "[0.8302529]\n",
      "[0.5918584]\n",
      "[0.25102973]\n",
      "[-0.36403275]\n",
      "[-0.00778794]\n",
      "[0.8317754]\n",
      "[-1.488956]\n",
      "[-1.9352393]\n",
      "[-0.8257456]\n",
      "[-0.21138954]\n",
      "[2.874474]\n",
      "[0.75759196]\n",
      "[-0.5962012]\n",
      "[-0.3263862]\n",
      "[0.73250914]\n",
      "[1.9447472]\n",
      "[-0.5683968]\n",
      "[0.5459709]\n",
      "[-0.7500577]\n",
      "[-0.47685695]\n",
      "[3.1835659]\n",
      "[1.851625]\n",
      "[1.5864606]\n",
      "[0.12343836]\n",
      "[-0.50575805]\n",
      "[1.4442749]\n",
      "[-0.18482065]\n",
      "[-0.30442595]\n",
      "[0.10396051]\n",
      "[1.7561145]\n",
      "[-0.18861794]\n",
      "[-0.02389145]\n",
      "[-0.6393137]\n",
      "[1.5361943]\n",
      "[-0.30776048]\n",
      "[5.4961543]\n",
      "[0.58926725]\n",
      "[2.6436377]\n",
      "[0.7344017]\n",
      "[0.7579894]\n",
      "[1.4850583]\n",
      "[0.10696721]\n",
      "[2.8710713]\n",
      "[-0.5598557]\n",
      "[-0.91631675]\n",
      "[0.62727]\n",
      "[3.8265834]\n",
      "[-2.3142788]\n",
      "[-0.91742015]\n",
      "[1.3118851]\n",
      "[0.9782598]\n",
      "[-0.18931222]\n",
      "[-2.1558285]\n",
      "[0.52857566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5621705]\n",
      "[0.1444807]\n",
      "[-0.08885193]\n",
      "[-0.90181756]\n",
      "[0.40858936]\n",
      "[-0.0588119]\n",
      "[0.9000032]\n",
      "[0.3929901]\n",
      "[2.1083846]\n",
      "[0.15536237]\n",
      "[-0.13328815]\n",
      "[-2.7231357]\n",
      "[0.57939744]\n",
      "[1.1882522]\n",
      "[0.21703649]\n",
      "[1.2945673]\n",
      "[-0.05131364]\n",
      "[0.19786286]\n",
      "[-1.4715712]\n",
      "[-0.78230095]\n",
      "[3.1786237]\n",
      "[0.12220287]\n",
      "[-2.6648645]\n",
      "[-5.231811]\n",
      "[0.02941251]\n",
      "[0.5215843]\n",
      "[1.8426752]\n",
      "[0.9650328]\n",
      "[0.9555521]\n",
      "[0.35329223]\n",
      "[-5.173972]\n",
      "[2.0723016]\n",
      "[-0.25925946]\n",
      "[5.801266]\n",
      "[-0.8969815]\n",
      "[0.29925394]\n",
      "[1.7506864]\n",
      "[0.12661457]\n",
      "[-0.25194216]\n",
      "[-0.7263763]\n",
      "[0.4575734]\n",
      "[1.4874682]\n",
      "[1.0444353]\n",
      "[-2.4178755]\n",
      "[-3.545973]\n",
      "[-1.1340258]\n",
      "[-0.05240011]\n",
      "[-0.30144405]\n",
      "[-2.8630388]\n",
      "[-0.5351536]\n",
      "[1.0819898]\n",
      "[-0.09566784]\n",
      "[-2.8047605]\n",
      "[-5.5947385]\n",
      "[-2.1287677]\n",
      "[0.47428346]\n",
      "[-0.35837483]\n",
      "[-0.3033471]\n",
      "[-1.2538354]\n",
      "[0.84073424]\n",
      "[-1.84028]\n",
      "[0.19482112]\n",
      "[0.12602687]\n",
      "[0.4965732]\n",
      "[-2.7032416]\n",
      "[1.8896377]\n",
      "[1.6534238]\n",
      "[0.47615004]\n",
      "[-1.9502704]\n",
      "[0.793272]\n",
      "[-0.14410734]\n",
      "[-0.15169668]\n",
      "[1.6008791]\n",
      "[-0.25135446]\n",
      "[-0.45233154]\n",
      "[-1.0247791]\n",
      "[2.6742067]\n",
      "[-8.049738]\n",
      "[0.52452207]\n",
      "[-0.9874315]\n",
      "[1.1897545]\n",
      "[3.3388286]\n",
      "[-0.65892506]\n",
      "[3.147356]\n",
      "[-0.32674146]\n",
      "[-0.503165]\n",
      "[2.1684952]\n",
      "[-0.8982043]\n",
      "[-0.78999615]\n",
      "[0.3370192]\n",
      "[-0.389081]\n",
      "[0.29903674]\n",
      "[3.0808668]\n",
      "[0.03344488]\n",
      "[-1.3969498]\n",
      "[-0.42566776]\n",
      "[-2.8166707]\n",
      "[0.1463337]\n",
      "[-3.3573768]\n",
      "[1.0577567]\n",
      "[-0.48416734]\n",
      "[-4.375508]\n",
      "[-0.69512606]\n",
      "[0.6939814]\n",
      "[-0.00404549]\n",
      "[-5.1082945]\n",
      "[-0.25021267]\n",
      "[1.8818569]\n",
      "[-0.04833817]\n",
      "[-0.5497713]\n",
      "[-0.365]\n",
      "[-0.00075698]\n",
      "[-0.87745833]\n",
      "[-0.15847373]\n",
      "[-1.1836107]\n",
      "[-1.6008117]\n",
      "[-0.3166759]\n",
      "[0.31159568]\n",
      "[-1.0415244]\n",
      "[-0.6383507]\n",
      "[-0.11354876]\n",
      "[1.3777529]\n",
      "[3.1572123]\n",
      "[-0.039042]\n",
      "[2.2689726]\n",
      "[0.8558545]\n",
      "[0.6550927]\n",
      "[-4.1367164]\n",
      "[1.0461202]\n",
      "[1.3500984]\n",
      "[0.9570358]\n",
      "[-1.7621593]\n",
      "[-0.23941708]\n",
      "[-0.7352226]\n",
      "[0.67062974]\n",
      "[1.5491414]\n",
      "[-0.42131853]\n",
      "[-4.0293837]\n",
      "[0.36579227]\n",
      "[-1.3534415]\n",
      "[-2.3012357]\n",
      "[-0.00695443]\n",
      "[-0.5181825]\n",
      "[-1.5518475]\n",
      "[0.38510847]\n",
      "[1.4198351]\n",
      "[-1.2750299]\n",
      "[1.4994605]\n",
      "[0.3577268]\n",
      "[-1.8144932]\n",
      "[-1.1462305]\n",
      "[-1.9689105]\n",
      "[-1.2034225]\n",
      "[-0.7639885]\n",
      "[-0.6772537]\n",
      "[-1.5961807]\n",
      "[-2.1233137]\n",
      "[-1.0785022]\n",
      "[-0.23613858]\n",
      "[-2.1176136]\n",
      "[1.9286847]\n",
      "[-2.4017737]\n",
      "[-0.8001666]\n",
      "[-2.6690989]\n",
      "[1.0549715]\n",
      "[-1.073379]\n",
      "[0.08819437]\n",
      "[-8.794806]\n",
      "[0.52318764]\n",
      "[-0.68285537]\n",
      "[-1.2362723]\n",
      "[2.1962023]\n",
      "[-1.0960383]\n",
      "[0.41690207]\n",
      "[0.07781076]\n",
      "[2.6013153]\n",
      "[-2.4896998]\n",
      "[2.7784455]\n",
      "[-2.2888048]\n",
      "[-0.8513417]\n",
      "[-1.6462743]\n",
      "[3.1290698]\n",
      "[-1.2967672]\n",
      "[-0.43127632]\n",
      "[-0.5229528]\n",
      "[0.70191026]\n",
      "[-0.369725]\n",
      "[-0.1897769]\n",
      "[3.5194283]\n",
      "[-5.731695]\n",
      "[-1.221828]\n",
      "[-0.15417528]\n",
      "[-1.1152322]\n",
      "[-0.4722104]\n",
      "[-3.3365247]\n",
      "[1.7187018]\n",
      "[-1.2421391]\n",
      "[0.49212146]\n",
      "[-0.12663364]\n",
      "[0.5541463]\n",
      "[-2.5786197]\n",
      "[-0.11829877]\n",
      "[-0.45589805]\n",
      "[0.7153883]\n",
      "[1.0179634]\n",
      "[0.19159222]\n",
      "[1.9040318]\n",
      "[-3.3847263]\n",
      "[-1.0915074]\n",
      "[0.24788547]\n",
      "[0.422292]\n",
      "[-3.3359926]\n",
      "[-3.6830485]\n",
      "[-0.47182775]\n",
      "[-1.719523]\n",
      "[0.20421171]\n",
      "[-3.7993808]\n",
      "[1.5944564]\n",
      "[1.4620967]\n",
      "[-0.5690961]\n",
      "[1.3502916]\n",
      "[-0.7111621]\n",
      "[0.34114742]\n",
      "[-0.15997791]\n",
      "[-0.21206951]\n",
      "[-1.1002612]\n",
      "[0.6936021]\n",
      "[-0.16421294]\n",
      "[0.1966424]\n",
      "[1.1609521]\n",
      "[-1.0145093]\n",
      "[-0.30647087]\n",
      "[1.3608229]\n",
      "[1.5003622]\n",
      "[-2.0019531]\n",
      "[-0.17367887]\n",
      "[0.8282697]\n",
      "[-3.9647648]\n",
      "[-4.704633]\n",
      "[-0.44256997]\n",
      "[0.88526106]\n",
      "[4.384344]\n",
      "[0.9550328]\n",
      "[2.1852276]\n",
      "[1.0546501]\n",
      "[-1.773426]\n",
      "[-0.40420294]\n",
      "[-0.0117352]\n",
      "[0.40133452]\n",
      "[-1.836219]\n",
      "[-0.02574921]\n",
      "[0.12926173]\n",
      "[-0.297225]\n",
      "[0.24133492]\n",
      "[-0.6901469]\n",
      "[0.73322344]\n",
      "[0.33253312]\n",
      "[-1.0612242]\n",
      "[-0.84391]\n",
      "[-0.58771276]\n",
      "[-1.4012113]\n",
      "[-1.6140382]\n",
      "[-1.5684041]\n",
      "[0.3956051]\n",
      "[1.3195689]\n",
      "[-1.2093527]\n",
      "[-0.09410357]\n",
      "[-1.4199355]\n",
      "[4.3162565]\n",
      "[0.9230404]\n",
      "[-1.8433609]\n",
      "[0.18580103]\n",
      "[0.56714463]\n",
      "[-2.0501072]\n",
      "[2.396964]\n",
      "[-0.04791307]\n",
      "[-2.500224]\n",
      "[-2.3444147]\n",
      "[0.4026034]\n",
      "[-0.36973882]\n",
      "[1.7316742]\n",
      "[-0.52611303]\n",
      "[0.07201529]\n",
      "[-0.52055645]\n",
      "[-2.6399364]\n",
      "[-0.27998495]\n",
      "[-0.18044567]\n",
      "[-3.5394223]\n",
      "[1.3201699]\n",
      "[1.4619243]\n",
      "[0.05320072]\n",
      "[2.9243464]\n",
      "[1.660959]\n",
      "[-1.1224227]\n",
      "[0.4716606]\n",
      "[0.816921]\n",
      "[2.3234146]\n",
      "[-1.7115595]\n",
      "[-1.0270872]\n",
      "[0.5258882]\n",
      "[-1.2533405]\n",
      "[-0.68955564]\n",
      "[1.3482335]\n",
      "[0.37056518]\n",
      "[-0.62040424]\n",
      "[-0.60952973]\n",
      "[-1.3471746]\n",
      "[-1.2997041]\n",
      "[0.13209558]\n",
      "[0.847785]\n",
      "[0.01883674]\n",
      "[-0.21090293]\n",
      "[-0.47186446]\n",
      "[-0.83595586]\n",
      "[0.45248556]\n",
      "[-0.15591598]\n",
      "[0.9962208]\n",
      "[-1.4537342]\n",
      "[-1.3931758]\n",
      "[-1.4726954]\n",
      "[-2.2494168]\n",
      "[0.17705178]\n",
      "[-1.4684956]\n",
      "[1.0039191]\n",
      "[1.4872525]\n",
      "[-3.126579]\n",
      "[-4.69015]\n",
      "[-3.0245037]\n",
      "[-0.50096726]\n",
      "[0.6237571]\n",
      "[4.6157966]\n",
      "[0.34074378]\n",
      "[0.44593453]\n",
      "[1.3411031]\n",
      "[-0.19211483]\n",
      "[-1.4184403]\n",
      "[-2.19717]\n",
      "[-0.5006902]\n",
      "[-0.0102005]\n",
      "[2.2560167]\n",
      "[-2.241312]\n",
      "[-0.60116005]\n",
      "[0.19938135]\n",
      "[-0.33646393]\n",
      "[-0.05972576]\n",
      "[3.3777225]\n",
      "[0.76249266]\n",
      "[-0.25698686]\n",
      "[-1.128984]\n",
      "[-1.9401121]\n",
      "[-1.4433966]\n",
      "[-1.8588564]\n",
      "[1.9520309]\n",
      "[-1.5424993]\n",
      "[-0.8539083]\n",
      "[-1.5474184]\n",
      "[0.1703341]\n",
      "[0.09777451]\n",
      "[-1.4933097]\n",
      "[-0.10488224]\n",
      "[-0.64745593]\n",
      "[-2.6274855]\n",
      "[0.4953599]\n",
      "[-0.40136766]\n",
      "[-2.2578902]\n",
      "[-0.01143074]\n",
      "[-1.1327295]\n",
      "[-0.6618614]\n",
      "[-1.5183713]\n",
      "[2.1051135]\n",
      "[-0.75400496]\n",
      "[5.8968315]\n",
      "[10.771331]\n",
      "[0.72590137]\n",
      "[-0.5634055]\n",
      "[-0.14302754]\n",
      "[-5.8066134]\n",
      "[-6.285313]\n",
      "[-0.07013702]\n",
      "[1.2991726]\n",
      "[-0.80194855]\n",
      "[-2.1782982]\n",
      "[-0.97378755]\n",
      "[1.4619894]\n",
      "[0.0773952]\n",
      "[-0.17302823]\n",
      "[-4.0417585]\n",
      "[-2.4344342]\n",
      "[-2.6769578]\n",
      "[-0.9217701]\n",
      "[-0.2607317]\n",
      "[-0.86061096]\n",
      "[0.495574]\n",
      "[-0.33417916]\n",
      "[1.0847673]\n",
      "[-1.170537]\n",
      "[-0.843982]\n",
      "[-1.3061068]\n",
      "[-1.849339]\n",
      "[-0.26775885]\n",
      "[-0.5678294]\n",
      "[0.9090674]\n",
      "[0.1265471]\n",
      "[0.6705214]\n",
      "[-0.68058515]\n",
      "[-0.87400866]\n",
      "[0.5306351]\n",
      "[1.0097046]\n",
      "[0.27562213]\n",
      "[0.25836325]\n",
      "[-2.3722699]\n",
      "[0.5555694]\n",
      "[-0.92354107]\n",
      "[0.36407924]\n",
      "[-0.93781996]\n",
      "[-4.2071023]\n",
      "[-1.0053875]\n",
      "[-0.69302297]\n",
      "[-0.59513426]\n",
      "[2.41895]\n",
      "[0.95862603]\n",
      "[-3.1999955]\n",
      "[0.80869365]\n",
      "[3.0726047]\n",
      "[-1.0573571]\n",
      "[0.8796787]\n",
      "[-0.7784171]\n",
      "[-0.21279645]\n",
      "[-2.7792706]\n",
      "[0.01377082]\n",
      "[1.0851915]\n",
      "[0.9549923]\n",
      "[-0.41546154]\n",
      "[-1.2046497]\n",
      "[-0.1295793]\n",
      "[0.869447]\n",
      "[4.3590317]\n",
      "[-0.5291395]\n",
      "[-1.5512238]\n",
      "[-1.3106544]\n",
      "[0.3902111]\n",
      "[-0.85362315]\n",
      "[1.0485747]\n",
      "[1.045964]\n",
      "[-1.3822463]\n",
      "[-0.5458634]\n",
      "[1.6469316]\n",
      "[-0.28442812]\n",
      "[-0.5267873]\n",
      "[-0.8078623]\n",
      "[-1.4227428]\n",
      "[-3.163117]\n",
      "[0.17643952]\n",
      "[-0.8243048]\n",
      "[-0.68961954]\n",
      "[1.336081]\n",
      "[-0.85257745]\n",
      "[-1.4869096]\n",
      "[-0.8731141]\n",
      "[2.3546863]\n",
      "[0.08563447]\n",
      "[-0.97475076]\n",
      "[1.8734875]\n",
      "[0.6446867]\n",
      "[-3.593425]\n",
      "[2.0625443]\n",
      "[2.3578603]\n",
      "[-2.0455744]\n",
      "[0.16032386]\n",
      "[1.7784953]\n",
      "[-0.69354534]\n",
      "[-2.1931784]\n",
      "[3.9008427]\n",
      "[0.30804324]\n",
      "[-0.2863767]\n",
      "[0.2425232]\n",
      "[0.10708976]\n",
      "[1.35339]\n",
      "[0.85233116]\n",
      "[0.40493584]\n",
      "[-0.19947958]\n",
      "[-1.0213859]\n",
      "[-0.6460633]\n",
      "[0.52525234]\n",
      "[-0.7200377]\n",
      "[-2.1085737]\n",
      "[-1.6534705]\n",
      "[-0.5491202]\n",
      "[0.25142813]\n",
      "[1.7009382]\n",
      "[3.7100496]\n",
      "[0.22249067]\n",
      "[0.6763978]\n",
      "[-0.7244642]\n",
      "[-5.1548624]\n",
      "[-0.3694358]\n",
      "[-3.7388556]\n",
      "[-1.7988808]\n",
      "[-0.41146612]\n",
      "[0.7596717]\n",
      "[-2.6082594]\n",
      "[-5.6188245]\n",
      "[-2.2334309]\n",
      "[-0.8414438]\n",
      "[-0.481184]\n",
      "[1.0651691]\n",
      "[-0.5534029]\n",
      "[0.9131887]\n",
      "[-0.17906308]\n",
      "[-0.80542326]\n",
      "[0.04454184]\n",
      "[0.01726365]\n",
      "[0.14943671]\n",
      "[0.00370002]\n",
      "[-0.63404274]\n",
      "[-1.780582]\n",
      "[0.6522126]\n",
      "[-1.2586787]\n",
      "[0.7175052]\n",
      "[-0.2779137]\n",
      "[-4.689316]\n",
      "[0.20443773]\n",
      "[-8.93456]\n",
      "[-0.31380725]\n",
      "[1.2146194]\n",
      "[-0.20532179]\n",
      "[-1.3782396]\n",
      "[0.16694474]\n",
      "[1.1837904]\n",
      "[0.47535753]\n",
      "[-2.5762396]\n",
      "[0.6982876]\n",
      "[-1.2559988]\n",
      "[2.5870502]\n",
      "[0.65492654]\n",
      "[-0.59270525]\n",
      "[2.3916924]\n",
      "[-0.97876143]\n",
      "[0.5555866]\n",
      "[-0.3976097]\n",
      "[-3.9553306]\n",
      "[2.3999398]\n",
      "[0.04581213]\n",
      "[-0.9367101]\n",
      "[1.3923192]\n",
      "[3.408482]\n",
      "[0.48784757]\n",
      "[-1.4872768]\n",
      "[-1.2209492]\n",
      "[9.430392]\n",
      "[-0.7542293]\n",
      "[-2.5023835]\n",
      "[-4.318204]\n",
      "[-1.301213]\n",
      "[-4.377065]\n",
      "[2.1009238]\n",
      "[0.02481079]\n",
      "[0.0542357]\n",
      "[0.4577036]\n",
      "[-0.17930841]\n",
      "[-0.87086296]\n",
      "[-0.89880323]\n",
      "[0.6960242]\n",
      "[1.9100227]\n",
      "[-1.8081579]\n",
      "[-0.28808665]\n",
      "[-1.6348841]\n",
      "[0.4281137]\n",
      "[-3.1377285]\n",
      "[-1.5269554]\n",
      "[0.41891313]\n",
      "[0.04317284]\n",
      "[5.356509]\n",
      "[-2.4218044]\n",
      "[0.01195478]\n",
      "[2.8298247]\n",
      "[-0.1572051]\n",
      "[-3.05957]\n",
      "[-0.6681249]\n",
      "[-1.5905128]\n",
      "[-2.2128198]\n",
      "[0.80157137]\n",
      "[-5.851141]\n",
      "[-1.6543989]\n",
      "[-0.73512363]\n",
      "[0.13512063]\n",
      "[-1.126672]\n",
      "[1.862488]\n",
      "[-5.6992674]\n",
      "[-1.4671847]\n",
      "[1.8872393]\n",
      "[3.8706386]\n",
      "[-0.18221092]\n",
      "[0.6208286]\n",
      "[-4.3521676]\n",
      "[-2.0148242]\n",
      "[-1.1506424]\n",
      "[0.9393928]\n",
      "[-0.02922082]\n",
      "[-1.4149923]\n",
      "[0.22543478]\n",
      "[-1.6949053]\n",
      "[0.32699418]\n",
      "[0.07781887]\n",
      "[0.56017447]\n",
      "[-1.5261219]\n",
      "[-1.1249952]\n",
      "[1.7313921]\n",
      "[0.5750053]\n",
      "[-0.07190156]\n",
      "[-1.2916732]\n",
      "[-0.30309844]\n",
      "[-2.1017654]\n",
      "[0.07220697]\n",
      "[-0.98950315]\n",
      "[0.5152378]\n",
      "[-0.79358435]\n",
      "[-1.4518776]\n",
      "[1.5685616]\n",
      "[0.7284]\n",
      "[2.1394515]\n",
      "[0.24540567]\n",
      "[-0.41259074]\n",
      "[-1.1234465]\n",
      "[1.470376]\n",
      "[0.5751207]\n",
      "[-1.8998301]\n",
      "[0.3939147]\n",
      "[-1.683944]\n",
      "[-1.2155325]\n",
      "[4.5088596]\n",
      "[0.44690824]\n",
      "[-2.0828574]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(train_dataset)\n",
    "for i in range(len(train_labels)):\n",
    "    print(train_labels[i]-predictions[i])\n",
    "#It does shit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30758007",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "191c759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very simple model, this is just from the TF tutorial\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3349e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 17:14:48.153909: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n",
      "2022-12-02 17:15:06.995527: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 17:15:06.996197: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 17:15:06.996230: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-12-02 17:15:06.996813: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-02 17:15:06.996877: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 35s 115ms/step - loss: 1.8759 - val_loss: 1.3473\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.1238 - val_loss: 0.9743\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.0241 - val_loss: 1.0253\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.9366 - val_loss: 1.0009\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.9197 - val_loss: 0.9164\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.8630 - val_loss: 0.9224\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.8142 - val_loss: 0.9000\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.7948 - val_loss: 0.8958\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.7981 - val_loss: 0.9261\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.8317 - val_loss: 0.8892\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.8085 - val_loss: 0.8841\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.7670 - val_loss: 0.8877\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.7317 - val_loss: 0.8798\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.7329 - val_loss: 0.8925\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.7695 - val_loss: 0.8986\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.7949 - val_loss: 0.8777\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.7523 - val_loss: 0.8761\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.7182 - val_loss: 0.8812\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6895 - val_loss: 0.8781\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.7149 - val_loss: 0.8834\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.7554 - val_loss: 0.8745\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.7530 - val_loss: 0.8747\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6991 - val_loss: 0.8624\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6734 - val_loss: 0.8633\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6975 - val_loss: 0.8694\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.7230 - val_loss: 0.8559\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6919 - val_loss: 0.8602\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6758 - val_loss: 0.8487\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6788 - val_loss: 0.8572\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6516 - val_loss: 0.8489\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6585 - val_loss: 0.8534\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6771 - val_loss: 0.8448\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6667 - val_loss: 0.8426\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6225 - val_loss: 0.8481\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6483 - val_loss: 0.8535\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6948 - val_loss: 0.8394\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6717 - val_loss: 0.8346\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.6483 - val_loss: 0.8326\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6375 - val_loss: 0.8279\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.6268 - val_loss: 0.8328\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6349 - val_loss: 0.8274\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.6524 - val_loss: 0.8306\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.6274 - val_loss: 0.8214\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.6073 - val_loss: 0.8177\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.6203 - val_loss: 0.8263\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6258 - val_loss: 0.8239\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.6064 - val_loss: 0.8162\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6068 - val_loss: 0.8248\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.5858 - val_loss: 0.8138\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.5982 - val_loss: 0.8302\n",
      "Distributed time:  45.35399055480957\n",
      "2/2 - 0s - loss: 0.8132 - 89ms/epoch - 45ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "history = model.fit(train_dataset, epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1d325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7359c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, layers, models, regularizers\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "layers.Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.001), input_shape=input_shape),\n",
    "layers.MaxPooling2D(),\n",
    "layers.Dropout(0.2),\n",
    "layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer = regularizers.l2(0.001)),\n",
    "layers.MaxPooling2D(),\n",
    "layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer = regularizers.l2(0.001)),\n",
    "layers.MaxPooling2D(),\n",
    "layers.Dropout(0.2),\n",
    "layers.Flatten(),\n",
    "layers.Dense(512, activation='relu', kernel_regularizer = regularizers.l2(0.001)),\n",
    "layers.Dense(2)])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b2875be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 3s 23ms/step - loss: 0.9632 - val_loss: 0.9910\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.8939 - val_loss: 0.9876\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.8773 - val_loss: 0.9375\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.8628 - val_loss: 1.0255\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.8110 - val_loss: 0.9692\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.8129 - val_loss: 0.9733\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7913 - val_loss: 0.9357\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.7803 - val_loss: 0.8934\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.7693 - val_loss: 0.8994\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7721 - val_loss: 0.8937\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7606 - val_loss: 0.9456\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7574 - val_loss: 0.9081\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7606 - val_loss: 0.8781\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7608 - val_loss: 0.9418\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7313 - val_loss: 0.9510\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7390 - val_loss: 0.8716\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7284 - val_loss: 0.8790\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7339 - val_loss: 0.8809\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7223 - val_loss: 0.8957\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7303 - val_loss: 0.8960\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.7358 - val_loss: 0.8667\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.7427 - val_loss: 0.8840\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7265 - val_loss: 0.8934\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7244 - val_loss: 0.8947\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7160 - val_loss: 0.8576\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7183 - val_loss: 0.8579\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7235 - val_loss: 0.8581\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7179 - val_loss: 0.8875\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7182 - val_loss: 0.8534\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7191 - val_loss: 0.9039\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7080 - val_loss: 0.8758\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7043 - val_loss: 0.8668\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6987 - val_loss: 0.8674\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7097 - val_loss: 0.8563\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7072 - val_loss: 0.8944\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7072 - val_loss: 0.8622\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7265 - val_loss: 0.8794\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7048 - val_loss: 0.8507\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7098 - val_loss: 0.8817\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7049 - val_loss: 0.8487\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7019 - val_loss: 0.9014\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7216 - val_loss: 0.8730\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7120 - val_loss: 0.8558\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7026 - val_loss: 0.8674\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7032 - val_loss: 0.8727\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7022 - val_loss: 0.8459\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7150 - val_loss: 0.9237\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6952 - val_loss: 0.8677\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7045 - val_loss: 0.8556\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6887 - val_loss: 0.8443\n",
      "Distributed time:  110.40149569511414\n",
      "2/2 - 0s - loss: 0.9211 - 63ms/epoch - 32ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "\n",
    "data_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=360, width_shift_range=4,\n",
    "    height_shift_range=4,zoom_range=0.3)\n",
    "\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_stamps, train_labels), epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a39570",
   "metadata": {},
   "source": [
    "## Ntampka Freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f0baddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides = 2))\n",
    "#model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(layers.MaxPooling2D((2, 2)))\n",
    "#model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ee5324b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 3s 24ms/step - loss: 2.3089 - val_loss: 1.0234\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 1.0567 - val_loss: 0.7018\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.9000 - val_loss: 0.9061\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.8078 - val_loss: 0.5924\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7604 - val_loss: 0.5261\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6698 - val_loss: 0.4978\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6945 - val_loss: 0.4486\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6680 - val_loss: 0.5177\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6235 - val_loss: 0.4497\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6653 - val_loss: 0.4496\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5704 - val_loss: 0.4773\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5829 - val_loss: 0.4205\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5767 - val_loss: 0.4038\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6035 - val_loss: 0.4058\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5569 - val_loss: 0.9171\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5809 - val_loss: 0.3948\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5858 - val_loss: 0.4305\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6211 - val_loss: 0.4474\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5401 - val_loss: 0.4150\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5497 - val_loss: 0.7649\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5395 - val_loss: 0.3954\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5400 - val_loss: 0.3605\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5253 - val_loss: 0.5257\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5456 - val_loss: 0.3627\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5485 - val_loss: 0.6208\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5761 - val_loss: 0.4366\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5505 - val_loss: 0.4131\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5436 - val_loss: 0.5725\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5524 - val_loss: 0.6313\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5163 - val_loss: 0.4059\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5593 - val_loss: 0.3811\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5356 - val_loss: 0.4813\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5167 - val_loss: 0.3799\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5006 - val_loss: 0.7792\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4893 - val_loss: 0.4502\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5017 - val_loss: 0.3851\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5501 - val_loss: 0.3925\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5258 - val_loss: 0.7473\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5062 - val_loss: 0.4323\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5034 - val_loss: 0.4128\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4998 - val_loss: 0.5239\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5054 - val_loss: 0.4974\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5049 - val_loss: 0.5777\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5496 - val_loss: 0.5068\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5318 - val_loss: 0.4449\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5062 - val_loss: 0.5073\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5126 - val_loss: 0.3716\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5081 - val_loss: 0.5295\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5521 - val_loss: 0.5952\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5058 - val_loss: 0.4682\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=360, width_shift_range=4,\n",
    "    height_shift_range=4,zoom_range=0.3)\n",
    "\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_stamps, train_labels), epochs=int(50), \n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4031b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_test = []\n",
    "for thing in test_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_test.append((truth.numpy()-pred)/pred)\n",
    "        \n",
    "errs_val = []\n",
    "for thing in val_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_val.append((truth.numpy()-pred)/pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fbe8d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13101828\n",
      "0.14401387\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(errs_test)))\n",
    "print(np.mean(np.abs(errs_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8070",
   "metadata": {},
   "source": [
    "# Compton y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efe7a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/project/r/rbond/jorlo/datasets/act_y_stamps/'\n",
    "\n",
    "with np.load(data_dir + 'all_clusters.npz') as data:\n",
    "    stamps = data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9a9968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ffc681757f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(stamps[50])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc6391e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuts out any maps that have nans in them\n",
    "flags = []\n",
    "for i in range(stamps.shape[0]):\n",
    "        if np.any(np.isnan(stamps[i,...])):\n",
    "                flags.append(i)\n",
    "\n",
    "stamps = np.delete(stamps, flags, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e10dc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 2\n",
    "\n",
    "stamps *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6771819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = stamps.shape[0]\n",
    "train_size = int(0.7 * tot)\n",
    "val_size = int(0.15 * tot)\n",
    "test_size = int(0.15 * tot)\n",
    "\n",
    "train_stamps = stamps[:train_size]\n",
    "val_stamps = stamps[train_size:train_size + val_size]\n",
    "test_stamps = stamps[train_size + val_size:]\n",
    "\n",
    "input_shape = train_stamps.shape[1:]\n",
    "\n",
    "act_catalog = fits.open('/gpfs/fs0/project/r/rbond/jorlo/cluster_catalogs/DR5_cluster-catalog_v1.0b2.fits')\n",
    "\n",
    "labels = act_catalog[1].data['M500Cal'][:tot]\n",
    "\n",
    "train_labels = labels[:train_size]\n",
    "val_labels = labels[train_size:train_size + val_size]\n",
    "test_labels = labels[train_size + val_size:]\n",
    "\n",
    "#This just sets the # of samples we include in a trianing epoch, which is called the batch size. Autotune is a bit\n",
    "#of magic that allows tf to dynamically set some hyperparameters in an optimal way. See https://www.tensorflow.org/guide/data_performance\n",
    "batch_size = 500\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_stamps, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_stamps, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_stamps, test_labels))\n",
    "\n",
    "#We shuffle our data (i.e. just mix up the order) and batch it\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "#Preloads data into memory\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "caf6beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79a3b133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 34ms/step - loss: 3.5184 - val_loss: 3.2655\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.2043 - val_loss: 2.9547\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.8732 - val_loss: 2.5695\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.4489 - val_loss: 2.0659\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.8822 - val_loss: 1.3886\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.1453 - val_loss: 0.6503\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5391 - val_loss: 0.6598\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6397 - val_loss: 0.7156\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5754 - val_loss: 0.5531\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.4914 - val_loss: 0.5192\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.4925 - val_loss: 0.5067\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4614 - val_loss: 0.4928\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4439 - val_loss: 0.4906\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4341 - val_loss: 0.4656\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4102 - val_loss: 0.4296\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3745 - val_loss: 0.3902\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3425 - val_loss: 0.3542\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.3092 - val_loss: 0.3158\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.2732 - val_loss: 0.2775\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.2373 - val_loss: 0.2384\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.2003 - val_loss: 0.2057\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1763 - val_loss: 0.1821\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1642 - val_loss: 0.1745\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1624 - val_loss: 0.1717\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1620 - val_loss: 0.1700\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1604 - val_loss: 0.1694\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1576 - val_loss: 0.1689\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1562 - val_loss: 0.1700\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1575 - val_loss: 0.1684\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1558 - val_loss: 0.1681\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1562 - val_loss: 0.1667\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1564 - val_loss: 0.1660\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1556 - val_loss: 0.1659\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1549 - val_loss: 0.1655\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1530 - val_loss: 0.1652\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1528 - val_loss: 0.1659\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1535 - val_loss: 0.1665\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1551 - val_loss: 0.1644\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1528 - val_loss: 0.1647\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1534 - val_loss: 0.1639\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1518 - val_loss: 0.1635\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1502 - val_loss: 0.1652\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1529 - val_loss: 0.1636\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1533 - val_loss: 0.1627\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1505 - val_loss: 0.1625\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1519 - val_loss: 0.1626\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.1511 - val_loss: 0.1621\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1495 - val_loss: 0.1620\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1494 - val_loss: 0.1618\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.1495 - val_loss: 0.1622\n",
      "Distributed time:  6.534914016723633\n",
      "2/2 - 0s - loss: 0.1887 - 46ms/epoch - 23ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "history = model.fit(train_dataset, epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25593fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_dataset)\n",
    "\n",
    "errs = []\n",
    "for i in range(len(train_labels)):\n",
    "    errs.append((train_labels[i]-predictions[i])/predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a122d097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3689581"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e50f8",
   "metadata": {},
   "source": [
    "# Compton-y CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480006e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/project/r/rbond/jorlo/datasets/act_y_stamps/'\n",
    "\n",
    "with np.load(data_dir + 'all_clusters.npz') as data:\n",
    "    stamps = data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6910cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuts out any maps that have nans in them\n",
    "flags = []\n",
    "for i in range(stamps.shape[0]):\n",
    "        if np.any(np.isnan(stamps[i,...])):\n",
    "                flags.append(i)\n",
    "\n",
    "stamps = np.delete(stamps, flags, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4c865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 2\n",
    "stamps *=factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf70abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps = np.expand_dims(stamps, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51815cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = stamps.shape[0]\n",
    "train_size = int(0.7 * tot)\n",
    "val_size = int(0.15 * tot)\n",
    "test_size = int(0.15 * tot)\n",
    "\n",
    "train_stamps = stamps[:train_size]\n",
    "val_stamps = stamps[train_size:train_size + val_size]\n",
    "test_stamps = stamps[train_size + val_size:]\n",
    "\n",
    "input_shape = train_stamps.shape[1:]\n",
    "\n",
    "act_catalog = fits.open('/gpfs/fs0/project/r/rbond/jorlo/cluster_catalogs/DR5_cluster-catalog_v1.0b2.fits')\n",
    "\n",
    "labels = act_catalog[1].data['M500Cal'][:tot]\n",
    "\n",
    "train_labels = labels[:train_size]\n",
    "val_labels = labels[train_size:train_size + val_size]\n",
    "test_labels = labels[train_size + val_size:]\n",
    "\n",
    "#This just sets the # of samples we include in a trianing epoch, which is called the batch size. Autotune is a bit\n",
    "#of magic that allows tf to dynamically set some hyperparameters in an optimal way. See https://www.tensorflow.org/guide/data_performance\n",
    "batch_size = 100\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_stamps, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_stamps, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_stamps, test_labels))\n",
    "\n",
    "#We shuffle our data (i.e. just mix up the order) and batch it\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "#Preloads data into memory\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d5a601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very simple model, this is just from the TF tutorial\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fcde651b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 38ms/step - loss: 8.7106 - val_loss: 0.4624\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.7310 - val_loss: 1.4153\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6642 - val_loss: 0.7713\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.8035 - val_loss: 0.4639\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3226 - val_loss: 0.5732\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4164 - val_loss: 0.3302\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2680 - val_loss: 0.3276\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2487 - val_loss: 0.2646\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2136 - val_loss: 0.2592\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1945 - val_loss: 0.2279\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1783 - val_loss: 0.2137\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1665 - val_loss: 0.2100\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1616 - val_loss: 0.1997\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1497 - val_loss: 0.1918\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1448 - val_loss: 0.1860\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1395 - val_loss: 0.1808\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1316 - val_loss: 0.1767\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1281 - val_loss: 0.1721\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1229 - val_loss: 0.1674\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1189 - val_loss: 0.1642\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1156 - val_loss: 0.1620\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1125 - val_loss: 0.1592\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1095 - val_loss: 0.1567\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1087 - val_loss: 0.1556\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1059 - val_loss: 0.1527\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1044 - val_loss: 0.1513\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1001 - val_loss: 0.1492\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0986 - val_loss: 0.1483\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0962 - val_loss: 0.1463\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0954 - val_loss: 0.1465\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0927 - val_loss: 0.1444\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0921 - val_loss: 0.1436\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0898 - val_loss: 0.1425\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0887 - val_loss: 0.1415\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0876 - val_loss: 0.1407\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0866 - val_loss: 0.1415\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0852 - val_loss: 0.1396\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0857 - val_loss: 0.1413\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0837 - val_loss: 0.1385\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0823 - val_loss: 0.1383\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0817 - val_loss: 0.1378\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0822 - val_loss: 0.1388\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0791 - val_loss: 0.1367\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0801 - val_loss: 0.1363\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0787 - val_loss: 0.1374\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0775 - val_loss: 0.1349\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0767 - val_loss: 0.1349\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0771 - val_loss: 0.1366\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0768 - val_loss: 0.1364\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0737 - val_loss: 0.1352\n",
      "Distributed time:  7.8156418800354\n",
      "2/2 - 0s - loss: 0.0879 - 48ms/epoch - 24ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "history = model.fit(train_dataset, epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "edf31cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = test_dataset\n",
    "\n",
    "predictions = model.predict(comp)\n",
    "\n",
    "\n",
    "\n",
    "errs = []\n",
    "for i in range(len(comp)):\n",
    "    errs.append((comp[i]-predictions[i])/predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5b113708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37266538"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0e0c8",
   "metadata": {},
   "source": [
    "## Ntampaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff0b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides = 2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b4c8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 16:07:26.044900: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n",
      "2022-12-05 16:07:26.454254: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-05 16:07:26.454842: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-05 16:07:26.454869: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-12-05 16:07:26.455432: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-12-05 16:07:26.455482: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 14ms/step - loss: 3.4565 - val_loss: 0.5492\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1796 - val_loss: 0.1955\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1294 - val_loss: 0.2067\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1147 - val_loss: 0.1681\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1076 - val_loss: 0.1605\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1046 - val_loss: 0.1524\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0940 - val_loss: 0.1579\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1003 - val_loss: 0.1390\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0978 - val_loss: 0.1576\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0848 - val_loss: 0.1326\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0929 - val_loss: 0.1407\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0858 - val_loss: 0.1314\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1005 - val_loss: 0.1382\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0778 - val_loss: 0.1389\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0821 - val_loss: 0.1300\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0718 - val_loss: 0.1323\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0647 - val_loss: 0.1310\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0650 - val_loss: 0.1349\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0650 - val_loss: 0.1260\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0612 - val_loss: 0.1241\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.1470\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0588 - val_loss: 0.1256\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0589 - val_loss: 0.1441\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0616 - val_loss: 0.1406\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.1333\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0656 - val_loss: 0.1227\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0566 - val_loss: 0.1239\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0527 - val_loss: 0.1298\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0585 - val_loss: 0.1285\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0579 - val_loss: 0.1294\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0561 - val_loss: 0.1207\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0647 - val_loss: 0.1664\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0611 - val_loss: 0.1373\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0514 - val_loss: 0.1276\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0539 - val_loss: 0.1240\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0513 - val_loss: 0.1197\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0545 - val_loss: 0.1297\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0517 - val_loss: 0.1242\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0509 - val_loss: 0.1214\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0508 - val_loss: 0.1239\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0645 - val_loss: 0.1394\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0671 - val_loss: 0.1386\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0634 - val_loss: 0.1204\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0523 - val_loss: 0.1192\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0493 - val_loss: 0.1220\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0520 - val_loss: 0.1189\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0473 - val_loss: 0.1235\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0472 - val_loss: 0.1207\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0461 - val_loss: 0.1279\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0721 - val_loss: 0.1839\n",
      "Distributed time:  13.764232397079468\n",
      "7/7 - 0s - loss: 0.2099 - 75ms/epoch - 11ms/step\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "#Actually fits the model to the data. data_augmentation.flow generates augmented data sets from the data sets we pass it.\n",
    "#Epochs sets the number of rounds of fitting to perform. \n",
    "history = model.fit(train_dataset, epochs=int(50), \n",
    "                    validation_data=val_dataset)\n",
    "tic = time.time()\n",
    "\n",
    "print('Distributed time: ', tic-toc)\n",
    "\n",
    "#Evaluate how well our model does\n",
    "test_loss = model.evaluate(test_dataset, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a364cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07291223\n",
      "0.063318625\n"
     ]
    }
   ],
   "source": [
    "errs_test = []\n",
    "for thing in test_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_test.append((truth.numpy()-pred)/pred)\n",
    "        \n",
    "errs_val = []\n",
    "for thing in val_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_val.append((truth.numpy()-pred)/pred)\n",
    "        \n",
    "print(np.mean(np.abs(errs_test)))\n",
    "print(np.mean(np.abs(errs_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ef902812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 2s 14ms/step - loss: 0.2221 - val_loss: 0.1426\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1434 - val_loss: 0.1479\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1315 - val_loss: 0.2901\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1538 - val_loss: 0.1509\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1415 - val_loss: 0.1995\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1329 - val_loss: 0.2254\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1122 - val_loss: 0.1455\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1472 - val_loss: 0.9802\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1450 - val_loss: 0.1447\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1157 - val_loss: 0.1344\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1308 - val_loss: 0.1444\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1280 - val_loss: 0.4403\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1079 - val_loss: 0.1314\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0910 - val_loss: 0.1420\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0737 - val_loss: 0.1355\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0843 - val_loss: 0.1389\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0738 - val_loss: 0.1363\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1025 - val_loss: 0.1275\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0926 - val_loss: 0.1884\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0887 - val_loss: 0.1390\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0956 - val_loss: 0.1348\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0899 - val_loss: 0.1271\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0859 - val_loss: 0.1326\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0687 - val_loss: 0.1518\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0897 - val_loss: 0.1272\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0949 - val_loss: 0.2231\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1100 - val_loss: 0.1588\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0886 - val_loss: 0.1325\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1434 - val_loss: 0.1424\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0845 - val_loss: 0.1959\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0934 - val_loss: 0.1290\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0837 - val_loss: 0.1372\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0671 - val_loss: 0.1282\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0662 - val_loss: 0.1330\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0801 - val_loss: 0.1301\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0969 - val_loss: 0.1430\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1028 - val_loss: 0.1394\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0868 - val_loss: 0.1449\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0803 - val_loss: 0.1280\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0773 - val_loss: 0.1221\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0765 - val_loss: 0.1370\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0864 - val_loss: 0.1520\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1105 - val_loss: 0.3361\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.1127 - val_loss: 0.1371\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0835 - val_loss: 0.1840\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0728 - val_loss: 0.1248\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0818 - val_loss: 0.1540\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0689 - val_loss: 0.1337\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0780 - val_loss: 0.1542\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0674 - val_loss: 0.1749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=360, width_shift_range=4,\n",
    "    height_shift_range=4,zoom_range=0.3)\n",
    "\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_stamps, train_labels), epochs=int(50), \n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4dcc0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_test = []\n",
    "for thing in test_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_test.append((truth.numpy()-pred)/pred)\n",
    "        \n",
    "errs_val = []\n",
    "for thing in val_dataset:\n",
    "    for i in range(thing[1].shape[0]):\n",
    "        truth = thing[1][i]\n",
    "        pred = model.predict(np.expand_dims(thing[0][i], axis = 0))\n",
    "        errs_val.append((truth.numpy()-pred)/pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ef877a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055931102\n",
      "0.06512552\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(errs_test)))\n",
    "print(np.mean(np.abs(errs_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258c4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-clusters",
   "language": "python",
   "name": "ml-clusters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

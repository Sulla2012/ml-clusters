{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ebf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9966190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82da8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERS\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "keep_prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5965ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'freq'\n",
    "\n",
    "if data_type == 'freq':\n",
    "    data_dir = '/project/r/rbond/jorlo/datasets/act_freq_stamps/'\n",
    "\n",
    "    with np.load(data_dir + 'all_clusters.npz') as data:\n",
    "        pos_im = data['arr_0']\n",
    "    with np.load(data_dir + 'randoms.npz') as data:\n",
    "        neg_im = data['arr_0']    \n",
    "\n",
    "    nchan = 3\n",
    "        \n",
    "if data_type == 'ilc':\n",
    "\n",
    "    data_dir = '/project/r/rbond/jorlo/datasets/act_y_stamps/'\n",
    "\n",
    "    with np.load(data_dir + 'ilc_all_clusters.npz') as data:\n",
    "        pos_im = data['arr_0']\n",
    "    with np.load(data_dir + 'ilc_randoms.npz') as data:\n",
    "        neg_im = data['arr_0']  \n",
    "        \n",
    "    pos_im = np.expand_dims(pos_im, axis=-1)\n",
    "    neg_im = np.expand_dims(neg_im, axis=-1)\n",
    "    \n",
    "    nchan = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25d2c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4195, 41, 41, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa6faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = min(pos_im.shape[0], neg_im.shape[0])\n",
    "train_size = int(0.7 * tot)\n",
    "val_size = int(0.15 * tot)\n",
    "test_size = int(0.15 * tot)\n",
    "\n",
    "train_pos = pos_im[:train_size]\n",
    "val_pos = pos_im[train_size:train_size + val_size]\n",
    "test_pos = pos_im[train_size + val_size:]\n",
    "\n",
    "train_neg = neg_im[:train_size]\n",
    "val_neg = neg_im[train_size:train_size + val_size]\n",
    "test_neg = neg_im[train_size + val_size:]\n",
    "\n",
    "input_shape = train_pos.shape[1:]\n",
    "\n",
    "train_images = np.concatenate((train_pos,train_neg))\n",
    "val_images = np.concatenate((val_pos,val_neg))\n",
    "test_images = np.concatenate((test_pos,test_neg))\n",
    "\n",
    "train_labels = np.array(train_pos.shape[0]*[int(1)] + train_neg.shape[0]*[int(0)])\n",
    "val_labels = np.array(val_pos.shape[0]*[int(1)] + val_neg.shape[0]*[int(0)])\n",
    "test_labels = np.array(test_pos.shape[0]*[int(1)] + test_neg.shape[0]*[int(0)])\n",
    "\n",
    "train_images = train_images.transpose(0,3,1,2)\n",
    "val_images = val_images.transpose(0,3,1,2)\n",
    "test_images = test_images.transpose(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228a539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=360, width_shift_range=4,\n",
    "#height_shift_range=4,zoom_range=0.3)\n",
    "\n",
    "augment = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(360),\n",
    "    torchvision.transforms.RandomHorizontalFlip([0.5]),\n",
    "    torchvision.transforms.RandomVerticalFlip([0.5]),   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673152a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.Tensor(train_images)\n",
    "val_images = torch.Tensor(val_images)\n",
    "test_images = torch.Tensor(test_images)\n",
    "\n",
    "train_labels = torch.Tensor(train_labels).type(torch.LongTensor)\n",
    "val_labels = torch.Tensor(val_labels).type(torch.LongTensor)\n",
    "test_labels = torch.Tensor(test_labels).type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbcbf87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 3, 41, 41])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6469b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6b63e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffdbe1dc490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2,0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017bbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "#model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "model.fc = nn.Linear(512, 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b8eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        #print(y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0d48a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.870585  [    0/ 5872]\n",
      "loss: 0.697831  [  640/ 5872]\n",
      "loss: 0.687635  [ 1280/ 5872]\n",
      "loss: 0.632885  [ 1920/ 5872]\n",
      "loss: 0.597870  [ 2560/ 5872]\n",
      "loss: 0.541971  [ 3200/ 5872]\n",
      "loss: 0.536737  [ 3840/ 5872]\n",
      "loss: 0.446188  [ 4480/ 5872]\n",
      "loss: 0.488809  [ 5120/ 5872]\n",
      "loss: 0.396185  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.416242 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.438200  [    0/ 5872]\n",
      "loss: 0.356182  [  640/ 5872]\n",
      "loss: 0.268703  [ 1280/ 5872]\n",
      "loss: 0.387935  [ 1920/ 5872]\n",
      "loss: 0.411044  [ 2560/ 5872]\n",
      "loss: 0.486236  [ 3200/ 5872]\n",
      "loss: 0.309742  [ 3840/ 5872]\n",
      "loss: 0.450505  [ 4480/ 5872]\n",
      "loss: 0.306317  [ 5120/ 5872]\n",
      "loss: 0.295895  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.303031 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.346753  [    0/ 5872]\n",
      "loss: 0.269558  [  640/ 5872]\n",
      "loss: 0.248766  [ 1280/ 5872]\n",
      "loss: 0.218884  [ 1920/ 5872]\n",
      "loss: 0.238437  [ 2560/ 5872]\n",
      "loss: 0.193072  [ 3200/ 5872]\n",
      "loss: 0.314573  [ 3840/ 5872]\n",
      "loss: 0.188437  [ 4480/ 5872]\n",
      "loss: 0.322146  [ 5120/ 5872]\n",
      "loss: 0.241842  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.258005 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.275403  [    0/ 5872]\n",
      "loss: 0.239093  [  640/ 5872]\n",
      "loss: 0.185669  [ 1280/ 5872]\n",
      "loss: 0.143579  [ 1920/ 5872]\n",
      "loss: 0.227703  [ 2560/ 5872]\n",
      "loss: 0.248060  [ 3200/ 5872]\n",
      "loss: 0.148553  [ 3840/ 5872]\n",
      "loss: 0.204767  [ 4480/ 5872]\n",
      "loss: 0.136779  [ 5120/ 5872]\n",
      "loss: 0.165735  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.231068 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.161743  [    0/ 5872]\n",
      "loss: 0.187824  [  640/ 5872]\n",
      "loss: 0.215237  [ 1280/ 5872]\n",
      "loss: 0.206585  [ 1920/ 5872]\n",
      "loss: 0.240866  [ 2560/ 5872]\n",
      "loss: 0.169265  [ 3200/ 5872]\n",
      "loss: 0.144229  [ 3840/ 5872]\n",
      "loss: 0.182123  [ 4480/ 5872]\n",
      "loss: 0.160287  [ 5120/ 5872]\n",
      "loss: 0.187668  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.207700 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.145405  [    0/ 5872]\n",
      "loss: 0.142512  [  640/ 5872]\n",
      "loss: 0.166380  [ 1280/ 5872]\n",
      "loss: 0.200196  [ 1920/ 5872]\n",
      "loss: 0.171028  [ 2560/ 5872]\n",
      "loss: 0.228613  [ 3200/ 5872]\n",
      "loss: 0.107777  [ 3840/ 5872]\n",
      "loss: 0.137335  [ 4480/ 5872]\n",
      "loss: 0.136726  [ 5120/ 5872]\n",
      "loss: 0.107982  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.184597 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.149695  [    0/ 5872]\n",
      "loss: 0.186882  [  640/ 5872]\n",
      "loss: 0.126582  [ 1280/ 5872]\n",
      "loss: 0.108704  [ 1920/ 5872]\n",
      "loss: 0.134033  [ 2560/ 5872]\n",
      "loss: 0.079349  [ 3200/ 5872]\n",
      "loss: 0.175806  [ 3840/ 5872]\n",
      "loss: 0.243983  [ 4480/ 5872]\n",
      "loss: 0.078974  [ 5120/ 5872]\n",
      "loss: 0.101651  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.166657 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.090856  [    0/ 5872]\n",
      "loss: 0.100067  [  640/ 5872]\n",
      "loss: 0.124925  [ 1280/ 5872]\n",
      "loss: 0.118605  [ 1920/ 5872]\n",
      "loss: 0.129295  [ 2560/ 5872]\n",
      "loss: 0.094232  [ 3200/ 5872]\n",
      "loss: 0.238450  [ 3840/ 5872]\n",
      "loss: 0.096523  [ 4480/ 5872]\n",
      "loss: 0.102215  [ 5120/ 5872]\n",
      "loss: 0.133394  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.156473 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.061280  [    0/ 5872]\n",
      "loss: 0.089331  [  640/ 5872]\n",
      "loss: 0.087832  [ 1280/ 5872]\n",
      "loss: 0.080006  [ 1920/ 5872]\n",
      "loss: 0.096058  [ 2560/ 5872]\n",
      "loss: 0.087040  [ 3200/ 5872]\n",
      "loss: 0.148797  [ 3840/ 5872]\n",
      "loss: 0.085137  [ 4480/ 5872]\n",
      "loss: 0.069092  [ 5120/ 5872]\n",
      "loss: 0.068709  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.134074 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.156331  [    0/ 5872]\n",
      "loss: 0.106432  [  640/ 5872]\n",
      "loss: 0.089406  [ 1280/ 5872]\n",
      "loss: 0.111475  [ 1920/ 5872]\n",
      "loss: 0.073015  [ 2560/ 5872]\n",
      "loss: 0.105898  [ 3200/ 5872]\n",
      "loss: 0.055374  [ 3840/ 5872]\n",
      "loss: 0.124209  [ 4480/ 5872]\n",
      "loss: 0.104079  [ 5120/ 5872]\n",
      "loss: 0.079750  [ 5760/ 5872]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.148310 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177e509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.174688 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bd57975",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/project/r/rbond/jorlo/ml-clusters/models/torch-act/act-resnet18.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f35acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379878cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb4d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3037a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(nchan, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class test_Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(test_Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(nchan, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256, 64)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class cifar_test_Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(cifar_test_Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256, 64)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = test_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a5dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b85de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940c1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #print(y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            #print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16e4b6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693437  [    0/ 4862]\n",
      "loss: 0.692682  [  640/ 4862]\n",
      "loss: 0.688887  [ 1280/ 4862]\n",
      "loss: 0.692684  [ 1920/ 4862]\n",
      "loss: 0.693431  [ 2560/ 4862]\n",
      "loss: 0.691977  [ 3200/ 4862]\n",
      "loss: 0.694814  [ 3840/ 4862]\n",
      "loss: 0.695524  [ 4480/ 4862]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693517 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.690546  [    0/ 4862]\n",
      "loss: 0.694796  [  640/ 4862]\n",
      "loss: 0.691999  [ 1280/ 4862]\n",
      "loss: 0.694794  [ 1920/ 4862]\n",
      "loss: 0.694095  [ 2560/ 4862]\n",
      "loss: 0.696863  [ 3200/ 4862]\n",
      "loss: 0.693384  [ 3840/ 4862]\n",
      "loss: 0.691324  [ 4480/ 4862]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693590 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.695430  [    0/ 4862]\n",
      "loss: 0.692708  [  640/ 4862]\n",
      "loss: 0.694021  [ 1280/ 4862]\n",
      "loss: 0.692061  [ 1920/ 4862]\n",
      "loss: 0.690791  [ 2560/ 4862]\n",
      "loss: 0.696526  [ 3200/ 4862]\n",
      "loss: 0.694622  [ 3840/ 4862]\n",
      "loss: 0.692715  [ 4480/ 4862]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693266 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.695312  [    0/ 4862]\n",
      "loss: 0.692062  [  640/ 4862]\n",
      "loss: 0.695987  [ 1280/ 4862]\n",
      "loss: 0.691406  [ 1920/ 4862]\n",
      "loss: 0.696619  [ 2560/ 4862]\n",
      "loss: 0.694607  [ 3200/ 4862]\n",
      "loss: 0.692720  [ 3840/ 4862]\n",
      "loss: 0.695845  [ 4480/ 4862]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693529 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.695816  [    0/ 4862]\n",
      "loss: 0.690271  [  640/ 4862]\n",
      "loss: 0.695149  [ 1280/ 4862]\n",
      "loss: 0.695730  [ 1920/ 4862]\n",
      "loss: 0.692731  [ 2560/ 4862]\n",
      "loss: 0.694524  [ 3200/ 4862]\n",
      "loss: 0.695109  [ 3840/ 4862]\n",
      "loss: 0.693329  [ 4480/ 4862]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693236 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49479b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1])\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1])\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0])\n",
      "[1,    10] loss: 0.003\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0])\n",
      "tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1])\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0])\n",
      "[1,    20] loss: 0.003\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "[1,    30] loss: 0.003\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0])\n",
      "tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1])\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    40] loss: 0.003\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1])\n",
      "[1,    50] loss: 0.003\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1])\n",
      "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "[1,    60] loss: 0.003\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1])\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])\n",
      "[1,    70] loss: 0.003\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0])\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        print(labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947140da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693157 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddae3feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab04d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fba999c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.202\n",
      "[1,  4000] loss: 1.839\n",
      "[1,  6000] loss: 1.657\n",
      "[1,  8000] loss: 1.554\n",
      "[1, 10000] loss: 1.480\n",
      "[1, 12000] loss: 1.389\n",
      "[2,  2000] loss: 1.306\n",
      "[2,  4000] loss: 1.257\n",
      "[2,  6000] loss: 1.223\n",
      "[2,  8000] loss: 1.173\n",
      "[2, 10000] loss: 1.148\n",
      "[2, 12000] loss: 1.107\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.060096  [    0/50000]\n",
      "loss: 0.890363  [  400/50000]\n",
      "loss: 0.372836  [  800/50000]\n",
      "loss: 1.821747  [ 1200/50000]\n",
      "loss: 0.707152  [ 1600/50000]\n",
      "loss: 0.345539  [ 2000/50000]\n",
      "loss: 0.877902  [ 2400/50000]\n",
      "loss: 1.524981  [ 2800/50000]\n",
      "loss: 2.065835  [ 3200/50000]\n",
      "loss: 0.595890  [ 3600/50000]\n",
      "loss: 1.007974  [ 4000/50000]\n",
      "loss: 1.117746  [ 4400/50000]\n",
      "loss: 0.597372  [ 4800/50000]\n",
      "loss: 0.854401  [ 5200/50000]\n",
      "loss: 1.041796  [ 5600/50000]\n",
      "loss: 1.052626  [ 6000/50000]\n",
      "loss: 1.214143  [ 6400/50000]\n",
      "loss: 1.106204  [ 6800/50000]\n",
      "loss: 0.805889  [ 7200/50000]\n",
      "loss: 0.478496  [ 7600/50000]\n",
      "loss: 0.453088  [ 8000/50000]\n",
      "loss: 0.777674  [ 8400/50000]\n",
      "loss: 0.684724  [ 8800/50000]\n",
      "loss: 1.556073  [ 9200/50000]\n",
      "loss: 0.855956  [ 9600/50000]\n",
      "loss: 1.304052  [10000/50000]\n",
      "loss: 0.975991  [10400/50000]\n",
      "loss: 1.312486  [10800/50000]\n",
      "loss: 0.939122  [11200/50000]\n",
      "loss: 1.570962  [11600/50000]\n",
      "loss: 1.059263  [12000/50000]\n",
      "loss: 0.603696  [12400/50000]\n",
      "loss: 1.262460  [12800/50000]\n",
      "loss: 0.899003  [13200/50000]\n",
      "loss: 0.516347  [13600/50000]\n",
      "loss: 1.412817  [14000/50000]\n",
      "loss: 1.940828  [14400/50000]\n",
      "loss: 1.421291  [14800/50000]\n",
      "loss: 1.527958  [15200/50000]\n",
      "loss: 0.660640  [15600/50000]\n",
      "loss: 2.087256  [16000/50000]\n",
      "loss: 1.992364  [16400/50000]\n",
      "loss: 0.742845  [16800/50000]\n",
      "loss: 0.919370  [17200/50000]\n",
      "loss: 2.403116  [17600/50000]\n",
      "loss: 0.861498  [18000/50000]\n",
      "loss: 0.633578  [18400/50000]\n",
      "loss: 0.306112  [18800/50000]\n",
      "loss: 0.616085  [19200/50000]\n",
      "loss: 1.428372  [19600/50000]\n",
      "loss: 1.214573  [20000/50000]\n",
      "loss: 0.237692  [20400/50000]\n",
      "loss: 0.888335  [20800/50000]\n",
      "loss: 1.127685  [21200/50000]\n",
      "loss: 1.894028  [21600/50000]\n",
      "loss: 1.653085  [22000/50000]\n",
      "loss: 0.887104  [22400/50000]\n",
      "loss: 0.768280  [22800/50000]\n",
      "loss: 1.276955  [23200/50000]\n",
      "loss: 0.724101  [23600/50000]\n",
      "loss: 1.389786  [24000/50000]\n",
      "loss: 1.412832  [24400/50000]\n",
      "loss: 0.697471  [24800/50000]\n",
      "loss: 1.534279  [25200/50000]\n",
      "loss: 0.221640  [25600/50000]\n",
      "loss: 0.670590  [26000/50000]\n",
      "loss: 2.082668  [26400/50000]\n",
      "loss: 0.617315  [26800/50000]\n",
      "loss: 0.219669  [27200/50000]\n",
      "loss: 1.576400  [27600/50000]\n",
      "loss: 0.521071  [28000/50000]\n",
      "loss: 0.667109  [28400/50000]\n",
      "loss: 0.446861  [28800/50000]\n",
      "loss: 1.891521  [29200/50000]\n",
      "loss: 0.504127  [29600/50000]\n",
      "loss: 0.900785  [30000/50000]\n",
      "loss: 1.059539  [30400/50000]\n",
      "loss: 1.090192  [30800/50000]\n",
      "loss: 0.438544  [31200/50000]\n",
      "loss: 0.929783  [31600/50000]\n",
      "loss: 1.704021  [32000/50000]\n",
      "loss: 1.206818  [32400/50000]\n",
      "loss: 0.657587  [32800/50000]\n",
      "loss: 0.345679  [33200/50000]\n",
      "loss: 1.015960  [33600/50000]\n",
      "loss: 1.343335  [34000/50000]\n",
      "loss: 0.347090  [34400/50000]\n",
      "loss: 0.932786  [34800/50000]\n",
      "loss: 0.758853  [35200/50000]\n",
      "loss: 0.915432  [35600/50000]\n",
      "loss: 0.995270  [36000/50000]\n",
      "loss: 0.204249  [36400/50000]\n",
      "loss: 0.415024  [36800/50000]\n",
      "loss: 0.493138  [37200/50000]\n",
      "loss: 0.420066  [37600/50000]\n",
      "loss: 1.280315  [38000/50000]\n",
      "loss: 0.264580  [38400/50000]\n",
      "loss: 1.276514  [38800/50000]\n",
      "loss: 1.482562  [39200/50000]\n",
      "loss: 0.770859  [39600/50000]\n",
      "loss: 2.079191  [40000/50000]\n",
      "loss: 1.281586  [40400/50000]\n",
      "loss: 0.515493  [40800/50000]\n",
      "loss: 0.845626  [41200/50000]\n",
      "loss: 0.738795  [41600/50000]\n",
      "loss: 0.445399  [42000/50000]\n",
      "loss: 1.413969  [42400/50000]\n",
      "loss: 1.336969  [42800/50000]\n",
      "loss: 0.504164  [43200/50000]\n",
      "loss: 0.654976  [43600/50000]\n",
      "loss: 0.782981  [44000/50000]\n",
      "loss: 0.397871  [44400/50000]\n",
      "loss: 1.796160  [44800/50000]\n",
      "loss: 1.590147  [45200/50000]\n",
      "loss: 0.364167  [45600/50000]\n",
      "loss: 0.896703  [46000/50000]\n",
      "loss: 1.035134  [46400/50000]\n",
      "loss: 0.276762  [46800/50000]\n",
      "loss: 0.595292  [47200/50000]\n",
      "loss: 0.572970  [47600/50000]\n",
      "loss: 0.073754  [48000/50000]\n",
      "loss: 0.448753  [48400/50000]\n",
      "loss: 0.465436  [48800/50000]\n",
      "loss: 0.205332  [49200/50000]\n",
      "loss: 1.312722  [49600/50000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.639621  [    0/50000]\n",
      "loss: 1.653802  [  400/50000]\n",
      "loss: 0.292712  [  800/50000]\n",
      "loss: 0.695848  [ 1200/50000]\n",
      "loss: 0.864940  [ 1600/50000]\n",
      "loss: 1.086525  [ 2000/50000]\n",
      "loss: 0.244835  [ 2400/50000]\n",
      "loss: 0.362542  [ 2800/50000]\n",
      "loss: 0.355307  [ 3200/50000]\n",
      "loss: 1.828213  [ 3600/50000]\n",
      "loss: 0.367143  [ 4000/50000]\n",
      "loss: 1.277882  [ 4400/50000]\n",
      "loss: 0.404677  [ 4800/50000]\n",
      "loss: 1.039233  [ 5200/50000]\n",
      "loss: 0.240120  [ 5600/50000]\n",
      "loss: 0.288508  [ 6000/50000]\n",
      "loss: 1.079822  [ 6400/50000]\n",
      "loss: 0.691280  [ 6800/50000]\n",
      "loss: 0.616539  [ 7200/50000]\n",
      "loss: 0.250594  [ 7600/50000]\n",
      "loss: 0.478822  [ 8000/50000]\n",
      "loss: 1.108712  [ 8400/50000]\n",
      "loss: 0.735408  [ 8800/50000]\n",
      "loss: 1.227458  [ 9200/50000]\n",
      "loss: 0.875349  [ 9600/50000]\n",
      "loss: 0.750414  [10000/50000]\n",
      "loss: 0.579952  [10400/50000]\n",
      "loss: 0.667180  [10800/50000]\n",
      "loss: 0.434435  [11200/50000]\n",
      "loss: 0.222996  [11600/50000]\n",
      "loss: 0.785771  [12000/50000]\n",
      "loss: 1.845934  [12400/50000]\n",
      "loss: 0.322084  [12800/50000]\n",
      "loss: 0.806117  [13200/50000]\n",
      "loss: 0.476806  [13600/50000]\n",
      "loss: 0.948234  [14000/50000]\n",
      "loss: 1.287097  [14400/50000]\n",
      "loss: 0.955032  [14800/50000]\n",
      "loss: 1.221172  [15200/50000]\n",
      "loss: 0.647941  [15600/50000]\n",
      "loss: 1.866461  [16000/50000]\n",
      "loss: 0.596470  [16400/50000]\n",
      "loss: 0.736356  [16800/50000]\n",
      "loss: 0.987067  [17200/50000]\n",
      "loss: 1.188918  [17600/50000]\n",
      "loss: 1.121629  [18000/50000]\n",
      "loss: 0.608543  [18400/50000]\n",
      "loss: 0.826440  [18800/50000]\n",
      "loss: 0.224803  [19200/50000]\n",
      "loss: 1.689422  [19600/50000]\n",
      "loss: 0.592396  [20000/50000]\n",
      "loss: 0.991186  [20400/50000]\n",
      "loss: 2.725237  [20800/50000]\n",
      "loss: 1.783676  [21200/50000]\n",
      "loss: 0.919037  [21600/50000]\n",
      "loss: 0.479510  [22000/50000]\n",
      "loss: 2.341299  [22400/50000]\n",
      "loss: 0.858123  [22800/50000]\n",
      "loss: 0.478736  [23200/50000]\n",
      "loss: 0.325498  [23600/50000]\n",
      "loss: 0.964267  [24000/50000]\n",
      "loss: 0.981799  [24400/50000]\n",
      "loss: 0.135917  [24800/50000]\n",
      "loss: 0.860336  [25200/50000]\n",
      "loss: 1.013093  [25600/50000]\n",
      "loss: 0.680560  [26000/50000]\n",
      "loss: 0.200244  [26400/50000]\n",
      "loss: 0.774907  [26800/50000]\n",
      "loss: 1.491934  [27200/50000]\n",
      "loss: 0.056649  [27600/50000]\n",
      "loss: 0.515200  [28000/50000]\n",
      "loss: 1.231576  [28400/50000]\n",
      "loss: 0.140577  [28800/50000]\n",
      "loss: 0.326717  [29200/50000]\n",
      "loss: 1.532465  [29600/50000]\n",
      "loss: 0.731401  [30000/50000]\n",
      "loss: 0.604404  [30400/50000]\n",
      "loss: 0.812461  [30800/50000]\n",
      "loss: 0.689520  [31200/50000]\n",
      "loss: 0.852476  [31600/50000]\n",
      "loss: 0.542539  [32000/50000]\n",
      "loss: 0.864933  [32400/50000]\n",
      "loss: 1.742018  [32800/50000]\n",
      "loss: 0.607229  [33200/50000]\n",
      "loss: 1.449111  [33600/50000]\n",
      "loss: 1.386970  [34000/50000]\n",
      "loss: 0.857375  [34400/50000]\n",
      "loss: 0.867152  [34800/50000]\n",
      "loss: 1.795165  [35200/50000]\n",
      "loss: 0.335545  [35600/50000]\n",
      "loss: 0.849488  [36000/50000]\n",
      "loss: 0.586552  [36400/50000]\n",
      "loss: 0.363186  [36800/50000]\n",
      "loss: 1.388242  [37200/50000]\n",
      "loss: 0.587786  [37600/50000]\n",
      "loss: 0.626836  [38000/50000]\n",
      "loss: 0.426292  [38400/50000]\n",
      "loss: 1.404154  [38800/50000]\n",
      "loss: 1.268189  [39200/50000]\n",
      "loss: 0.529208  [39600/50000]\n",
      "loss: 0.831767  [40000/50000]\n",
      "loss: 0.620528  [40400/50000]\n",
      "loss: 0.456411  [40800/50000]\n",
      "loss: 0.862475  [41200/50000]\n",
      "loss: 1.396451  [41600/50000]\n",
      "loss: 1.297489  [42000/50000]\n",
      "loss: 0.222226  [42400/50000]\n",
      "loss: 0.772526  [42800/50000]\n",
      "loss: 0.380015  [43200/50000]\n",
      "loss: 0.525589  [43600/50000]\n",
      "loss: 1.086556  [44000/50000]\n",
      "loss: 0.762763  [44400/50000]\n",
      "loss: 0.389924  [44800/50000]\n",
      "loss: 1.205146  [45200/50000]\n",
      "loss: 0.751991  [45600/50000]\n",
      "loss: 1.432924  [46000/50000]\n",
      "loss: 0.451831  [46400/50000]\n",
      "loss: 1.337262  [46800/50000]\n",
      "loss: 0.885977  [47200/50000]\n",
      "loss: 1.849783  [47600/50000]\n",
      "loss: 0.634227  [48000/50000]\n",
      "loss: 0.344398  [48400/50000]\n",
      "loss: 0.358285  [48800/50000]\n",
      "loss: 1.302626  [49200/50000]\n",
      "loss: 0.863636  [49600/50000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.202002  [    0/50000]\n",
      "loss: 0.995692  [  400/50000]\n",
      "loss: 0.054721  [  800/50000]\n",
      "loss: 1.228471  [ 1200/50000]\n",
      "loss: 0.690716  [ 1600/50000]\n",
      "loss: 0.924596  [ 2000/50000]\n",
      "loss: 0.739889  [ 2400/50000]\n",
      "loss: 0.347516  [ 2800/50000]\n",
      "loss: 1.594353  [ 3200/50000]\n",
      "loss: 0.499001  [ 3600/50000]\n",
      "loss: 0.414310  [ 4000/50000]\n",
      "loss: 1.066976  [ 4400/50000]\n",
      "loss: 0.894724  [ 4800/50000]\n",
      "loss: 0.425009  [ 5200/50000]\n",
      "loss: 0.261705  [ 5600/50000]\n",
      "loss: 1.619232  [ 6000/50000]\n",
      "loss: 0.685091  [ 6400/50000]\n",
      "loss: 0.631607  [ 6800/50000]\n",
      "loss: 1.542419  [ 7200/50000]\n",
      "loss: 0.853632  [ 7600/50000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.742525  [ 8000/50000]\n",
      "loss: 0.778277  [ 8400/50000]\n",
      "loss: 1.118055  [ 8800/50000]\n",
      "loss: 0.483934  [ 9200/50000]\n",
      "loss: 1.168017  [ 9600/50000]\n",
      "loss: 3.126446  [10000/50000]\n",
      "loss: 0.204966  [10400/50000]\n",
      "loss: 0.672307  [10800/50000]\n",
      "loss: 0.938400  [11200/50000]\n",
      "loss: 1.151416  [11600/50000]\n",
      "loss: 0.302296  [12000/50000]\n",
      "loss: 1.558384  [12400/50000]\n",
      "loss: 1.100916  [12800/50000]\n",
      "loss: 0.109435  [13200/50000]\n",
      "loss: 1.966909  [13600/50000]\n",
      "loss: 0.198178  [14000/50000]\n",
      "loss: 3.015626  [14400/50000]\n",
      "loss: 0.780849  [14800/50000]\n",
      "loss: 1.495462  [15200/50000]\n",
      "loss: 0.285534  [15600/50000]\n",
      "loss: 0.262317  [16000/50000]\n",
      "loss: 1.562996  [16400/50000]\n",
      "loss: 2.144889  [16800/50000]\n",
      "loss: 0.275759  [17200/50000]\n",
      "loss: 0.508386  [17600/50000]\n",
      "loss: 2.900773  [18000/50000]\n",
      "loss: 0.261640  [18400/50000]\n",
      "loss: 1.003810  [18800/50000]\n",
      "loss: 0.680074  [19200/50000]\n",
      "loss: 0.399111  [19600/50000]\n",
      "loss: 0.417976  [20000/50000]\n",
      "loss: 1.109479  [20400/50000]\n",
      "loss: 1.602119  [20800/50000]\n",
      "loss: 0.740902  [21200/50000]\n",
      "loss: 0.912175  [21600/50000]\n",
      "loss: 0.702477  [22000/50000]\n",
      "loss: 1.340326  [22400/50000]\n",
      "loss: 0.292778  [22800/50000]\n",
      "loss: 3.001721  [23200/50000]\n",
      "loss: 0.282721  [23600/50000]\n",
      "loss: 0.429418  [24000/50000]\n",
      "loss: 1.204721  [24400/50000]\n",
      "loss: 1.009758  [24800/50000]\n",
      "loss: 0.989984  [25200/50000]\n",
      "loss: 0.544792  [25600/50000]\n",
      "loss: 0.138613  [26000/50000]\n",
      "loss: 0.727053  [26400/50000]\n",
      "loss: 1.291738  [26800/50000]\n",
      "loss: 0.380883  [27200/50000]\n",
      "loss: 0.737675  [27600/50000]\n",
      "loss: 1.668164  [28000/50000]\n",
      "loss: 0.424108  [28400/50000]\n",
      "loss: 1.128641  [28800/50000]\n",
      "loss: 0.448262  [29200/50000]\n",
      "loss: 0.502998  [29600/50000]\n",
      "loss: 1.390533  [30000/50000]\n",
      "loss: 0.461268  [30400/50000]\n",
      "loss: 0.468525  [30800/50000]\n",
      "loss: 0.276306  [31200/50000]\n",
      "loss: 0.202937  [31600/50000]\n",
      "loss: 1.118357  [32000/50000]\n",
      "loss: 0.761776  [32400/50000]\n",
      "loss: 1.906657  [32800/50000]\n",
      "loss: 0.744753  [33200/50000]\n",
      "loss: 0.430453  [33600/50000]\n",
      "loss: 0.125181  [34000/50000]\n",
      "loss: 1.102085  [34400/50000]\n",
      "loss: 3.156640  [34800/50000]\n",
      "loss: 1.313067  [35200/50000]\n",
      "loss: 1.359367  [35600/50000]\n",
      "loss: 1.016252  [36000/50000]\n",
      "loss: 0.646700  [36400/50000]\n",
      "loss: 0.217673  [36800/50000]\n",
      "loss: 0.690380  [37200/50000]\n",
      "loss: 0.968114  [37600/50000]\n",
      "loss: 0.067531  [38000/50000]\n",
      "loss: 0.738000  [38400/50000]\n",
      "loss: 1.576040  [38800/50000]\n",
      "loss: 0.448933  [39200/50000]\n",
      "loss: 0.769586  [39600/50000]\n",
      "loss: 2.070192  [40000/50000]\n",
      "loss: 0.186902  [40400/50000]\n",
      "loss: 1.291891  [40800/50000]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b02cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532f348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1640fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbee108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5b887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca1a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c54e5d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3343235/3906010888.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc1.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (3): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=625, bias=True)\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=625, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (fc2): Linear(in_features=625, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation of CNN/ConvNet Model\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "        # Conv -> (?, 28, 28, 32)\n",
    "        # Pool -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "        # Conv      ->(?, 14, 14, 64)\n",
    "        # Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "        # Conv ->(?, 7, 7, 128)\n",
    "        # Pool ->(?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L5 Final FC 625 inputs -> 10 outputs\n",
    "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#instantiate CNN model\n",
    "model = CNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d5944a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.333513  [    0/60000]\n",
      "loss: 2.310068  [ 6400/60000]\n",
      "loss: 2.273524  [12800/60000]\n",
      "loss: 2.255716  [19200/60000]\n",
      "loss: 2.244400  [25600/60000]\n",
      "loss: 2.212022  [32000/60000]\n",
      "loss: 2.213074  [38400/60000]\n",
      "loss: 2.173571  [44800/60000]\n",
      "loss: 2.146094  [51200/60000]\n",
      "loss: 2.090101  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.063510 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.088126  [    0/60000]\n",
      "loss: 2.029327  [ 6400/60000]\n",
      "loss: 1.865700  [12800/60000]\n",
      "loss: 1.799059  [19200/60000]\n",
      "loss: 1.561360  [25600/60000]\n",
      "loss: 1.439963  [32000/60000]\n",
      "loss: 1.300343  [38400/60000]\n",
      "loss: 1.148937  [44800/60000]\n",
      "loss: 1.130939  [51200/60000]\n",
      "loss: 1.039372  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.964817 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.035348  [    0/60000]\n",
      "loss: 1.024850  [ 6400/60000]\n",
      "loss: 0.736442  [12800/60000]\n",
      "loss: 0.960207  [19200/60000]\n",
      "loss: 0.845868  [25600/60000]\n",
      "loss: 0.868031  [32000/60000]\n",
      "loss: 0.871768  [38400/60000]\n",
      "loss: 0.744164  [44800/60000]\n",
      "loss: 0.841591  [51200/60000]\n",
      "loss: 0.886496  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.774923 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.766985  [    0/60000]\n",
      "loss: 0.864394  [ 6400/60000]\n",
      "loss: 0.576703  [12800/60000]\n",
      "loss: 0.865990  [19200/60000]\n",
      "loss: 0.773797  [25600/60000]\n",
      "loss: 0.791727  [32000/60000]\n",
      "loss: 0.804248  [38400/60000]\n",
      "loss: 0.685180  [44800/60000]\n",
      "loss: 0.774078  [51200/60000]\n",
      "loss: 0.833923  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.724752 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.676635  [    0/60000]\n",
      "loss: 0.793786  [ 6400/60000]\n",
      "loss: 0.523416  [12800/60000]\n",
      "loss: 0.812837  [19200/60000]\n",
      "loss: 0.729965  [25600/60000]\n",
      "loss: 0.759640  [32000/60000]\n",
      "loss: 0.761278  [38400/60000]\n",
      "loss: 0.654649  [44800/60000]\n",
      "loss: 0.741718  [51200/60000]\n",
      "loss: 0.792836  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.693916 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.622157  [    0/60000]\n",
      "loss: 0.745662  [ 6400/60000]\n",
      "loss: 0.489755  [12800/60000]\n",
      "loss: 0.771136  [19200/60000]\n",
      "loss: 0.704861  [25600/60000]\n",
      "loss: 0.740639  [32000/60000]\n",
      "loss: 0.727107  [38400/60000]\n",
      "loss: 0.631473  [44800/60000]\n",
      "loss: 0.722566  [51200/60000]\n",
      "loss: 0.757392  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.669962 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.581996  [    0/60000]\n",
      "loss: 0.708483  [ 6400/60000]\n",
      "loss: 0.464897  [12800/60000]\n",
      "loss: 0.736457  [19200/60000]\n",
      "loss: 0.686697  [25600/60000]\n",
      "loss: 0.724395  [32000/60000]\n",
      "loss: 0.700341  [38400/60000]\n",
      "loss: 0.615090  [44800/60000]\n",
      "loss: 0.709408  [51200/60000]\n",
      "loss: 0.726727  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.649821 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.549971  [    0/60000]\n",
      "loss: 0.677273  [ 6400/60000]\n",
      "loss: 0.443762  [12800/60000]\n",
      "loss: 0.707362  [19200/60000]\n",
      "loss: 0.670232  [25600/60000]\n",
      "loss: 0.709434  [32000/60000]\n",
      "loss: 0.676416  [38400/60000]\n",
      "loss: 0.600556  [44800/60000]\n",
      "loss: 0.699560  [51200/60000]\n",
      "loss: 0.699143  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.631920 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.523097  [    0/60000]\n",
      "loss: 0.649075  [ 6400/60000]\n",
      "loss: 0.424766  [12800/60000]\n",
      "loss: 0.681190  [19200/60000]\n",
      "loss: 0.654910  [25600/60000]\n",
      "loss: 0.693513  [32000/60000]\n",
      "loss: 0.654135  [38400/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ml-torch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e339e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cceed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0dfca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d08af891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "738cc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e5a77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a35e2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        print(y)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d84af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65abcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "ml-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
